# ML
# Kaggle 스터디
## Porto - Data Preparation & Exploration

<br>

### Norminal Vs Ordinal
- Norminal : order나 ranking이 없는 그저 category만 다른 것들
    - e.g) 노랑, 빨강, 파랑
- Ordinal : ranking이나 order가 존재하는 피처
    - e.g) 타이타닉의 1등급선, 2등급선, 
이제 안헷갈리겠지

<br>

### 과정
- 데이터 보기
    - 대회의 Data description 꼭 살펴보기
    - `head()`, `shape`, `info()`으로 쌩 데이터 확인
    - `drop_duplicates()` 으로 shape 비교하여 trainin_data에 중복있는지 확인
    - train과 test의 shape가 다른지 확인 (타겟피처 하나차이일 때는 OK)
    - dummification, 즉, binary로 만들 것들 슬쩍 한 번 어떻게 해야할지 봄.
- 메타데이터 만들기
    - 이 사람은 이 대회에서 하도 값들이 뭔가 깐간해서 값 꺼내쓰기 좋게 메타데이터 만들었음. 구성은 이렇게
        - role : input, ID, target
        - level : nominal, interval, ordinal, binary
        - keep : True or False
        - dtype: int, float, str
        - train.colums 반복해서 train.colums[i]의 role, level 등등을 지정하고 이걸 Dic으로 저장한 뒤 meta라는 이름으로 DataFrame 으로 만듦. 
        - 이걸 가지고 keep해놓고싶지않은 데이터는 False해버리고 이런 식으로 유용하게 씀.
- 통계 확인
    - `describe()` 로 mean, std 이런거 봐야하는데 ***categorical*** 은 그런걸 보는게 그다지 중요한 의미 없다고 함.
    - Interval, Ordinal 등을 보면서 scaling해야할 것들이 뭔지 확인
        - min~max 까지의 분포도가 비슷한지
        - null 혹은 missing 값들이 있는지
    - 값의 밸런스가 너무 안좋으면(피쳐의 값이 어느 하나의 값으로 몰려있다던지) undersampling이나 oversampling
        

<br><br><br>

# Coursera - CNN Week 2 Programming Assignment
시험이라 코드는 없음
- ResNet을 만들어봤다.
    - `identity_block` 과 `covolutional_block` 을 계속 포개면서 만들어보니 저번에 했던 손사인 알아보는 정확도가 높이 올라갔다.
    - keras 정말 뭐든 할 수 있구나 생각보다 쉽게 구현되어서 놀랐다.
- pre-trained CNN 을 가지고 Transfer_learning을 해봤다.
    - 데이터셋을 디렉토리에서 불러올 때 train valid 로 나눠버리면서 가져와봄
        - tensorflow.keras.preprocessing의 `image_data_set_from_directory()` 사용
    - Preprocess와 data-augmentation을 SequentialAPI로 해봄
        - `prefetch()` 을 사용해서 메모리 병목현상을 예방할 수 있음
        - `tf.keras.Sequential()` 모델을 만들어서 
            - `RandomFlip(...)`, `(RandomRotation(0.2)` 등을 `add` 하면 편하게 증강가능
    - FunctionalAPI와 MobileNet을 가지고 pre-trained 모델을 새로운 모델에 적용시켜봄
        - `tf.keras.applications.MobileNetV2` 를 이용하여 이미 훈련된 MobileNetV2을 pre-train 모델로 사용
        - 훈련된 모델의 top layer를 제거하고 새 분류 층을 집어넣어어야함
            - `model.trainable=False` 로 다른 레이어를 얼린다. 이 때, 탑레이어는 제외해야함
            - 베이스 모델에 preprocess를 거친 데이터를 넣고 풀링과 드랍아웃을 해줌. 그리고 Dense를 이용하여 fc를 한다.
            - 그리고 훈련시킴
    - 정확도를 올리기 위해서 마지막 레이어를 튜닝해봄
        - 베이스모델의 일정 레이어만 반복문으로 골라서 `trainable=True` 를 해주고 
        - loss_fucntion(e.g BinaryCrossentropy), optimizer(e.g Adam), metrics(e.g accuracy)를 설정해서 compile해준다.

<br><br>

# Coursera - Convolutional Neural Networks
## Detection Algorithms
Object Detection 을 해보자.

<br>

### Object Localization (객체 위치 식별)
Object Detection 을 하기 위해서는 일단 위치를 식별하는 것을 학습해야한다. 직사각형을 이용해서 위치를 구분(classification)하는 것이다. <br>
탐지한 객체가 어디에 있는지를 알려주기위해 ConvNet의 출력에 `bounding box` 의 요소 bx, by, bh, bw를 가지는 아웃풋을 더 만든다. <br>
그렇다면 총 아웃풋은 다음과 같을 것이다.

- P_c: 객체가 있는지 없는지 (background랑은 다르다.)
    - 이것이 0(없다)면 다른 아웃풋은 관심도안줌
    - 첫번째 유닛(y^_1)으로 둔다.
- bx : 전체 이미지의 가로에서 박스가 어디있는지
- by : 전체 이미지의 세로에서 박스가 어디있는지
- bh : 객체(bounding box)의 높이
- bw : 객체(bounding box)의 넓이
- 원래 출력하려던 class1, 2, 3...

<br>

이것의 손실함수는 다음과 같다.
```
L(y^, y) = (y^_1- y_1)² + (y^21- y_2)² ... + (y^_γ- y_γ)²  
```
하지만 P_c가 0일 때, 즉, 객체가 없어서 y^_1이 0일 때는

```
L(y^, y) = (y^_1- y_1)²
```
이 된다. 나머지 요소들은 다 관심을 주지 않을 것이기 때문이다. <br>
이제 모델은 classification 뿐 아니라, localization까지 가능하게 되었다 WOW

<br>



