# **Bleu Score**
## Bleu Score 장점
- 계산이 쉽고 이해가 빠르다.
- 사람레벨과 대응된다.
- `language-independent` 하다.
    - 특정 언어에 국한되지 않는다는 말.
- 둘 이상의 ground truth 문장이 있을 때 사용가능함
- 광범위하게 쓰이기 때문에 결과를 비교하기가 쉬움

<br><br>

## Bleu Score 단점
- 단어의 `의미` 를 고려하지 않는다.
- 정확히 일치하는 단어만 찾는다.
    - e.g) rain과 raining은 아주 다른 단어로 취급
- 말의 중요성을 무시한다.
    - e.g) to나 an과 같은 별 상관없는 단어도 다 중요하다고 취급
- 단어들의 나열을 무시한다.

<br><br>

## WER/CER
target 문장이 모호하지않거나 해석의 대상이 아니라면 Bleu Score은 이상적인 metric이 아니다. Speech-to-text와 같은 모델들은 Bleu Score를 사용하지 않는다. 대신 일반적으로 WER(Word Error Rate) 혹은 비슷한 CER(Character Error Rate)를 metric으로 사용한다. 이 metric은 **예측 아웃풋과 target 스크립트를 단어별로(CER은 char별로) 비교** 하여 둘 사이의 **차이의 수를 파악**한다. 이 차이는 다음과 같다.

- target에는 있지만 예측에는 존재하지않는 단어 (삭제)
- target에는 없지만 예측에는 존재하는 단어 (삽입)
- target과 예측 사이에서 변경된 단어 (대체)

<br>

WER의 공식은 간단하다. 총 단어 수에 대한 차이의 퍼센테이지이다.

```
Word Error Rate = 삭제 + 삽입 + 대체 / target의 총 단어 수
```
*(WER은 두 단어간에 차이를 측정하는 Levenstein 거리를 기반에 둔 metric)*

<br><br>

WER도 많이 쓰이긴 하지만 단점들이 좀 있다. <br>
- 의미와 관련해 중요한 단어와 그렇지 않은 단어를 구분하지 못 한다.
- 단어를 고려할 때, 두 단어가 싱글 char로 다른지 아니면 완전히 다른지를 구분하지 못한다.


<br><br><br>

# Decoding in NLP
Decoding 알고리즘은 아웃풋 단어를 선택하는데 도움이 되는 알고리즘이다. 대게 **모델이** 할당한 probability에 의해 `token` / `word` / `character` 을 **선택**한다고 생각할 수 있다. 이것은 언어기반 task에서는 **반만 정답**이라고 한다. 언어기반에서는 일반적으로 특정 `word`/`token` 의 **확률을 나타내는 array를 출력하는 모델을 만든다**. 이 array의 확률 중에 가장 높은 확률의 token을 선택하는 것이 충분히 논리적이긴하지만 더 깊이 들어가면 이렇게 단순하게 해결되진않는다. 그래서 token을 선택할 때 이 decoding을 수행해줄 대체 방법이 필요하다.

<br>



## Greedy-Decoder
디코딩 알고리즘 중에 하나인 Greedy-Decoder 를 알아보자. 잠재적 아웃풋 리스트와 이미 계산된 확률 분포를 취하고 **가장 확률이 높은 단어를 선택**하는 가장 간단한 접근 방식이다. 잘 작동하긴하지만 긴 문장에서는 문제가 많아서 다른 디코딩 알고리즘에 비해 **아웃풋 퀄리티가 낮은 경우가 많다.** 특정 단어나 문장에 달라붙어 이 단어 집합을 반복적으로 가장 높은 확률로 할당하기 때문이다.

<br><br>

## BEAM SEARCH DECODER

<br>

![](https://miro.medium.com/max/911/1*b0Ju6O3uP2Gxo5Ho_z2HKA.png)


<br>

Greedy-Decoder 에서는 각 단계에서 단일 단어를 다룬다. 만약 모든 단계에서 여러 단어를 트랙킹하고 여러 가능성을 생성할 수 있다면? 그것이 바로 beam search 알고리즘이 하는 일이다. 근처에 있는 단어가 아닌 **미래의 여러 단어들을 확인하고 그 결합을 평가하여 최상의 옵션을 선택**한다.(최종적으로는 *트리형식*임) 이 과정에서 잠재적인 아웃풋 시퀀스를 리턴하는데 이것이 우리가 고려하는 옵션의 수이며 검색하고 있는 'Beam'의 수이다.
<br>

시퀀스의 순위를 매기고 가장 가능성이 높은 것을 선택하기 때문에 반복적인 시퀀스로 변해버릴 수 있다. 그래서 디코딩의 `temperature` 를 올린다. `temperature`는 아웃풋의 randomness(임의성)을 제어해준다. <br>
더 자세한 것은 [20210908 코세라 TIL](https://github.com/matamong/Study/blob/master/TIL_Diary/20210908.md)로..

<br><br>

## Pure-Sampling
greedy-decoding과 비슷하지만, 확률이 가장 높은 단어를 선택하는 대신 전체 vacabulary 확률 분포에서 단어를 무작위로 샘플링한다. 무작위성을 부여함으로써 같은 단어를 반복하는 것에서 벗어날 수 있지만 일관성이 부족할 수 있다.

<br><br>

## TOP-K SAMPLING DECODER
Pure-Sampling과 비슷하지만 전체 vacabulary 확률 분포에서 샘플링하지않고 상위 k개 probable 단어들에서 샘플링한다. k=1이면 greedy 탐색이 되는 것이고 k가 vocab의 사이즈라면 Pure-Sampling 디코더가 되는 것이다.


<br><br><br><br>

* * *

https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b <br>

https://towardsdatascience.com/the-three-decoding-methods-for-nlp-23ca59cb1e9d <br>

https://medium.com/voice-tech-podcast/visualising-beam-search-and-other-decoding-algorithms-for-natural-language-generation-fbba7cba2c5b