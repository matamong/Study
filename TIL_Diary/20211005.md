# Transformer

<br>

![Transformer 2](https://www.tensorflow.org/images/tutorials/transformer/transformer.png)

<br>

논문에서 나온 위의 그림을 본 뒤, Transformer를 이용하여 영어->프랑스어로 기계번역을 한다고 생각하면서 복습하자.

<br>

## Encoder
RNN의 인코더는 한 스텝마다 단어를 인코딩(word-embedding만들기)했다. 그러나 Transformer의 인코더는 time step 컨셉이 없고 모든 단어를 인코딩한다. 여기에 `위치 인코딩`을 더 해서 문장의 단어가 어디에 있는지를 파악할 수 있게 도와줌으로써 더 좋은 해석을 할 수 있게한다. 이 것은 단어 embedding + positional Encoding을 더함으로써 구현할 수 있고 이렇게 나온 결과는 **context를 가진 embedding**이 된다. <br>
이것은 `Multi-Head Attention`과 `Feed Forward`레이어를 거치게된다.
- `Multi-Head Attention` : self-attention을 통해 문장의 어느 부분에 포커스를 맞춰야할지를 계산한다. 문장의 i번째 단어가 같은 문장의 다른 단어와 얼마나 관계가 있는지를 계산하는 방식을 사용한다. 모든 단어에서 attention vector를 만들어내는데 이것은 단어와 문장의 관계를 알려준다.
    - self-attention은 본인에게 가중치를 더 둘 수 있는데 이것이 단점이 된다. 우리는 다른 단어들과의 관계를 보고싶기때문이다. 그래서 단어마다 예를 들면 8개의 attention vector를 또 만들어서 그것의 가중치의 평균을 취하는 attention vector를 사용한다. 이것이 Multi-Head attention 블록이라고 불리는 이유이다.
- `Feed Forward` : 단순한 feed forward방식의 신경망이다. attention vector의 모든 것을 feed forward한다. 여기서 다음 인코더나 디코더가 소화하기 쉬운 형태로 만들어준다.
    - 이 때 앞서 계산했던 attention vector들이 한 번에 한 벡터들만 feed forward 레이어로 들어갈 수 있는데 각각의 attention net들은 서로에게 독립적이기 때문에 병렬화를 할 수 있다! 그래서 여기서는 병렬화를 해준다.

<br>

## Decoder
여기서도 임베딩을 할 때 `위치 인코딩` 을 더한다.
- `Encoder-Decoder Attention`: self-attention을 통하여 영어와 프랑스어의 각각의 단어 임베딩을 가지고 관계를 매핑하고 출력한다.
- `Masked Multi-Head Attention`: 다음 프랑스 단어를 생성할 때 영어 문장에서 나온 단어들을 사용할 수 있는데 다만 이 때 사용할 수 있는 단어를 오직 프랑스 문장의 이전 단어로 제한한다. 프랑스 문장의 모든 단어를 사용하면 학습은 안되고 그냥 다음 단어를 내뱉는 것 밖에 되지않는다. 그래서 행렬연산에서 병렬연산을 할 때 나중에 나올 단어를 masking해야한다.(attention 망이 못 사용하도록 0으로 만들어줌)
- `Feed Forward`: 다음으로 attention vector를 Feed Forward로 보낸다. 여기서는 다음 디코더나 Linear 레이어에 소화가 잘 되도록 출력벡터를 형성해준다.
- `Linear Layer`: 놀랍게도 또 다른 Feed Forward 레이어다. 뉴런은 출력하고싶었던 프랑스어의 단어와 같다.
- `Softmax` : 여기서는 Softmax를 사용하여 확률분포로 만들어준다.

이런식으로 다음으로 올 단어로 확률이 높은 단어를 디코더에서 계속 출력해준다. EOS가 나올 때 까지!



<br><br><br><br>

# Weights & Biases
모델링 진행상황을 추적할 수 있도록 하는 추적도구이다.
- 한 번에 모든 시도들을 다 추적할 수 있음
- 실행을 쉽게 비교할 수 있음
- 진행상황을 문서화할 수 있음
- 모든 팀의 결과를 볼 수 있음

<br><br><br><br>



<br>

* * *
[Transformer Neural Networks - EXPLAINED! (Attention is all you need)
](https://youtu.be/TQQlZhbC5ps)