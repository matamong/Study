# Tensorflow 자격증
## Transfer Learning
pre-trained 인셉션 모델 사용해보기.

```python
import os

from tensorflow.keras import layers
from tensorflow.keras import Model
from tensorflow.keras.applications.inception_v3 import InceptionV3

local_weights_file = '로컬 가중치 파일'

pre_trained_model = InceptionV3(
    input_shape = (150, 150, 3),
    include_top = False,
    weights = None
)

pre_trained_model.load_weights(local_weights_file)

for layer in pre_trained_model.layers:
    layer.trainable = False

# summary를 이용해서 마지막 레이어를 알 수 있따.
last_layer = pre_trained_model.get_layer('mixed7')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output
```

```
last layer output shape:  (None, 7, 7, 768)
```

```python
from tensorflow.keras.optimizers import RMSprop

# Flatten the output layer to 1 dimension
x = layers.Flatten()(last_output)

# Add a fully connected layer with 1,024 hidden units and ReLU activation
x = layers.Dense(1024, activation='relu')(x)

# Add a dropout rate of 0.2
x = layers.Dropout(0.2)(x)

# Add a final sigmoid layer for classification
x = layers.Dense(1, activation='sigmoid')(x)

model = Model(pre_trained_model.input, x)

model.compile(
    optimizer = RMSprop(learning_rate=0.0001),
    loss = 'binary_crossentropy',
    metrics = ['accuracy']
)
```

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import os
import zipfile

zip_ref = zipfile.ZipFile('zip 파일 경로', 'r')
zip_ref.extractall('tmp/')
zip_ref.close()

# Define our example diretories and files
base_dir = 'tmp/cats_and_dogs_filtered'

train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

train_cats_dir = os.path.join(train_dir, 'cats')
train_cats_dir = os.path.join(train_dir, 'dogs')
validation_cats_dir = os.path.join(validation_dir, 'cats')
validation_cats_dir = os.path.join(validation_dir, 'dogs')

train_cat_fnames = os.listdir(train_cats_dir)
train_dog_fnames = os.listdir(train_dogs_dir)

# Add our data-augmentation parameters to ImageDataGenerator
train_datagen = ImageDataGenerator(
    rescale = 1./255.,
    rotation_range = 40,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    shear_range = 0.2,
    zoom_range = 0.2,
    horizontal_flip = True
)

# Note that the validation data should not be augmented!
test_dataget = ImageDataGenerator(rescale = 1.0/255.)

# Flow training images in batches of 20 using train_datagen generator
train_generator = train_datagen.flow_from_directory(
    train_dir,
    batch_size = 20,
    class_mode = 'binary',
    target_size = (150, 150)
)
```

```python
history = model.fit_generator(
    train_generator,
    validation_data = validation_generator,
    steps_per_epoch = 100,
    epochs = 20,
    validation_steps = 50,
    verbose = 2
)
```

```python
import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.histroy['loss']
val_loss = histroy.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc=0)
plt.figure()

plot.show()
```

<br>

Dropout으로 overfitting을 피해줬다.

<br><br>

# Multiclass Classification

```python
train_datagen = ImageDataGenrator(rescale=1./255)

train_generator = train_dataget.flow_from_directory(
    train_dir,
    target_size=(300, 300),
    batch_size=128,
    class_mode='categorical'
)
```

```python
model = tf.keras.models.Sequential([
    ...
    tf.keras.layers.Dense(3, activation='softmax')
])
```

```python
model.compile(
    loss='categorical_crossentropy',
    optimizer=RMSprop(lr=0.001),
    metrics=['acc']
)
```

- clas_mode 를 `'categorical'`로 바꾼다.
- 멀티클래스 구분에 맞게 모델의 마지막 레이어 유닛수와 activation을 바꿔준다.
- 멀티클래스 구분에 맞게 loss function을 바꿔준다.

<br>

파일을 읽어서 라벨과 이미지의 numpy를 쪼개주는 방법은 다음과 같다.
```python
def get_data(filename):
  # You will need to write code that will read the file passed
  # into this function. The first line contains the column headers
  # so you should ignore it
  # Each successive line contians 785 comma separated values between 0 and 255
  # The first value is the label
  # The rest are the pixel values for that picture
  # The function will return 2 np.array types. One with all the labels
  # One with all the images
  #
  # Tips: 
  # If you read a full line (as 'row') then row[0] has the label
  # and row[1:785] has the 784 pixel values
  # Take a look at np.array_split to turn the 784 pixels into 28x28
  # You are reading in strings, but need the values to be floats
  # Check out np.array().astype for a conversion
    with open(filename) as training_file:
      # Your code starts here
        read = csv.reader(training_file, delimiter=',')
        first_line = True
        temp_images=[]
        temp_labels=[]
        for row in read:
            if first_line:
                first_line = False
            else:
                temp_labels.append(row[0])
                images_data = row[1:875]
                as_array = np.array_split(images_data, 28)
                temp_images.append(as_array)
        images = np.array(temp_images).astype('float')
        labels = np.array(temp_labels).astype('float')
      # Your code ends here
    return images, labels

path_sign_mnist_train = f"{getcwd()}/../tmp2/sign_mnist_train.csv"
path_sign_mnist_test = f"{getcwd()}/../tmp2/sign_mnist_test.csv"
training_images, training_labels = get_data(path_sign_mnist_train)
testing_images, testing_labels = get_data(path_sign_mnist_test)

# Keep these
print(training_images.shape)
print(training_labels.shape)
print(testing_images.shape)
print(testing_labels.shape)
```

<br><br>

## Natural Language Processing in TensorFlow

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'I love my dog',
    'I love my cat'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index)
```
```python
{'i': 1, 'my': 3, 'dog': 4, 'cat': 5, 'love': 2}
```

- `word_index` 는 tokenizer가 인코딩한 단어의 인덱스를 dict 형식으로 반환해준다. 이 때 대문자가 있으면 소문자로 변형해준다.
- tokenizer는 !,? 과 같은 것들을 자동으로 무시해준다.

<br><br>

이렇게 단어를 토큰화하여주었으므로 다음으로 할 일은 주어진 문장을 이 토큰을 이용하여 값들의 list로 만들어주는 것이다. 이렇게 하면 이 list의 길이를 만질 수 있게되고 입력으로 넣을 때 편하다.

```python
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'I love my dog',
    'I love my cat',
    'You love my dog!',
    'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)

print(word_index)
print(sequences)
```
```python
{'amazing': 10, 'dog': 3, 'you': 5, 'cat': 6, 'think': 8, 'i': 4, 'is': 9, 'my': 1, 'do': 7, 'love': 2}

[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]
```
- `texts_to_sequences` 는 인코딩한 단어의 시퀀스를 제공해준다.

<br><br>

### 보지 못한 단어 처리

```python
test_data = [
    'i really love my dog',
    'my dog loves my manatee'
]

test_seq = tokenizer.texts_to_sequences(test_data)
print(test_seq)

```

<br>

```python
[[4, 2, 1, 3], [1, 3, 1]]

{'amazing': 10, 'dog': 3, 'you': 5, 'cat': 6, 'think': 8, 'i': 4, 'is': 9, 'my': 1, 'do': 7, 'love': 2}
```
- 읽어보지 못 한 문장이 들어간 테스트 문장을 넣으면 위와 같이 나온다.
- 이렇기 때문에 굉장히 많은 데이터가 들어가야 단어를 무시하지 않을 것이고, 차라리 단어가 무시되는 것보다 특별한 값을 넣어줘서 카운트 되게 해줄 수도 있을 것이다.

무시당하는 사전을 위해 다음과 같이 해줄 수 있다.

```python
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'I love my dog',
    'I love my cat',
    'You love my dog!',
    'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100, oov_token='<oov>')
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)

test_data = [
    'i really love my dog',
    'my dog loves my manatee'
]

test_seq = tokenizer.texts_to_sequences(test_data)

print(test_seq)

```
```python
[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]

{'think': 9, 'amazing': 11, 'dog': 4, 'do': 8, 'i': 5, 'cat': 7, 'you': 6, 'love': 3, '<oov>': 1, 'my': 2, 'is': 10}

```
- 완벽하진않지만 `oov_token`을 넣어줘서 사전에 존재하지않는 단어를 대체한다.

<br><br>

### Padding으로 입력 값을 똑같이
다음과 같이 padding을 넣어줘서 모든 문장의 길이를 똑같이 만들어줄 수 있다.

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

sentences = [
    'I love my dog',
    'I love my cat',
    'You love my dog!',
    'Do you think my dog is amazing?'
]

tokenizer = Tokenizer(num_words = 100, oov_token='<oov>')
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(sentences)
padded = pad_sequences(sequences)

```
