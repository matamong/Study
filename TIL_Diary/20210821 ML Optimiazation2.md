# ML

# Coursera Optimization Algorithm
잘 작동되는 모델을 찾으려고 훈련을 반복해야하는 숙명에 놓여있는 머신러닝. <br>
훈련의 반복이니까 모델을 빠르게 학습시키는 것도 중요하다. <br>
엄청나게 많은 큰 데이터를 훈련하는 것은 엄청 느릴 것인데 그때문에 **최적화 알고리즘을 사용하여 효율성을 좋게만들어야한다.** <br>
그러므로 다음과 같은 알고리즘을 활용할 수 있다. <br>
<br>
지난 시간에 미니배치경사하강법을 알았는데 그것보다 더 효율적인 알고리즘을 오늘 배운다. <br>
하지만 일단, 그에 앞서서 다음을 알아야한다. <br>
- **`Exponentilally weighted average (지수가중평균)`**
- **`Bias Correction (편향 보정)`**  

<br>

위를 숙지하고 나서, 경사하강법의 효율성에 도움이되는 다음 알고리즘들을 살펴보자.
- **`Gradient Descent with Momentum (모멘텀 알고리즘 혹은 모멘텀이 있는 경사하강법)`**
- **`RMSProp`**
- **`Adam (Adaptive Moment Estimation)`**

<br>

그 다음, 학습 알고리즘의 속도를 높일 수 있는
- `Learning Decay (학습률 감쇠)`

을 알아보자.

마지막으로 
- `Local Optima` 의 문제점

을 알아본다.

 <br> <br>

## **`Exponentilally weighted average(지수가중평균)`**
매개변수를 바꿈으로써 약간씩 다른 효과를 얻게되고 이를 통해 가장 잘 작동하는 것을 찾는 것. <br>
통계학에서 나왔고 신경망 훈련에 사용되는 몇몇 최적화 알고리즘의 주요 구성요소이다.

### What is it?
런던의 1년 동안의 기후 데이터를 가지고 있다고 생각하자. 이 데이터의 **시간 흐름에 따른 온도의 이동 평균 흐름** 을 알아보고 싶다. <br> 
단순히 평균값을 구하면 될 것 같지만, 온도가 높고 낮음이 시간에 따라 크기 때문에 특정한 날짜의 기온 흐름을 계산할 때 과거의 데이터가 분석흐름을 망칠 것이다. <br>
그러므로 계산을 할 때 과거의 데이터에서는 영향을 덜 받게, 최근의 데이터에서는 영향을 많이 받을 수 있게 영향력을 달리할 수 있는 계산을 해야한다. 그래서 사람들은 최근 데이터에 과거와 현재 데이터의 가중치를 적용하여 흐름을 계산했고 그것을 단순이동평균이라고 불렀다. 근데 우리는 이것에서 더 나아가서 **과거의 영향력이 시간이 지남에 따라 지수적으로 감소** 할 수 있게끔 영향력을 죽여버리고 싶다. 즉, 지수적으로 감소하는 가중치를 적용하여 오래된 데이터일수록 현재의 경향에 더 적은 영향을 미치게 하는 것이다. 그것이 **`Exponentilally weighted average(지수가중평균)`** 이다.

<br><br>

### How can I do?
- 각 날의 데이터는 **Θ** 라고 생각하자. 
    - ex) Θ = 8월 12일의 온도
- 0~1의 값을 가지는 가중치 **β** 가 있다.
    - ex) β = 0.9
- **Vｔ**를 그 날의 경향이라고 생각하면 경향은 아래와 같이 구할 수 있다.

```math
V0 = 0
V1 = β*V0 + (1-β)*Θ1
V2 = β*V1 + (1-β)*Θ2-1
...

vt = β*vt-1 + (1-β)*Θt
```
β 를 0.9로 설정하면 10일동안의 기온 평균과 비슷해지고 빨간색 그래프를 얻게 된다.
![](https://github.com/matamatamong/img/blob/main/Visualizations/EWA.PNG?raw=true)

<br><br>

β 를 0.98 으로 설정하면 50일동안의 기온 평균과 비슷해지고 보라색 그래프를 얻게 된다.
![](https://github.com/matamatamong/img/blob/main/Visualizations/EWA-purple-graph.PNG?raw=true)

<br><br>

곡선이 부드러워졌다. 더 많은 날짜의 기온과 평균을 이용하였기때문이다. 큰 범위에서 기온 평균을 구하기 때문에 올바른 값에선 벗어날 수 밖에 없다.오래된 데이터에 가중치가 비교적 크게 잡혀있으므로 최신의 동향보다는 과거의 데이터에 비교적 더 많이 이끌리는 것이다. <br>
**즉, 지수가중평균은 β 가 커지면 더 느리게 적응한다.**

그럼 β가 낮을 때는 어떨까?
β가 0.5일 때는 2일의 기온평균만 구하는 것과 비슷해진다. 그래서 노란색 그래프를 얻게된다.
![](https://github.com/matamatamong/img/blob/main/Visualizations/EWA3.PNG?raw=true)

<br><br>

노이즈가 많지만 기온 변화에 빠르게 적응한다. 즉 최신의 경향을 더 많이 캐치할 수 있는 것이다.

<br>

### 머신러닝에선 어떻게 쓰일까?
앞에서 식을 봤을 때 V100까지 구한다고 생각해보자. <br>
V100을 구하려면 앞의 항이 필요할 것이다. 근데 앞의 항은 앞의 항에서 나왔고 앞의 항은 그 앞의 항에서 나오고.... 반복이다. <br>
그럼 굳이 앞의 항들을 각각 메모리에 다 저장할 필요가 있을까? <br>
V100로 오기까지의 앞선 항들의 값들은 모두 메모리에 저장할 필요없이 계속 메모리에 덮어씌우기만 하면 될 것이다. 그래서 아주 적은 메모리를 사용하게 되어 효율적이다. 평균을 계산하는 가장 정확하고 최선인 방법은 아니다. 그럴려면 그냥 명시적으로 10일이나 50일 구하고 10이나 50으로 나누는 것이 더 추정치가 좋다. 그런데 그럴려면 메모리가 많이 들고 복잡한 구현이 필요하다. **머신러닝은 많은 변수를 계산해야하기 때문에 많은 변수의 평균(흐름에 따른)을 내고싶으면 컴퓨터 계산비용과 메모리 효율이 좋은 이 `지수 가중 평균방법` 을 쓴다.**

<br><br>

## **`Bias Correction(편향보정)`**
앞에서 지수가중평균으로 평균을 효율적으로 계산하는 방법을 알아봤는데 편향보정이라고 불리는 기술적인 세부사항으로 평균을 더 정확하게 계산할 수 있다. <br>

앞에서 봤던 지수가중평균법을 실제로 계산해보자. <br>
β는 0.98로 지정했다.

```math
Vt = β*Vt-1 + (1-β)*Θt


V0 = 0
V1 = β*V0 + (1-β)*Θ1
V1 = 0 + 0.02*Θ1

V2 = 0.98*V1 + 0.02*Θ2
V2 = 0.98*(0.02*Θ1) + 0.02*Θ2
V2 = 0.0196*Θ1 + 0.02*Θ2
```
Θ1 과 Θ2가 양수라고 가정하면 V2는 Θ1 과 Θ2보다 훨씬 작아지게 된다. <br>
시작하는 추정값이 좋지 않아진다. 그래프를 보더라도 앞부분이 뚝 떨어져있다.<br>

<br> 이것이 더 나은 값이 될 수 있도록 수정해야한다. 물론 방법은 있다.
특히 추정초기단계를 더 정확하게 보장해주는 방법은 바로바로.... <br>

**Vｔ 대신 vｔ/1-β^t를 사용하는 것이다!**

<br>

현재 알고싶은 t가 2일 때, <br>
t=2 : 1-β^2 = 1-(0.98)² = 0.0396 <br>
이 되므로 둘째 날의 온도를 추정한 값은 다음과 같다.
```
V2 / 0.0396 = 0.0196*Θ1 + 0.02*Θ2 / 0.0396
```

t가 커질수록 B^t는 0에 가까워질 것이다. <br>
이것은 t가 커질수록 편향보정의 효과가 거의 없어진다는 뜻이다.
그래도 이것으로 초기단계의 학습에서는 더 나은 추정값을 가지게 될 것이다.

<br>

### 머신러닝에서는 어떻게 쓰일까?
사실 머신러닝에서 **지수가중평균법을 구현하는 경우에 대부분 편향 보정을 거의 사용하지않는다.** 지수가중평균법의 초기단계의 추정값이 좀 안좋긴 해도 그 구간만 지나면 괜찮으니까 기다렸다가 편향된 추정이 지나간 후부터 시작해버린다. 그래서 **초기단계의 편향이 신경쓰일 때 편향보정을 사용한다.**

<br><br>

## **`Gradient Descent with Momentum (모멘텀 알고리즘 혹은 모멘텀이 있는 경사하강법)`**
자, 이제 위의 것들을 알았으니 일반적인 경사하강법을 빠르게 동작하게할 수 있는 모멘텀 알고리즘을 배우자. <br>
이것은 **경사에 대해서 지수가중평균을 계산하고 그 값으로 가중치를 업데이트하는 것이다.**

<br><br>

### 그게 뭔데?
뭔지 알아보기 위해서 일반적인 경사하강법과 비교해보자. <br>
일반적인 경사하강법 혹은 미니배치경사하강법으로 비용함수를 최적화한다고 생각해보자. 비용함수의 등고선이 있으면 어떠한 최솟값을 향해 나아갈 것이다. 그 모양은 다음과 같다.
![](https://github.com/matamatamong/img/blob/main/Visualizations/gradientdescent-momentom1.PNG?raw=true)

<br><br>
학습률이 많은 단계를 거치게되면 최솟값으로 나아가면서 천천히 진동하는데, 이것은 다음과 같은 단점을 만든다.
- 속도를 느리게 만들고 더 큰 학습률을 사용하는 것을 막는다.
- 오버슈팅할 가능성도 있다.
- 수직축에서는 진동을 막기위해 학습이 느리게 일어나길 바라지만 수평축에서는 더 빠른 학습이 필요하다.

<br><br>
경사하강법은 진동도 너무 많고 학습률이 높을 수록 뭔가 정신을 못 차린다. <br>
그냥 쭉 최솟값으로 달려버리면 좋을 것 같은데 가능할까? 세상엔 똑똑한 사람들이 많다. 그들은 물리에서 아이디어를 얻어서 기울기가 0인 곳으로 천천히 내려가는 것 대신, 가속을 주어 최소인 곳을 찾게된다. <br> 
어떻게 해야할지 알아보자.
- 각각의 반복(t)에서 보편적인 dw와 db를 계산
- `지수 가중 평균법` 과 비슷한 식을 써서 `이동평균` 을 w에 대한 도함수로 계산한다.
```
# 지수 가중 평균법 : Vt = β*Vt-1 + (1-β)*Θt


Vdw = β*Vdw + (1-β)*dw
Vdb = β*Vdb + (1-β)*db
```

그리고 가중치를 이렇게 업데이트해준다.
```
α = learning_rate


w = w - α*Vdw
b = b - α*Vdb
```

이렇게 계산하면 
- 각각의 도함수는 경사를 내려갈 때 가속을 부여
- β의 값은 1보다 조금 작기 때문에 제한없이 빨라지는 것을 막음
- 지난번에 계산한 몇가지 도함수의 경사 평균을 구해서 수직방향의 진동이 0에 가까운 값으로 평균이 만들어지게된다.
- 반면에 수평방향에서 모든 도함수는 오른쪽을 가르키고 있기 때문에 수평방향의 평균은 꽤 큰 값을 가지게 된다.
- 결과적으로 수직방향을 훨씬 진동이 적어지고 수평방향에서는 더 빠르게 움직일 수 있다.


그 결과 빠르게 최솟값을 향해 내려갈 수 있게된다. <br>
이것이 모멘텀을 이용한 경사하강법이다. <br>
경사하강법이 모든 이전 단계를 독립적으로 취하는 대신에 이 알고리즘은 가속을 주고 **모멘텀** 을 제공할 수 있게 해주는 것이다. <br>
- **모멘텀** : 물리학에서, 물체가 한 방향으로 지속적으로 변동하려는 경향. 동력, 추진력, 여세 등등으로 이해할 수 있다.

<br> <br>


![By Liliy Jiang](https://miro.medium.com/max/800/1*zVi4ayX9u0MQQwa90CnxVg.gif) <br>
*보라색은 Momentum을, 하늘색은 일반적인 경사하강법을 시각화한 것 (By. Liliy Jiang. Thank You!!!)*
<br>
이름이 왜 이렇게 지어졌는지 이제 알겠다. :woman_facepalming: <br>
시각화에서도 보이지만, 모멘텀은 로컬 옵티마에 걸려도 빠져나올 기회가 있다.

<br><br>

### 구현 디테일 TIP
- β가 0.9일 때 매우 잘 작동한다.
- 앞서 말했듯이 편향보정은 초반 10번정도의 반복 뒤에는 보정효과가 없기 때문에 경사하강법이나 모멘텀을 구현할때 쓰는 사람 거의 없음.
- `Vdw = 0` 으로 초기화 하는 것은 `dw` 와 같이 차원이 0으로 이루어진 행렬이다.

<br><br>

## **`RMSProp(Root Mean Square Prop)`**
모멘텀을 이용해서 경사하강법을 빠르게 할 수 있었다. 그리고 경사하강법을 빠르게 할 수 있는 알고리즘이 또 있다. 바로 RMSProp이다. 이것은 도함수의 제곱을 지수가중평균하는 것이다. <br>
✋ 잠깐 여기서 다시 기억해보자. 경사하강법에서 빨리 최솟값에 도달하려면 
- 진동하고있는 수직의 학습률은 느리게
- 최솟값을 향해 나아가야하는 수평방향의 학습률은 빠르게

해야한다.

### 어떻게 작동하는지?

```
# 지수 가중 평균법 : Vt = β*Vt-1 + (1-β)*Θt


Sdw = β*Sdw + (1-β)*dw²
Sdb = β*Sdb + (1-β)*db²
```
그리고 가중치를 이렇게 업데이트해준다.
```
α = learning_rate


w = w - α * dw/√Sdw
b = b - α * db/√Sdb
```
이렇게 계산해주면
- dw가 상대적으로 작으므로 w를 업데이트할 때 상대적으로 작은 숫자를 나눠진다.
    - 수평방향에서의 업데이트는 증감한다.
- db가 상대적으로 크므로 b를 업데이트할 때 큰 숫자로 나눠진다.
    - 수직방향에서의 업데이트는 감소한다.
- 수직방향의 진동은 감소되고 수평방향은 계속 나아가게 된다.
<br>

이렇게 빠르게 학습하고 수직 방향으로 발산하지않게되는 것이다!!

### Tip
- 구현할 때 알고리즘이 0으로 나눠지지않도록 조심하자. `Sdw`의 제곱근이 0에 가깝다면 값이 폭.발 할 수 있다.
    - `√Sdw` 에 10^(-8) 더해주자.


아직 더 멋진 것이 남아있다
바로 이 RMSProp과 앞서봤던 Momentum을 결합한 멋진 것을 만들어낼것이다.

<br><br>

## **`Adam (Adaptive Moment Estimation)`**
**Momentum** 과 **RMSProp** 을 합친 알고리즘인 **Adam**

### 어떻게 작동하는지?

```
# 지수 가중 평균법 : Vt = β*Vt-1 + (1-β)*Θt


# Vdw = 0, Sdw = 0, Vdb = 0, Sdb = 0 로 초기화

# 모멘텀을 구현해준다. (RMSProp과 구분하기위해 β1으로 표기)
Vdw = β1*Sdw + (1-β1)*dw
Vdb = β1*Sdb + (1-β1)*db

# RMSProp을 구현해준다. (모멘텀과 구분하기위해 β2으로 표기)
Sdw = β2*Sdw + (1-β2)*dw²
Sdb = β2*Sdb + (1-β2)*db²

# 각각 편향보정을 해준다.
V^corrected dw = Vdw / (1-β^t)
V^corrected db = Vdb / (1-β^t)

S^corrected dw = Sdw / (1-β2^t)
S^corrected db = Sdb / (1-β2^t)
```
그리고 가중치를 이렇게 업데이트해준다.
```
α = learning_rate


w := w - α * V^correcte dw / √S^corrected dw + ε
b := b - α * V^correcte db / √S^corrected db + ε

```
*ε : 0이되어 값이 폭발하는 것을 막기위해 더해주는 10^-8 값*

<br>

이렇게 계산하면 
- **모멘텀** 의 속도
- **RMSProp** 의 다양한 방향으로 그라디언트를 적용하는 능력

이 둘을 합친 효과를 낸다.

<br>

신경망에 잘 작동한다는 것이 증명되어 일반적으로 많이 쓰이는 알고리즘이다.

<br>

### **Adam** 의 Hyperparameters
- α : learning_rate인 α는 매우 중요하기도하고 보정될 필요가 있기 때문에 다양한 값을 시도해서 잘 맞는 것을 찾아야한다.
- β1 : 모멘텀에 관한 항인 β1(이동평균, 가중평균, 즉 도함수의 평균을 계산)은 기본적으로 0.9가 좋다.
- β2 : dw^2와 db^2의 이동가중평균인 β2는 Adam 논문 저자가 0.999를 추천했다.
- ε : 0이되어 값이 폭발하는 것을 막는 ε는 값이 크게 상관없지만 Adam 논문 저자가 10^(-8) 을 추천했다. 보통 이 값을 설정하지 않아도 전체 성능에는 영향이 없다.

<br><br><br>

##  **`Learning Decay (학습률 감쇠)`**
학습 알고리즘의 속도를 높이는 한가지 방법은 시간에 따라 학습률을 천천히 하는 것이다. 이를  `Learning Decay (학습률 감쇠)` 라고 부른다. 

<br>

### 왜 학습률이 감쇠해야함?
배치를 잘게 자른 미니배치 경사하강법을 사용한다고 생각해보자. <br> 
- 고정된 α (learning_rate) 를 사용할 경우 최솟값에 정확하게 수렴하지 않고 주변을 돌아다니게 된다.
- α를 줄임으로써 학습 초기 단계에서는 훨씬 큰 스텝으로 진행하다가 학습이 수렴할수록 작은 스텝으로 진행하게 할 수 있다.
- 이 경우 단계마다 진행 정도가 작아지면서 **주변을 돌아다니는 대신에 최솟값 주변에서 밀집하여 진동할 것이다.**


### 구현
- 1 epoch 는 배치 한번을 통과하는 단위다.
- α0 는 초기학습률이다.

1. Learning Decay
```
α = (1 / 1 + decay_rate * epoch) * α0
```



2. 그 외 사람들이 사용하는 식.
```
# 2-1
α = 0.95 * α0  (α가 1보다 작은 값을 가져서 빠르게 감소함)

# 2-2
α = (k / √epoch_num) * α0

# 2-3
α = (k / √t) * α0

# 2-4
이산계단

```

이렇게 계산하면 학습률은 점차 감소한다. <br>
응은 학습률 감소를 잘 안 쓰는는 것 같다. 왜일까

<br><br>

## **`Local Optima의 문제점`**
이제까지 알아본 최적화 알고리즘을 잘못쓰면 로컬 옵티마에 빠진다. <br>
그런데 충분히 큰 신경망을 학습시킨다면 지역 최적값에 빠질일은 잘 없다. 
문제는 고차원 공간이 될 수록 지역 최적값보다는 안장점이 되기가 쉽다. <br>

![](https://github.com/matamatamong/img/blob/main/Visualizations/Saddlepoint.PNG?raw=true)
*앤드류 응의 안장점 설명이 귀여워서 가지고와봤다.*

<br><br>

진짜 문제는 이 안장형태에서 나올 수 있는 **`Plateaus(안정지대)`** 이다. 뭐나면 아주 오랫동안 미분값을 0에 가깝게 유지하는 지역을 말하는데 이게 학습을 엄청 지연시킨다. <br>
이 문제를 모멘텀이나 RMSProp, Adam 등의 알고리즘으로 도움을 받을 수 있을 것이다. (못 할 수도 있겠지) <br>

<br><br><br><br>

# Kaggle

## 대충 정리
타이타닉 노트북 세개정도 필사해보고 `binary classification` 대충 어떻게 돌아가는지 복습
- 데이터 확인
    - 주어진 train, test 데이터 확인 및 target 데이터확인
    - 각 Feature 들의 value 확인
    - 각 Feature 들의 Null 확인
- 데이터 엔지니어링
    - 비슷한 Feature 합치기
    - NULL 채우기
    - 데이터 숫자로 맵핑
    - 쓸모없는 Feature Drop
    
복습 여기까지 함.


## Normalized Gini-Coefficient
정렬된 실제 값이 스왑 수로 측정된 임의의 상태에서 얼마나 멀리 떨어져 있는지를 측정하는 방법. Porto 대회의 채점방식으로 쓰였다. 

<br><br><br><br>
* * * 
**참고** <br>
[[머신러닝] 지수 가중 평균 (Exponentially Weighted Averages) ](https://m.blog.naver.com/nilsine11202/221903150950) <br>
**[A Visual Explanation of Gradient Descent Methods (Momentum, AdaGrad, RMSProp, Adam) By.Liliy Jiang](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)**
