# Tensorflow 자격증
## Sequence models
## Time series에 DNN 적용하기

먼저, feature과 label로 나눠서 학습해야한다.
- `feature` : series 의 여러 값, window size라고도 한다.
- `label` : series의 다음 값
- e.g) time이 30day이면 30개의 값을 feature로 사용하고 다음 값을 label로 정한다. 그리고 time마다 하나의 label에 30개의 feature가 match되도록 신경망을 학습시킨다.

```python
dataset = tf.data.Dataset.range(10)

for val in dataset:
    print(val.numpy())
```
```python
0
1
2
3
4
5
6
7
8
9

```

- tfds 를 사용하여 대충 데이터를 불러오면 위와 같을 것이다. 이것을 살짝 변형해보자.

<br>

```python
dataset = tf.data.Datset.range(10)
dataset = dataset.window(5, shift=1)

for window_dataset in dataset:
    for val in window_dataset:
        print(val.numpy(), end=' ')
    print()
```
```python
0 1 2 3 4
1 2 3 4 5
2 3 4 5 6
3 4 5 6 7
4 5 6 7 8
5 6 7 8 9
6 7 8 9
7 8 9
8 9
9

```
- `dataset.window` 를 사용하여 데이터를 확장시켰다.
    - widow로 정해준것 처럼 5번째에서 끊고, 1씩 이동하고 있는 것을 볼 수 있다.

<br>

```python
dataset = tf.data.Datset.range(10)
dataset = dataset.window(5, shift=1, drop_remainder=True)

for window_dataset in dataset:
    for val in window_dataset:
        print(val.numpy(), end=' ')
    print()
```
```python
0 1 2 3 4
1 2 3 4 5
2 3 4 5 6
3 4 5 6 7
4 5 6 7 8
5 6 7 8 9
```

- `drop_remainder` 를 True로 설정하면 window에 해당하지 않는 남은 데이터를 잘라내줄 것이다.

<br>

```python
dataset = tf.data.Datset.range(10)
dataset = dataset.window(5, shift=1, drop_remainder=True)
dataset = dataset.flat_map(lambda window: window.batch(5))

for window in dataset:
    print(window.numpy())
```
```python
[0 1 2 3 4]
[1 2 3 4 5]
[2 3 4 5 6]
[3 4 5 6 7]
[4 5 6 7 8]
[5 6 7 8 9]
```
- 신경망에 넣어주기 위해 `window.numpy()`를 사용하여 numpy list로 손쉽게 바꿔줌.
- `flat_map` 을 사용하여 데이터를 다루기 쉽게 flat하게 만들어준다.

<br>

```python
dataset = tf.data.Datset.range(10)
dataset = dataset.window(5, shift=1, drop_remainder=True)
dataset = dataset.flat_map(lambda window: window.batch(5))
dataset = dataset.map(lambda window: (window[:-1], window[-1:]))
dataset = dataset.shuffle(buffer_size=10)
dataset = dataset.batch(2).prefetch(1)

for x,y in dataset:
    print('x = ', x.numpy())
    print('y = ', y.numpy())
```
```python
x = [[4 5 6 7] [1 2 3 4]]
y = [[8] [5]]
x = [[3 4 5 6] [2 3 4 5]]
y = [[7] [6]]
x = [[5 6 7 8] [0 1 2 3]]
y = [[9] [4]]
```

- `feature`과 `label`로 쪼갰다.
- `dataset.shuffle` 을 이용하여 데이터를 섞어준다. bias를 막아준다.
- `dataset.batch` 를 이용하여 배치 2로 설정해주어서 3개의 x,y를 얻었다.

<br><br>

## window 적용한 데이터 신경망에 먹이기

```python
def windowed_dataset(series, window_size, batch_size, suffle_buffer):
    dataset = tf.data.Dataset.from_tensor_slices(series)
    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda window: windwo.batch(window_size + 1))
    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))
    dataset = dataset.batch(batch_size).prefetch(1)

    return dataset
```
- 먼저 tfds를 tensor slice로 이용해서 불러온 뒤 원하는데로 window해주는 메소드를 작성해보자.
    - shuffle_buffer : 일반 shuffle보다 빠르게 진행할 수 있다. 

<br>

```python
split_time = 1000

time_train = time[:split_time]
x_train = series[:split_time]

time_valid = time[split_time:]
x_valid = series[split_time:]
```
- 학습하기 전에 데이터셋을 train, valid 셋으로 나눠준다.

<br>

### 간단한 회귀

```python
window_size = 20
batch_size = 32
shuffle_buffer_size = 1000

dataset = windowed_dataset(series, window_size, batch_size, shuffle_buffer_size)
l0 = tf.keras.layers.Dense(1, input_shape=[window_size])

model = tf.keras.models.Sequenctial([l0])

model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))
model.fit(dataset, epochs=100, verbose=0)

print('Layer weights {}'.format(l0.get_weights()))
```

- **optimizer의 파라미터를 조정해서 정확도와 빠르기를 개선할 수 있다.**

<br><br>

자, 지금까지 봐온 값들은 딱 그 시점에서의 수평축 time series에 대한 값들이다. 이제 거기에 맞는 y를 예측할 차례다.

<br><br>

### Prediction

```python
print(series[1:21])

model.predict(series[1:21][np.newaxis])
```
```python
[49.35275  53.314735  57.711823  48.934444  48.931244 .... 총 20개....]

array([[49.08478]], dtype=float32)
```

- `np.newaxis` 는 모델이 사용했던 shape에 맞춰서 reshape 해준다.
- 딱 그 시점의 다음 값을 예측해준다.

<br>

```python
forecast = []

for time in range(len(series) - window_size):
    forecast.append(model.predict(series[time:time + window_size][np.newaxis]))

forecast = forecast[split_time - window_size:]
results = np.array(forecast)[:, 0, 0]

tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()
```
- 모든 time series의 20개의 point들에 대하여(window size를 20으로 했었으니까) 예측을 해주려면 위와 같이 해준다.
- 트레이닝에 사용했던 split_time의 이후를 예측한다.

<br>

```python
plt.figure(figsize=(10, 6))

plot_series(time_valid, x_valid)
plot_series(time_valid, results)
```

<br><br>

## DNN에 적용

```python
dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer)

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, input_shape=[window_size], activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))
model.fit(dataset, epochs=100, verbose=0)
```

- 레이어를 세 개 가지는 간단한 DNN을 적용해보자.
- 싱글 레이어를 사용했던 것보다 accuracy를 더 좋게 할 수 있다.

<br>

## lr_schedule 로 학습률 조정

```python
dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer)

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(10, input_shape=[window_size], activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1)
])

lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20)
)
optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)

model.compile(loss='mse', optimizer=optimizer)

history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])
```
``` python
lrs = 1e-8 * 10**(np.arrange(100) / 20)
plt.semilogx(lrs, history.history['loss'])
plt.axis([1e-8, 1e-3, 0, 300])
```

- optimizer의 learning rate를 epoch 횟수에 따라 변경할 수 있게 하면 정확도를 끌어올릴 수 있다.
- 두번째 방식으로 y는 epoch에 대한 loss, x는 learning-rate를 차트를 그릴 수 있다.
- 차트를 그려서 loss가 가장 낮은 learning-rate를 설정하자. 
    - optimizer = tf.keras.optimizers.SGD(lr=`7e-8`, momentum=0.9)
        - learning rate 10^-7 에서 loss가 적다면 그렇게 설정해주고 간단한 DNN을 이용해서 다시 학습시켜준다. 

<br>

```python
loss = history.history['loss']
epochs = range(len(acc))
plot.plot(epochs, loss, 'b', label='Training Loss')
plot.show()
```
- 위의 코드로 학습할 때 loss를 볼 수 관찰할 수 있다.

<br><br>

## time series 에 LSTM 적용하기

**지금까지 시퀀스를 모두 무시하였다면 이제 RNN을 통하여 time series의 시퀀스를 적용하여 학습해보자.**

<br>

- 기존 RNN과 살짝 다른 것은 input_windows의 shape가 [batch size, #time stpes, #dims] 차원을 가지고 있는 것이다.
    - `dims` : 각각의 타임 스텝의 인풋 차원이다.
        - univariate는 1, multi는 많겠지.
    - e.g) window_size가 30이고 batch size가 4인 univariate이면 Input은 [4, 30, 1] 일 것이다.
    - e.g) 3개의 뉴런이 쌓여있다면 위의 input의 Output은 [4, 30, 3]

<br>

```python
model = keras.models.Sequenctial([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20),
    keras.layers.Denss(1)
])
```
- RNN을 하나 더 쌓은 단순한 RNN이다. 
- `input_shape` 를 잘 보자. 
    - tensorflow는 첫번째 차원이 batch size라고 예측하기 때문에 어느 크기를 써도 상관없기 때문에 정의하지않아도 된다. (None)
    - univariate time series를 사용하기 때문에 마지막 차원은 1이된다.(1)
- return_sequences=True 를 첫번째 레이어에만 사용하면 아웃풋이 최종 하나가 될 것이고 두번째 레이어에도 사용하면 아웃풋은 여러개가 될 것이다.

<br>

### Lambda layers

```python
model = keras.models.Sequenctial([
    keras.layers.Lambda(lambda x : tf.expand_dims(x, axis=-1), input_shape=[None]),
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20),
    keras.layers.Denss(1)
    keras.layers.Lambda(lambda x: x * 100.0)
])
```
- lambda 레이어를 덧붙여보자.
    - `lambda` : TensorFlow kares의 기능을 효과적으로 확장하기 위해 임의의 작업을 수행할 수 있게 해주는 레이어이며 모델 정의 자체 내에서 이를 수행할 수 있게 해준다.
- 첫번째 lambda 레이어는 손쉽게 차원을 확장시켜준다.
    - windowed_dataset 에서 반환하는 값은 2차원인데 우리는 앞에서 배운것처럼 [batch size, #time stpes, #dims] 차원이 필요하다. 그래서 -1을 이용하여 차원을 expand해줌
- 두번째 lambda 레이어는 lambda를 이용하여 출력을 스케일 업 해준다. 이렇게 해주면 학습이 용이해진다.
    - RNN의 디폴트 activation은 tanh이라서.

<br>

### learning rate 유동적으로 조절하기

```python
train_set = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)

model = tf.keras.models.Sequential([
    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1, input_shape=[None])),
    tf.keras.layers.SimpleRNN(40, return_sequences=True),
    tf.keras.layers.SimpleRNN(40),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x: x * 100.0)
])

lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))

optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)

model.compile(
    loss=tf.keras.losses.Huber(),
    optimizer=optimizer,
    metrics=['mae']
)

history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])
```

- LearningRateScheduler 로 epoch당 learning rate를 조절했다.
- Huber() 라는 새로운 loss 함수를 사용.

<br>

하지만 아직까지 괜찮은 성능이 나오진 않고있다.

<br>

## LSTM

```python
tf.keras.backend.clear_session()
dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

model = tf.keras.models.Sequenctial([
    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x:x * 100.0)
])

model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))
model.fit(dataset, epochs=100, verbose=0)
```

<br><br>

## Convolution을 더해주자.


```python

model = tf.keras.models.Sequenctial([
    tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding='causal', activation='relu', input_shape=[None, 1]),
    tf.keras.layers.LSTM(32, return_sequences=True),
    tf.keras.layers.LSTM(32, return_sequences=True),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x:x * 200.0)
])

optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])

model.fit(dataset, epochs=500)
```
```python
def windowed_dataset(series, window_size, batch_size, suffle_buffer):
    series = tf.expand_dims(series, axis=-1)

    dataset = tf.data.Dataset.from_tensor_slices(series)
    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda window: windwo.batch(window_size + 1))
    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))
    dataset = dataset.batch(batch_size).prefetch(1)

    return dataset
```

- input_shape를 확장해주는 첫번째 lambda 레이어가 사라졌다. 그래서 windowed_dataset 헬퍼 함수를 두번째 코드블록과 같이 조금 수정해줄 필요가 있다.
- 이렇게 해주면 지금까지 해왔던 방식 중 가장 좋은 결과를 내지만 여전히 노이즈가 많이 껴있는 피크에서의 예측은 아쉬울 수 있다.

### 성능 높이기

- **학습을 조금 더 오래해주자.**
- **`tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))` 으로 변경**
위 두 방법은 오버피팅의 문제가 있다.
- `batch size` 를 알맞게 조절해보자.
- 데이어에 맞게 window_size, split_time을 조정해보자.
- tf.keras.layers.Dense의 유닛을 조정해보자.
- optimizer의 learning rate를 조정해보자.
- `filter` 수를 조정해보자.
    - 이러면 Dense 숫자도 filter와 같이 따라가주는게 좋음


<br><br>

# Full Stack Deep Learning 
## Setup Lab
앞으로 사용할 코드를 쭉 살펴봤다. Pytorch lightning을 더 사용했다.
- [Pytorch lightning](https://pytorch-lightning.rtfd.io/en/latest/) : Pytorch 추상화를 더 잘 해놓은 라이브러리.

<br>

## 구조

```
├── readme.md
├── text_recognizer
│   ├── data
│   │   ├── base_data_module.py
│   │   ├── __init__.py
│   │   ├── mnist.py
│   │   └── util.py
│   ├── __init__.py
│   ├── lit_models
│   │   ├── base.py
│   │   └── __init__.py
│   ├── models
│   │   ├── __init__.py
│   │   └── mlp.py
│   └── util.py
└── training
    ├── __init__.py
    └── run_experiment.py
```

- **text_recognizer** : 개발해야하며 어떻게든 배포해야하는 파이썬 라이브러리
    - **data** : 데이터에 관련된 모든 코드
        - **base_data_module.py**
            - **`def` load_and_print_info**
            - **`def` _download_raw_dataset**
            - **`class` BaseDataModule** : [python.lightningDataMudule](https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html) (데이터를 처리하는 데 필요한 모든 단계를 캡슐화하는 공유 가능하고 재사용 가능한 클래스.)을 param으로 받음. data를 다운로드, batch_size, dims설정, train/test split 등을 수행.
            - **mnist.py** : mnist 관련 데이터 setup
            - **util.py** : string->labels 등 시퀀스에 관련된 데이터를 변환하거나 쪼개는 것을 수행.
    - **lit_models**
        - **base.py**
            - **`class` Accuracy** : accuracy 관련 버그 때문에 그 부분 손 본 클래스
            - **`class` BaseLitModel** : PyTorch 모듈로 초기화해야 하는 일반 PyTorch-Lightning 클래스. add_to_argparse로 arg 할당하고 log 남기는 등..학습 알고리즘의 세부 사항 지정.
    - **models**
        - **mlp.py** : MLP 를 수행.
    - **util.py** : text_recognizer 모듈의 유틸리티 기능을 담당.
- **training** : text_recognizer를 개발하기 위한 지원 코드
    - **logs**
    - **run_experiment.py** : 학습 실행 프레임워크 담당. `lit_models` 로 초기화하고 `models` 가동!

<br>
터미널에서 바로 --를 이용해서 arg를 할당하고 바로 트레이닝해보고 파라미터를 이것저것 바꿔서 학습해보았다.

<br>
오늘 직접 코드를 가지고 하이퍼파라미터튜닝을 많이 해봐서 재밌었다.