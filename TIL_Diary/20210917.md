# Tensorflow 자격증
## Word Embeddings
단어와 그와 관련된 단어를 벡터화하여 다차원 공간에 군집화해놓는 word embedding을 알아보자. <br>

```python
!pip install -q tensorflow-datasets
```
- colab에서는 위의 코드로 데이터를 받아야한다.

<br>

```python
import tensorflow_datasets as tfds

imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)
```
- tensorflow_datasets을 이용하여 미리 제공되는 imdb영화 리뷰를 받자.


<br>

```python
import numpy as np

train_data, test_data = imdb['train'], imdb['test']
```
- `train`과 `test` 데이터를 쪼개서 할당.

<br>

```python
training_sentences = []
training_labels = []

testing_sentences = []
testing_labels = []

# str(s.tonumpy()) is needed in python3 instead of just s.numpy()
for s, l in train_data:
    training_sentences.append(str(s.tonumply()))
    training_labels.append(l.numpy())

for s, l in test_data:
    testing_sentences.append(str(s.tonumply()))
    testing_labels.append(l.numpy())


### 혹은 다음과 같이.
for s,l in train_data:
  training_sentences.append(s.numpy().decode('utf8'))
  training_labels.append(l.numpy())
  
for s,l in test_data:
  testing_sentences.append(s.numpy().decode('utf8'))
  testing_labels.append(l.numpy())
###

training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)
```

- label과 sequence를 분리하여 할당.
- numpy array로 바꿔줌

<br>

```python
import json

with open('json 경로', 'r') as f:
    datastore = json.load(f)

sentences = []
labels = []

for item in datastore:
    sentences.append(item['sentence 아이템'])
    labels.append(item['label 아이템'])

training_size = 20000

training_sentences = sentences[0:training_size]
testing_sentences = sentences[training_size:]
training_labels = labels[0:trainig_size]
testing_labels = labels[training_size:]
```
- 참고로 json을 열 때는 위와 같이 열어서 쪼개줄 수 있다.

<br>

```python
vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = '<OOV>'

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length)

```
- tokenizing을 해주고 sequence를 뽑아낸다.
- test 셋에도 똑같이 해준다.
    - training set을 적용한 tokenizer를 사용하기 때문에 testing_sequences는 많은 oov_token을 가질 수 밖에 없다. 모델이 보지않은 데이터이므로 이는 나중에 테스트 환경을 좋게만들어줌.

<br>

```python
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):
    return ' '.join([revers_word_index.get(i, '?') for i in text])

print(decode_review(padded[3]))
print(training_sentences[3])
```
- 인코딩된 리뷰를 확인하려면 위와 같이 확인해본다.

<br>

```python
model = tf.keras.Sequenctial([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

```
- 모델을 만들 때 tensorflow에서 제공하는 Embedding을 해준다.
- Embedding된 값은 2차원 어레이가 될 것이라 Flatten()해준다.
    - GlobalAveragePooling1D() 를 사용해도 된다. 좀 더 심플하고 더 빠를 수 있다.

<br>

```python
num_epochs = 10
model.fit(
    padded, 
    training_labels_final, 
    epochs=num_epochs,
    validation_data=(testing_padded, testing_labels_final)
    )
```
- model을 학습시켜준다.

<br>

```python
e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape) # shape: (vocab_size, embedding_dim)
```
- 모델의 첫번째 레이어는 embedding을 뜻하며 이것의 output을 보면 (vocab_size, embedding_dim) 으로 되어있는 것을 확인할 수 있다. 이는 vocab_size 만큼의 단어를 사용할 수 있고, 이것이 embedding_dim만큼의 차원을 가진 것이다.

<br>

```python
import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')

for word_num in range(1, vocab_size):
    word = revers_word_index[word_num]
    embeddings = weights[word_num]
    out_m.write(word + '\n')
    out_v.write('\t'.join([str(x) for x in embeddings]) + '\n')

out_v.close()
out_m.close()
```
- `vects.tsv`, `meta.tsv` 를 추출할 수 있다.

```python
try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download('vecs.tsv')
  files.download('meta.tsv')
```
- colab에서는 위와 같이 다운로드 할 수 있음.

<br>


```python
import matplotlib.pyplot as plt

def plot_graphs(history, string):
    plt.plot(histroy.histroy[string])
    plt.plot(history.histroy['val_' + string])
    plt.xlabel('Epochs')
    plt.ylabel(string)
    plt.legend([string, 'val_'+string])
    plt.show()

plot_graphs(history, 'acc')
plot_graphs(histroy, 'loss')
```
- 위와 같이 결과를 plot.


<br><br>

## 결과가 이상함..!
text 데이터를 다룰 때 정확도는 올라가는데 validation 정확도는 떨어지는 것을 자주 볼 수있다. 이럴 땐 하이퍼파라미터튜닝이 필요하겠다.

<br>

```python
vocab_size = 1000 # 10,000이었음
max_length = 16 # 32였음
```
- `vocab_size` 와 `max_length` 를 줄여본다.
- 이 경우에 loss 는 올라가지않고 평탄하겠지만 그럼에도 테스트 정확도는 높지않다.

<br>

```python
vocab_size = 1000
embedding_dim = 32 # 16 이었음
max_length = 16
```
- `emedding_dim` 을 조금 올려볼 수도 있다.
- 이 경우엔 별로 큰 변화를 보지 못한다.

<br>

**아무튼 이러한 시도를 통해서 loss가 증가하지않고 정확도는 증가하는 파라미터를 계속해서 찾아내야한다.**

<br><br>

## Pre-tokenized 데이터 사용하기

```
https://github.com/tensorflow/datasets/tree/master/docs/catalog
```
- 위의 링크에서 tensorflow에서 제공하는 pre-tokenized 데이터를 사용할 수 있다.

<br>

```python
import tensorflow_datasets as tfds

imdb, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)
```
- tfds를 이용하여 pre-tokenized imdb데이터를 가져오자
    - 텐서플로우 버전이 2.x 를 사용하자.

<br>

```python
train_data, test_data = imdb['train'], imdb['test']

# sub-word tokenizer에 접근하고 싶다면 아래 코드를 사용한다.
tokenizer = info.features['text'].encdoder

print(tokenizer.subwords)

BUFFER_SIZE = 10000
BATCH_SIZE = 64

train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))
test_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))
```
```python
['the_', ',', '.', 'a_', 'and_', 'of_', 'to_', 's_', 'is_' ...]
```
- train, test 데이터를 분리한다.
- pre-trained sub-words 토크나이저를 생성한다.

<br>

```python
sample_string = 'Tensorflow, from basics to mastery'

tokenized_string = tokenizer.encode(sample_string)
print('Tokenized string is {}'.format(tokenized_string))

original_string = tokenizer.decode(tokenized_string)
print('The original string:{}'.format(original_stirng))
```
```python
'Tokenized string is [6307, 2327, 4043, 2120, 2, 48, 4249, 4429, 7, 2652, 8050]'
'The original string : Tensorflow from basics to mastery'
```

- 위의 코드를 통해 인코딩과 디코딩이 어떻게 이루어지는지 알 수 있다.
    - `encode` 메소드를 통해 간단하게 encode할 수 있다.
    - `decode` 메소드를 통해 간단하게 decode할 수 있다.

<br>

```python
model = tf.keras.Sequenctial([
    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```
- pre-trained sub-words 토크나이저를 사용하면 Flatten이 쉽지않기 때문에 `GlobalAveragePooling1D` 을 이용해준다.

<br>

```python
for ts in tokenized_string:
  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))
```

- 위의 코드로 sub-word를 확인해보자. 이 sub-word 만으로는 모델 성능의 결과가 좋지않을 것이다. 단어자체만 바라보고 단어의 위치를 무시하여 시퀀스를 제대로 다루고있지않기 때문이다.

그래서 RNN을 이용할 것임.

<br><br>

## Sequence models

시퀀스를 잘 파악할 수 있도록 RNN모델들을 사용해보자.

## LSTM

cell state를 이용한 LSTM을 코딩해보자.

<br>

```python
model = tf.keras.Sequenctial([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```
```python
model = tf.keras.Sequenctial([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```
- 두번째와 같이 LSTM을 stacking할 수 있다. stack하면 결과가 더 좋을 수도있다.
- 하지만 간단한 모델이라 overfitting이 심하게 되는 것을 볼 수 있다.

<br><br>

## 

```python
model = tf.keras.Sequenctial([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),
    tf.keras.layers.GlobalMaxPooling1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```
- Convolution을 적용할 수 있다.
    - 5개의 단어들마다 128개의 필터가 적용되게 세팅했다.
- 하지만 간단한 모델이라 여전히 overfitting이 심하게 되는 것을 볼 수 있다.

<br>

```python
model = tf.keras.Sequenctial([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```
- GRU 를 사용할 수  있다.
- 하지만 간단한 모델이라 여전히 overfitting이 심하게 되는 것을 볼 수 있다.

<br><br>

## 문장 생성하기

```python
tokenizer = Tokenizer()

data = 'In the town of Athy one Jeremy Lanigan \n Battered ...'
corpus = data.lower().split('\n')

tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1
```

<br>

```python
input_sequences = []

for line in corpus:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)

max_sequence_len = max([len(x) for x in input_sequences])

# label을 잘 뽑아내기 위해서 padding을 pre로.
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))
```

이렇게 해주면 패딩이 적용된 input sequences는 다음과 같이 된다.
```python
[0 0 0 0 0 0 0 0 0 0 4 2]
[0 0 0 0 0 0 0 0 0 4 2 66]
[0 0 0 0 0 0 0 0 4 2 66 8]
[0 0 0 0 0 0 0 4 2 66 8 67]
[0 0 0 0 0 0 4 2 66 8 67 68]
[0 0 0 0 0 4 2 66 8 67 68 69]
[0 0 0 0 4 2 66 8 67 68 69 70]
```
- 각 list의 마지막 요소 하나만 label로 정하고 앞에 있는 요소들은 전부 input으로 정한다.

<br>

```python
xs = input_sequences[:, :-1]
labels = input_sequences[:, -1]

ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)
```
- 위에서 봤던 형식으로ㅏ 시퀀스를 x와 y로 나눠준다.
- classification을 하고싶으니까 라벨들을 원핫인코딩을 해준다.
     - np array의 y가 해당하는 인덱스는 1이 되고 나머지는 다 0.

<br>

```python
 model = Sequential()
  model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))
  model.add(Bidirectional(LSTM(20)))
  model.add(Dense(total_words, activation='softmax'))
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  history = model.fit(xs, ys, epochs=500, verbose=1)
```

<br>

지금까지의 방식으로는 다음으로 오는 단어들이 길어지면 길어질 수록 예측의 퀄리티가 낮아지는 것을 볼 수 있다. 단어가 반복되는 현상이 있을 수 있는 것이다. 각 단어가 기존 구문과 일치할 확률은 더 많은 단어를 만들수록 낮아지기 때문이다.

<br>

```python
model = Sequenctial()
model.add(Embedding(total_words, 100, input_length=max_sequence_len-1)) # 100 을 바꿔서 결과를 바꿔보자.
model.add(Bidirectional(LSTM(150))) # 150을 바꿔서 결과를 바꿔보자.
model.add(Dense(total_words, activation='softmax'))
adam = Adam(lr=0.01) # 러닝레이트를 바꿔보자. 결과에 크게 영향을 미칠 것이다.
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
history = model.fit(xs, ys, epochs=100, verbose=1) # epochs를 바꿔보자
```


# Sequence 데이터 다루기
## Time series (시계열)
### Train, validation and test sets
- Fixed Partitioning
    - 타임 시리즈가 계절성을 가지면 각 패턴에 따라 Train, validation, test set 으로 나눈다.
    - train과 valid를 비교한 뒤 valid까지 학습시키고, 그것을 test와 비교한 뒤,  test까지 학습시킨다.
        - 테스트 데이터가 현재 시점에서 가장 가까운 데이터이기 때문이다.
- Roll-Forward Partitioning
    - training period를 짧게 시작하여 점차적으로 늘린다. 각 반복에서 training period 동안 모델을 학습하고 이것을 valid period의 다음을 예측하는데 쓴다.

<br>

### Metrics
- errors = forecasts - actual
- **mse = np.square(errors).mean()**
    - 큰 오류가 잠재적으로 위험하고 작은 오류보다 비용이 훨씬 많이 든다면 mse를 선호
- rmse = np.sqrt(mse)
- **mae = np.abs(errors).mean()**
    - 이익이나 손실이 오차의 크기에 비례할 때 선호
- mape = np.abs(errors / x_valid).mean()

### Moving average(이동 평균) and differencing(차분)
- 이동 평균을 내서 그래프를 스무스하게 만드는 것은 좋아보이지만 많은 오차를 낼 수 있다. 
- 그래서 차분이라는 기술을 사용하여 시계열에서 추세(trend)와 계절성을 제거, 시계열 자체를 연구하는 대신 시간 T의 값과 이전 기간의 값의 차이를 연구한다.
- **즉, 이동평균을 사용하여 시계열을 예측하고 시간 T에서 365를 뺀 값을 다시 추가**
    - 이 때, 이동평균이 노이즈를 많이 제거했음에도 불구호가 차분을 했을 때 노이즈가 여전히 많은 것을 알 수 있다. 이 노이즈는 추가했던 이전기간의 값에서 나오는 것이다. 그렇기 때문에 이동 평균을 이전 노이즈에 사용해서 없애버리면 좀 더 스무스한 그래프를 얻을 수 있다.

<br><br>

### Trailing versus centered windows
- centered windows 를 사용하는 것이 trailing windows를 사용하는 것보다 더 정확하다.
- 그러나 미래의 값을 모르기 때문에 centered windows를 사용하여 현재의 값들을 스무스하게 만들 수 없다.
- 그러나 과거의 값들을 스무스하게 만들 땐 centered windows를 사용할 수 있다.