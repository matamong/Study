# 코딩테스트 문제풀이
## <이것이 취업을 위한 코딩 테스트다> 책
- 다익스트라, 플로이드 알고리즘 기본 2문제 2회, 실전 2문제

<br>

# Tensorflow 자격증
### ML 패러다임
코드를 직접 짜지않아도 모델이 알아서 해줌

### 신경망의 `Hello World`

```python
model = keras.Sequenctial([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer='sgd', loss='mean_squared_error')
```
- `Sequential` 은 keras의 API로 이것을 통해서 모델을 만들겠다.
- `Dense` 는 레이어를 뜻하며 여기서는 unit이 1이고 입력 차원은 [1]이라고 지정해줬다.
- `loss function` 으로 현재의 예측이 잘 맞는지 안 맞는지를 예측하고
- 그 데이터를 `optimizer` 에게 전달하여 새롭게 향상된 새로운 예측을 알아낸다.
   - `optimizer`는 `loss function`의 데이터를 이용하여 예측이 얼마나 잘 됐는지 안됐는지를 알아내는 것이다.
   - 이렇게 함으로써 각각의 예측은 전보다 더 나아진다.

<br>

```python
xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)
```
- numpy array를 이용하여 x와 y를 만들어준다.

<br>

```python
model.fit(xs, ys, epochs=500)
```
- 모델에게 x값과 y값을 어떻게 fit하는지 알아보라고 한다.
- `epochs` 는 학습의 반복을 얘기한다.
    - 한 학습은 다음과 같이 이루어진다.
        - 예측
        - `loss function`을 이용하여 예측이 얼마나 잘 됐는지 안됐는지를 파악
        - `optimizer`과 `data`를 이용하여 다른 예측

<br>

```python
model.predict([10.0])
```
- 모델이 학습을 완료하면 `predict`를 이용하여 예측할 수 있다.
- 앞에서 xs, ys의 관계를 인간인 우리가 예측했을 때는 `y = 2x -1` 라는 것을 알 수 있을 것을 알기에 답 `19`를 예측할 수 있다.
- 하지만 모델은 `19`가 아닌 `19`와 아주 가까운 수를 예측한다.
    - 이유는 다음과 같다.
        - 현재 xs, ys의 데이터의 수가 적다.
        - 모델은 probablity(가능성)을 예측하기에 정확한 수가 나오지 않는다. (주요 이유)
    - 우리는 이것을 정확하게 예측시키는 작업을 해야한다.


<br><br>

### CV
CV의 경우 많은 데이터셋을 다룬다. 10개의 클래스와 각각의 이미지가 28*28인 패션 mnist 데이터셋을 가지고 오려면 어떻게 해야할까?

<br>

```python
fashion_mnist = keras.datasets.fashion_mnist
(train_images, tarin_labels), (test_images, test_labels) = fashion_mnist.load_data()
```

- 다행히도 keras.datasets이 있다.
- 각각 데이터가 있고 label이 있는 데이터셋을 받는다.
- train을 보면 이미지인 `train_image` 가 있을 것이고 그 이미지를 설명하는 `label` 이 있을 것이다.
    - e.g) 이미지가 앵클부츠이면 label은 그를 뜻하는 숫자 9
    - `label` 이 숫자일 때 컴퓨터는 수행을 더 잘하며, 더 중요하게 텍스트로 표현하는 것 보다 언어에 대한 `bias`를 줄일 수 있다.

<br>

```python
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)
])
```
- 세 개의 레이어를 볼 수 있다.
- 첫번째 레이어와 마지막 레이어를 중요하게 보자.
    - 첫번째 레이어인 `Flatten` 레이어는 input_shape를 (28, 28)로 가지고 있는 것을 볼 수 있다. 입력하려는 이미지의 크기가 28*28이므로 이 부분을 명시해주는 것이다. 이렇게 해주면 Flatten 레이어는 이 사각형의 모양을 받아서 간단한 선형 array로 만들어준다. 이름 그대로 납작하게 해주는 것이다.
    - 마지막 레이어의 10개의 뉴런들을 가지고 있다. 데이터셋의 클래스들이 10개이기 때문이다. 데이터셋의 클래스와 마지막 뉴런들은 언제나 일치해야한다.
- 흥미로운 일은 가운데 레이어에서 일어난다(은닉 레이어라고도 부른다.). 이것은 128개의 뉴런들로 이루어져있는데 이것들을 x1, x2, x3 등등과 같은 함수의 변수로 생각해보자. 이 변수 784값들을 앵클부츠의 값 9로 바꾸고 다른 것에 대해서도 유사한 값을 모두 통합할 수 있는 어떠한 규칙이 있다. 지금 알기에는 좀 복잡할 수 있지만 이것이 신경망이 하는 일이다. 
    - 이 x1, x2, x3 등등에 가중치 w1, w2, w3을 계산해서 y의 값을 알아내는 것이다.
    - 이러한 방법을 당장 확실하게 이해할 순 없겠지만 계속 따라와보자.

### Callbacks

```python
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.6):
      print("\nReached 60% accuracy so cancelling training!")
      self.model.stop_training = True

mnist = tf.keras.datasets.fashion_mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

callbacks = myCallback()

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer=tf.optimizers.Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])
```

- callback을 이용해서 원하는 accuracy에 도달했을 때 training을 멈출 수 있다.

<br><br>

## CNN
위에서는 깊은 신경망을 통해서 정확도를 높이는걸 배웠다. 하지만 test셋에서는 정확도가 그리 높지않고 이미지에 빈 공간이 많아 효율적이지 못하다. 만약 특징만을 효율적으로 학습하고싶다면? 여기서 CNN이 나온다.

```python
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
```
- 마지막 세 줄은 배웠던 것과 같다.
- 첫번째 줄은 64개의 (3, 3)필터를 생성해달라고 하며 activation은 relu로 지정하여 음수의 값들은 내다버려달라고한다. 그리고 입력 shape를 명시해준다.
    - 마지막 1은 grayscale(흑백)이므로 1로 지정해준다.
- 두번째 줄은 합성곱들 중에 큰 값을 반환해주는 MaxPooling을 사용해준다. 2*2로 지정하여 4개의 픽셀 중 가장 큰 값이 살아남도록 해준다.
- 세번째 줄과 네번째 줄에 똑같이 Conv2D와 MaxPooling을 해준다.
- Flatten으로 쫙 펴준다.

이렇게 해주면 밑으로 갈 수록 내용물이 점점 간단해진다.

<br><br>

```python
model.summary()
```
- `model.summary()` 를 통하여 모델의 레이어들과 상세내용들을 테이블로 편하게 볼 수 있다.

- 이 테이블에서 첫번째 Output Shape 컬럼을 보는 것이 중요하다. 위의 CNN을 보면 첫번째 Output Shape 가 26*26으로 나올 것이다. 28*28이 들어갔는데 출력은 26*26이다. 왜 그럴까? 여기서 필터가 3*3이라는 것을 핵심으로 봐야한다. 이미지의 가장자리에 있는 2개까지의 픽셀들은 3*3의 필터를 사용할 수 없다. 3*3 필터가 모든 영역을 확보해야 되기 때문이다.
- MaxPooling으로 쫙 펴주었기 때문에 26*26이 13*13이 된다.
- 이것을 계속하면 


<br><br>

# A Chat with Andrew on MLOps: From Model-centric to Data-centric AI

MLOps 전반적인 개념에 접근하기 위해서 앤드류 응이 발표했던 MLOps를 시청했다.

- 모델의 정확도를 올리기 위해서 `Model-centric`, `Data-centric` 무엇에 더 접근해야할까?
    - 데이터중심적으로 접근하니까 더 올랐다.
- 데이터는 AI의 음식이다.
    - 우리를 셰프라고 비유한다면 음식의 퀄리티가 80%로 중요하고 기술이 20%로 중요하다고 할 수 있다. 여기서 음식은 데이터이고 기술은 모델을 학습시키는 것이라고 할 수 있다.
- 우리는 이렇게 중요한 중요한 데이터보다 모델을 학습시키는 것에 더 집중하고있다.
- 모델을 학습시키는 것은 머신러닝 프로젝트의 일부분일 뿐이다.
    - ML Project의 Lifecycle
        - Scope project (프로젝트를 정의한다.)
        - Collect data (데이터를 정의하고 모은다.)
            - data의 consistency가 정말 중요하다.
        - Train model (학습하고 에러를 분석하고 모델을 지속해서 향상시킨다.)
            - 종종 이 단계에서 다시 Collect data 단계로 갈 수 있다.
        - Deploy in production (배포하고 모니터하고 시스템을 지속시킨다.)
            - 종종 이 단계에서 Train model, Collect data로 단계로 다시 되돌아갈 수 있다.

계속 듣다보니 데이터가 중요하다는 것으로 귀결되어서 라이프사이클만 다 들어도 되겠더라. 어쨌든 데이터가 왜 중요한지는 배웠다.

<br><br>

# MLOps KR 커뮤니티 01 MLOps 춘추전국시대 정리 변성윤, SOCAR
풀스택딥러닝 MLOps들어가기 전에 MLOps에 대해서 전반적으로 알 수 있는 발표가 있어서 봤다. 너무 도움이 많이 되었음. <br>