# Kaggle PetFinder 아키텍처 선택
## Image Classification - ImageNet SOTA 모델 순위
이미지 분류 분야에서 ImageNet을 이용한 SOTA 모델 순위를 살펴보면서 어떤 모델이 분류 성능이 좋은지 알아봤다. 보다보니 성능이 좋은 것들의 공통점이 있었다. 
- 최근에 나온 모델이 성능이 높았다.
- `Transformer`, `Transformer + CNN`, `EfficientNet` 을 이용한 모델들이 상위권이었다.
- 상위권에 있는 모델들은 파라미터가 비교적 많다.

그리고 Transformer를 이용한 모델이 제일 성능이 좋을 것 같았는데 꼭 그건 아니어서 놀랐다. Swin Large모델보다 EfficientNet Noisy student가 성능이 더 좋았음.

<br><br>

## Meta Pseudo Labels
Transformer를 이용했던 Swin 모델을 둘러봤었고 Noisy student를 이용한 EfficientNet도 둘러봤었다. 그런데 Noisy student의 단점을 보완한 EfficientNet 모델이 5~6위에 파라미터도 비교적 많지않아서 한 번은 공부해볼만한 듯. 구현은 못하지만...

<br><br><br><br>

# Meta Pseudo Labels
저번에 공부했던 Noisy Student은 `teacher`가 생성한 `pseudo label`이 잘못되면 `student`가 계속 그 잘못된 데이터로 학습을 하게된다는 단점이 있었다. Meta Pseudo Labels는 이 단점을 보완하기 위해 나왔으며 `teacher`와 `student`를 동시에 학습한다. 학습은 다음과 같이 진행된다.

> - (1) student는 teacher에서 생성된 pseudo labeled data로 학습을 하고, 
> - (2) teacher은 student가 labeled image에 대해 얼마나 잘 작동하는지의 reward signal로 학습합니다.
> <br>
> *(https://deep-learning-study.tistory.com/560)*

<br>

>  teacher가 고정되어있는 Pseudo labels와는 달리 Meta Pseudo Labels의 teacher는 labeled 데이터에 대한 student의 결과의 feedback에 따라 학습된다.
> <br>
> *(https://velog.io/@khp3927/2020Meta-Pseudo-Labels)*

<br><br><br><br>

# Fine Tuning
모델을 선택했으면 Fine Tuning 과정을 거쳐야하는데 좀 더 자세히 알아보려고 공부해봤다. Fine Tuning은 전이학습 방법으로 `pre-trained model` 을 이용해서 새로운 모델 학습하는 것으로 다음과 같은 전략들을 사용한다.

- 전체 모델 싹 새로 학습
    - 모델의 구조만 활용하는 것. 연산을 엄청 해야함.
    - `learning rate` 적게! (이전 학습 내용 잊어버릴 수 있음)
- 레이어 일부 고정
    - 모델이 커질수록 속도가 빨리진다는 듯.
    - `learning rate` 적게! (이전 학습 내용 잊어버릴 수 있음)
- 레이어 고정, classifier만 새로 학습
    - 연산능력 부족할 때
    - 데이터셋이 적을 때
    - 풀고자하는 문제가 비슷할 때

<br><br>

## 어떤 전략을 선택해야할까?
- **데이터가 크고 유사성이 작을 때**
    - 전체 모델 싹 새로 학습하자. 유사성은 없지만 데이터가 많아서 사전 학습 모델 구조와 파라미터들을 가지고 시작해도 도움 많이 됨.
- **데이터가 크고 유사성이 높을 때**
    - 뭘 해도 좋음. 그 중에서도 레이어 일부 고정이 효과적일 듯. 데이터셋 크기가 커서 `overfitting` 문제에서 해방. 뒤 쪽 레이어 일부만 학습시켜도 충분할 듯.
- **데이터가 적고 유사성이 작을 때**
    - 데이터셋이 적아서 많은 레이어를 학습시키면 `overfitting` 문제가 있고 그렇다고 적은 레이어를 학습시키면 학습이 잘 안됨. `data augmentation` 고려해야함. 일부를 고정하는데 깊은 레이어까지 학습을 해야할 듯.
- **데이터가 적고 유사성이 높을 때**
    - 이럴 땐 레이어 고정, classifier만 새로 학습하는게 베스트.
- 배치크기가 작을수록 validation 정확도가 좋다. (정규화 적용이 잘 됨)

<br>
PetFinder 데이터셋이 거의 9000개이긴한데 colab으로 학습하는 것이 한계가 있다보니까(다섯시간 넘게 걸리고..) 자원이 덜 드는 쪽으로 fine tuning해봐야겠음.  Swin모델 레이어 고정하고 classifier만 변경 해봐도 좋을듯.

<br><br><br><br>



# Freeze BatchNorm when Fine Tuning
Fine tuning 과정에서 BatchNorm을 얼리길래 찾아봤다. 새로운 데이터셋을 가지고 fine tuning을 할 때, 원래 학습데이터셋과 다른 특성을 가질 경우 Batch 통계가 매우 다를 수 있다고한다. 그러므로 BatchNorm 레이어가 고정되지않으면 신경망은 새로운 BatchNorm 파라미터들을 학습하게되어버림. 즉 모든 파라미터들을 다시 학습해버리는 것임(파인튜닝의 의미가 없는 것). 그래서 고정해놓는 것. 안해놓으면 정확도가 점점 내려간다는 듯.

<br><br><br><br>

# CatBoost
부스팅의 주요 아이디어는 모델을 순차적으로 결합해서 greedy-search를 이용하여 예측 모델을 만드는 것이다. 트리를 이용해서 이전 트리의 실수를 학습하고 오류를 줄여나간다. CatBoost는 이것과 살짝 다른게 oblivious tree 를 사용한다는 것이다. 트리구조가 최적의 솔루션을 찾고 overfitting을 피하기 위해 정규화로 작동되는 것과 다르게 이 방법은 효율적이고 간단한 fitting을 한다.

## 특징 추출(Feature Extraction)
CatBoost를 할 때, EfficientNet모델을 Feature Extractors로 사용한다고 해서 공부해봤다. <br>
feature extraction은 차원 축소 과정의 하나로, raw 데이터의 initial set이 분할 되어서 관리하기 쉬운 그룹으로 줄이는 것이다. 이렇게 해서 효율적으로 자원을 사용하는 것이다.데이터 세트에서 중복 데이터의 양을 줄이는 데 도움을 주는 장점도 있다.


<br><br><br><br>

noisy student 가중치를 적용한 EfficientNet b2 모델 만들고 그걸 feature extrator로 만들고 catboost 하는 과정 한 번 따라해봄. 생각보다 괜찮았는데 튜닝한다고 해서 정확도가 드라마틱하게 올라가진않을 것 같음. 왜냐면 cnn은 얼굴위주로 구분을 하는데 transformer는 얼굴과 환경을 구분할 것이고 petfinder 대회에서 얼굴과 환경을 구분하는 것이 예측에 도움을 준다는 discussion을 봤기 때문이다. 흠. 내일 EfficientNet 만져보고 swin도 한 번 해봐야겠음.

<br><br>


* * *
- https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751 <br>

- https://towardsdatascience.com/catboost-regression-in-6-minutes-3487f3e5b329 <br>

- https://stackoverflow.com/questions/63016740/why-its-necessary-to-frozen-all-inner-state-of-a-batch-normalization-layer-when <br>

- https://deep-learning-study.tistory.com/560 <br>
- https://velog.io/@khp3927/2020Meta-Pseudo-Labels 
- [Semi-supervised Learning explained](https://www.youtube.com/watch?v=b-yhKUINb7o)