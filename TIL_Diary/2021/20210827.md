# ML
# Coursera - Convolutional Neural Network 
## Case Stuies
### 왜 케이스 스터디들을 봐야할까요?
합성곱 신경망을 구축하기 위해서는 효과적인 것들을 많이 봐야한다. <br>
누군가가 발견해낸 합성곱 신경망은 다른 곳에도 적용이 잘 된다. <br>
우리는 다음과 같은 것들을 볼것이다.

- Classic network
    - LeNet-5
    - AlexNet
    - VGG
- ResNet
- Inception

이 예시에서 나오는 수많은 것들은 CV외에도 다른 분야에서도 유용하게 쓰일 수 있을 것이다.

<br>

## Classic network
### LeNet-5 (1998)

#### 구조
![](https://github.com/matamatamong/img/blob/main/Visualizations/Lenet5.jpg?raw=true)

#### 특징
- NeNet-5는 흑백에서 작동한다.
- 그 시대에는 ReLU함수를 사용하지 않아서 LeNet-5의 논문을 보면 sigmoid/tanh 함수를 사용하는 것을 볼 수 있다.
- 논문을 읽으려면 섹션2에 중점을 두는 것이 좋을듯.
- 6만개 정도의 변수

<br>

### AlexNet(2012)
#### 구조
![](https://neurohive.io/wp-content/uploads/2018/10/AlexNet-1.png)

#### 특징
- NeNet과 유사하지만 훨씬 크다.
    - 6천만개정도의 변수
        - 성능이 훨씬 빠르다.
- 논문이 쓰여질 당시 GPU가 느려서 두 개의 GPU를 훈련했다.
- 비교적 논문이 쉬운편에 속해서 읽으려면 이것을 읽는걸 추천한다.
<br>

### VGG - 16 (2015)

#### 구조
![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg)

#### 특징
- 많은 하이퍼파라미터를 가지는 대신 
    - 합성곱에서
        - s = 1
        - f = 3*3
        - padding = same
    - 맥스풀링에서
        - s = 2
        - f = 2*2
을 사용한다.
- 16개의 가중치를 가지는 층이 있다.
- 1억 3천 800만개 정도의 변수를 가진 상당히 큰 신경망이다.
- VGG-19도 많이 쓴다.
- AlexNet 다음으로 읽기를 추천하는 논문이다.

<br>

## ResNets
고전적인 신경망을 넘어서 더 강력하고 고급진 신경망을 알아보자. 첫번째로 볼 것은 ResNets이다. <br>
아주 깊은 신경망을 훈련시키기 어려은 이유는 경사손실이나 경사가 폭발적으로 증가하는 문제가 있기 때문일 것이다. <br>
한 층의 활성값을 가지고 훨씬 깊은 층에 적용하는 `스킵연결` 을 이용하여 100개도 넘는 깊이의 신경망을 효율적으로 학습하게 만들 수 있는 ResNet을 알아보자.

### Residual block (잔여 블록)
ResNets은 잔여블록이라는 것으로 구성되어있다. 

#### Redidual의 예
층이 2개인 신경망의 일반적인 방식을 보자.

- a^[l]
- Lenear
- ReLU
- a^[l+1]
- :alien:
- Linear
- ReLU
- a^[l+2]

ResNet은 이것을 조금 바꾼다 <br>
a^[l] 을 복제하여 신경망의 더 먼 곳까지 단번에 가게 만든 뒤 ReLU 를 적용해주기 전, 👽이 있는 곳까지 보낸다. 이 길을 `short cut(skip connection)` 으로 부른다.<br> `short cut`안에 있는 것들은 잔여블록이 된다.
이렇게 될 때 마지막 부분은 

```
a^[l+2] = g(z^[l+2] + a^[l])
```

이 된다. <br>
![](https://github.com/matamatamong/img/blob/main/Visualizations/Residual%20Network.PNG?raw=true)

#### 뭐가 좋은데?
훨씬 깊은 신경망을 훈련시킬 수 있다. <br>
보통 일반적인 신경망을 훈련시키고 경사하강법이나 최적화 알고리즘을 이용하여 훈련시키면 층의 개수를 늘릴 수록 훈련 오류는 감소하다가 다시 증가한다. 원래는 계속 감소해야하는데 말이다. 평형망의 깊이가 매우 깊다면 최적화 알고리즘으로 훈련하는 것이 힘든 것이다. 하지만 ResNet은 활성값이나 중간 활성값을 취함으로써 훨씬 더 깊은 신경망을 사용할 수 있게 해준다. 이것은 경사 소실 문제에 많은 도움이 되고 층이 깊어져도 훈련 오류가 계속 감소하는 성능을 가질 수 있다.

<br><br>

#### 왜 좋은데?
보통은 많은 수의 층이 있을 수록 변수를 선택하기가 어렵다. <br>
하지만 ResNet은 성능에 손해가 없고 운이 좋다면 성능을 향상 시킬 수 있다.

#### 특징
- 동일 합성곱으로 차원이 유지되기 때문에 이것들이 가능하다.

<br><br>

## 1x1 Convolutions (Networks in Networks) (2013)
1*1 filter를 사용해서 합성곱을 하는 것이다. <br>
그저 곱셈의 연속이라고 생각할 수 있겠지만 이것이 여러개의 채널을 가지고 있을 때는 말이 달라진다! <br>

### 뭘 하는 것인데?
인풋과 필터의 채널을 각각 element-wise 곱을 하고 ReLU를 적용 하여 값을 계산하는 것이다.

### 어디에 쓰는데?
- 채널의 수가 너무 많아서 줄이고 싶을 때 쓰일 수 있다.
- 비선형성(ReLU)를 더해주기 때문에 더 복잡한 함수를 학습할 수 있게된다.

<br>

## Inception Network(GoogLeNet) (2014)
*WE NEED TO GO DEEPER* <br>

합성곱 신경망의 레이어를 디자인할 때 어떤 필터를 사용해야할지 풀링층을 사용할 것인지를 고민할 것이다. 인셉션 네트워크는 이것을 다 사용하는 것이다. 좀 복잡해지기는 하지만 성능은 뛰어나다. 출력들을 다 엮어낸 뒤 신경망이 스스로 원하는 변수나 필터크기의 조합을 학습하게 하는 것.

### 이게 뭔데?
- 필터의 크기를 정하지 않고 합성곱 또는 풀링층을 모두 사용하는 것이다.
- 신경망이 인셉션 모듈로 이루어진다.

![](https://github.com/matamatamong/img/blob/main/Visualizations/Inception-module.PNG?raw=true)

![](https://github.com/matamatamong/img/blob/main/Visualizations/Inception-network.PNG?raw=true)

### 특징
- 맥스풀링이나 다른 필터를 쓸 때 차원을 똑같이 해줘야한다.
- 계산비용이 너무 많이 들어서 앞에서 봤던 1x1 Convolutions 을 사용하여 병목구간을 만들어서 계산 비용을  줄인다.

<br><br>

## MobileNet (2017)
모바일같은 컴퓨터자원이 부족한 곳에서도 작동할 수 있는 신경망이다. `depthwise-separable convolutions` 를 토대로 한다.
<br>

###  MobileNet이 왜 필요한데?
- 배포할 때 적은 비용이 들어야할 필요가 있어졌다
- 모바일이나 임베디드 환경에서 잘 작동할 필요가 있어졌다.

### depthwise-separable convolutions 이 뭔데?
평범한 합성곱에서는 필터를 인풋에 대볼 때 차원을 다 포함해서 계산하기 때문에 계산자체에 자원이 많이 들어간다.  그래서, <br>

`depthwise convolution`로 필터의 깊이를 다 포함해서 계산하는 대신, 인풋 차원에 대응하는 필터의 차원만 계산해서 결과를 낸다. 그리고 난 뒤 <br>
`` 
`Pointwise Convolution` 을 해준다. 이것은 1*1*3 과 같은 필터로 차원은 깊지만 인풋에 대는 구역은 작은 필터를 통해서 인풋을 계산하는 것이다.
<br>

위의 두 단계를 합성곱한 것을 `depthwise-separable convolutions` 이라고 부른다. <br>

<br>

![](https://github.com/matamatamong/img/blob/main/Visualizations/Depthwise-Separable%20Convolution.PNG?raw=true)

### depthwise-separable convolution 의 비용
- 평범한 합성곱이 2160의 자원이 든다면
- depthwise-separable convolution 은 672의 자원이 든다.

평범한 합성곱보다 자원이 훠어어얼씬 덜 든다.

<br>

###  MobileNet 구조
MobileNet v2를 많이 쓴다. <br>
MobileNet v2 (2019)는 기본적인 것에 다음과 같은 것들이 더해진다.
- Residual Connection을 한다.
- expansion layer를 추가한다.
    - Depthwise 전에 추가한다.
- 위의 두 과정을 Bottleneck 블럭이라고 부른다.
    - Expansion 계산을 통해서 차원을 더 추가해주면 변수가 많아지고 더 복잡한 계산을 할 수 있게 해준다. 하지만 그렇게되면 모바일같은 곳에 배포할 때 메모리나 자원이 부족하게되니까 pointwise(projection이라고도 부름) convolution을 통해서 다시 변수를 줄여주는 것이다.
- 위의 과정을 17번 반복한다. 그것이 MobileNet v2다.

![](https://github.com/matamatamong/img/blob/main/Visualizations/MobileNet%20v2%20Bottleneck.PNG?raw=true)

<br>

## EfficientNet
MobileNet v1이나 v2는 컴퓨터자원적으로 아주 효율적이었다. 하지만 MobileNet이나 다른 설계들을 특정한 디바이스에 맞춰서 튜닝할 수 있을까? CV를 애플이나 안드로이드에 맞춰 튜닝을 할 것인가 등등말이다. <br>
EfficientNet은 신경망의 스케일을 특정 디바이스에 대해 적용할 수 있도록 도와준다.

### EfficientNet
보통의 신경망에서 이미지를 구분할 때, 스케일을 업하거나 다운을 하기 위해서는 다음과 같은 것들이 필요하다.
- 고해상도의 이미지 사용 (high `r` esolution)
- 신경망을 더 깊이한다. (`d` epth)
- 레이어를 더 넓혀본다. (`w` idth)
이 때, `r`, `d`, `w` 중에 뭘 적용해야 본인이 생각한 컴퓨터 자원에 맞게 스케일을 업하거나 다운할 수 있을까? <br>
이럴 때, 오픈소스인 EfficientNet을 사용해보자.

<br><br>

## ConvNet들을 이용할 때 실용적인 것들

### Open Source를 이용하자
논문을 가지고 바로 모델을 만들기에는 하이퍼파라미터나 변수가 너무 많아서 좋은 성능을 내기가 너무너무 힘들다. <br>
그러나 요즘엔 GitHub같은 곳에 오픈소스로 기여가 많이 되기 때문에 오픈소스를 보는 것이 좋다.
- e.g
    - residence를 구현하고 싶으면 구글링해서 그것을 구현한 GitHub Repo를 찾아서 clone해서 그것을 가지고 build 해보고 연습하자. transfer해도 좋다.

<br>

### Transfer Learning을 하자
하나부터 열까지 다 구현하려하지말고 다른 사람이 이미 해놓은 아키텍처를 `pre-train`으로 보고 Transfer해보자.
- e.g 1 트레이닝셋이 별로 없다.
    - 오픈소스 신경망을 다운로드한다.
        - 가중치도 다운로드한다.
    - 레이어들을 freeze한다. (트레인하지않고 기존의 것들을 쓴다.)
    - 거기서 자신만의 softmax 유닛만 생성하고 그것만 학습시킨다.

- e.g 2 트레이닝셋이 좀 있긴하다.
    - 오픈소스 신경망을 다운로드한다.
        - 가중치도 다운로드한다.
    - 거기서 자신만의 softmax 유닛만 생성하고 그것만 학습시킨다.
    - 하지만 여기서 레이어들을 몇개만 이용하여 학습시킨다. 
        - 마지막 몇개의 레이어만의 가중치만 사용한다.
        - 마지막 몇개의 레이어를 날려버리고 자신만의 히든유닛을 사용한다.
    - 
- e.g 3 트레이닝셋이 많다
    - 오픈소스 신경망을 다운로드한다.
        - 가중치도 다운로드한다.
    - 거기서 자신만의 softmax 유닛만 생성하고 그것만 학습시킨다.
    - 하지만 여기서 레이어들을 다 사용한다.

<br>

### Data Augmentation을 하자.
오늘날의 CV는 충분한 데이터를 얻어야한다. 그래서 데이터 증강은 아주 도움이 된다. <br>
CV에서 보편적인 데이터증강은 다음과 같다. <br>
데이터 증강도 방법이 많으니 오픈소스를 많이 보자.

#### 보편적인 데이터 증강
- Mirroring
- Random Crop
    - 완벽한 데이터증강은 아니다. 
- 이론적으로는 Rotation, Shearing, Local warping을 사용해도 되긴함.

#### 색상 이동(Color shifting)
- R,G,B 채널에 왜곡을 준다.

<br><br>

### State of Computer Vision
#### Data vs hand-engineering
- 평균적으로 데이터가 많을 수록 간단한 알고리즘으로 빠지는 경우가 많다. 
- 데이터가 없을 수록 직접 설계하는 `핸드 엔지니어링`으로 많이 빠진다.
CV는 데이터가 없기 때문에 `핸드 엔지니어링` 에 많이 의존했었 것 같다는 앤드류 응의 생각.

#### 벤치마크를 잘하거나 컴피티션 winning을 잘 하기 위한 팁
프로덕션에 있어 잘 수행한다고 해도 실제로 서비스에 사용하지 않는 것들은 다음과 같다.

- Ensembling
    - 자원 계산을 너무 해야함.
- test에서의 Multi-crop 
    - test image를 multi-crop하는 것은 실행 시간을 느리게한다.

<br><br><br>

# 캐글 스터디

<br>

### Tensorflow 자격증 취득 계획
#### **자격증 공부일정**
Coursera - DeepLearning Specialization 과정 `9/13` 일까지 완료한 뒤 자격증 공부시작하여 `일주일 안에` 도전

<br>

#### **어떻게?**
- Coursera의 [DeepLearning.AI TensorFlow Developer Professional Certificate](https://www.coursera.org/professional-certificates/tensorflow-in-practice) 강의 5일 안에 수강
- Laurence Moroney의 [깃허브](https://github.com/lmoroney/dlaicourse) 에서 시험 예제 풀기

<br>

#### **떨어지면?**
- 14일 뒤 다시 재응시할 수 있으니 9월 말에 다시 도전

<br>

#### **붙으면?**
- 시험 후기 블로깅
- GDP MLOps 과정 밟을 각 되는지 체크

