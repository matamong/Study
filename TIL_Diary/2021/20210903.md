# ML
# Coursera - Sequence Models - Recurrent Neural Networks
## Why Sequence models?
음성 인식, 자연 언어 처리 및 기타 영역을 변화시킨 시퀀스모델을 알아보자. 어떤 때에 유용할까?
- 음성인식
- 음악 생성
- sentiment classification
- DNA 시퀀스 분석
- 기계 번역
- 비디오 활동 인식
입력 x와 출력 y의 데이터가 시퀀스일 때 유용하다.

<br>

## Notation
```
X : Harry Potter and Hermione Granger invented a new spell. 
```
위의 문장을 입력으로 받는 시퀀스 모델을 구축한다고 가정해보자. <br>
이 모델은 문장에서 사람의 이름이 어디에 있는지 자동으로 알려주는 시퀀스 모델이다.(Named Entity Recognition) <br>
입력을 받았을 때 모델의 출력값은 다음과 같다.
- y output : 단어마다 한 개의 값
- target output : 각각의 입력 단어가 사람 이름에 포함되는지
좀 더 복잡한 출력이 필요하지만 여기서는 notaion을 알아볼 것이니까 간단한 출력을 사용하겠다. <br>
입력 X 시퀀스는 9개의 단어로 이루어져있다. 이 9개의 피쳐 집합의 순서를 표현하기 위해서 다음과 같은 표현을 쓰겠다.

```
X^<1> X^<2> ... X^<t>... X^<9>
```
<br>

출력값도 마찬가지로 다음과 같이 쓰겠다.

```
y^<1> y^<2> ... y^<t>... y^<9>
```

<br>

그리고 이 시퀀스의 길이를 아래와 같이 쓴다.
```
T_x = 9
T_y = 9
```

<br>

```
X^(i)^<t>
T_x^(i)
```
- X^(i)^<t> 는 i트레이닝 세트의 <t>번째 요소를 뜻 
- T_x^(i) 는 i트레이닝 세트의 데이터의 길이를 뜻

<br>
이제 자연어 처리를 다루는 시도를 하며 더 알아보자. <br>
먼저 결정해야할 일은 시퀀스의 개별 단어 표현 방법을 정하는 것이다. 먼저 딕셔너리를 만들어서 사전을 만들거나 선택한다. a부터 시작해서 z까지의 단어가 들어가있는 사전을 말이다. 이 안에 들어있는 단어는 몇십만개가 될 수 있지만 여기서는 일단 만개의 단어만 들어있다고 쳐보겠다. <br>

x^<1>(Harry)는 사전의 크기만한 벡터(10000)가 되고 사전의 값과 일치하는 인덱스는 1이 된다.(나머지는 0) 

<br>

X^<2>(Potter), X^<3>(And) 등등도 사전의 크기만하고 값이 일치하는 인덱스에 1이 들어가있는 벡터가 된다. 

<br>

즉, X^<t>(X의 문장 안에 들어가 있는 단어값들)는 `one-hot vector` 가 된다. 정확히 1:1 대응이 되는 것은 1이고 나머지는 다 0이 되기 때문이다. <br>
만약 사전에 대응되는 단어값이 없으면 어떻게 할까? 그럴땐 토큰을 새로 만들거나 가짜단어(unknown)를 만든다. 나중에 배우니 걱정하지말자.

<br><br>

## Recurrent Neural Network Model
자, 이제 용어는 알았으니 x와 y를 맵핑하는 recurrent 모델을 만들어보자. <br>

### 왜 기본 신경망은 안되는걸까?
기본 신경망을 먼저 써보자. 아까 예제에서 9개의 입력 단어는 one-hot 벡터가 신경망에 입력된다. 그리고 몇 개의 히든 레이어를 지나서 0과 1로 된 9개의 아웃풋 값을 출력할 것이다. (1은 사람이름을 뜻함) 이 방법에는 주요한 문제점이 있다.
- 입력과 출력의 길이가 **트레이닝 데이터마다 다르다.**
- 서로 다른 텍스트의 위치에서 **학습된 피쳐가 공유되지 않는다.**
    - X^<1>에서 Harry를 사람 이름이라고 학습했다면, X^<t>에서도 Harry를 사람 이름이라고 학습한다. No 효율이다. 그래서 이미지의 한 부분에서 학습된것이 다른 곳에도 전파되는 CNN같은 개념이 이 시퀀스 데이터에도 필요해지는 것이다. 이렇게 하면 파라미터의 개수도 줄여줄 수 있음.

<br>

### Recurrent Neural Networks
좌에서 우로 문자를 읽는다고 해보자. 그렇다면 
- X^<1>을 먼저 읽을 것이다.
- 이 X^<1>을 신경망에 넣는다.
- 히든레이어를 거친다.
- X^<1>의 아웃풋 Y^<1>을 출력한다.
- 두번째 X^<2>를 읽는다.
- 히든레이어를 거친다.
- 이 때, X^<2>만 사용하지 않고 첫번째 시점에서 연산한 정보의 일부를 사용한다.
    - 구체적으로 첫번째 시점의 activation 값이 전달되는 것이다.
- X^<2>의 아웃풋 Y^<2>을 출력한다.
- 반복한다.
RNN은 이런식으로 actiavtion값을 다음 스텝에 전해준다. Recurrent 라는 말이 참 어울린다. <br>
참고로 첫번째에서는 전 단계가 없다보니 보통 0의 벡터를 만들어서 받는다. 이렇게 해주면 각 단계에서 사용하는 파라미터가 공유가 된다. 여기서 이 과정을 제어해주는 파라미터들이 등장하는데 다음과 같다.
- 연결을 제어해주는 파라미터:  `W_ax`
- activation 값을 제어해주는 파라미터: `W_aa`
- 출력값을 제어해주는 파라미터: `W_ya`
이 친구들은 모든 타임 스텝에 들어가서 제어하는 역할을 한다.(나중에 더 자세히 알아볼거임) <br>
자, 다시 정리해보자. <br>
RNN은 예측값 y^^<3>을 예측할 때, X^<3>에서 정보를 얻는 것 뿐만아니라 X^<1>, X^<2>에서도 정보를 얻는다는 것이다. <br>
하지만 여기서 조금 아쉬운 것이 있다. 지금까지는 앞의 정보만을 사용해서 예측을 하고 있다. 이렇게 되면 뒷 부분의 정보가 주어지지않아 사람이름인지를 예측할 때 뭐가 뭔지 잘 알 수 없어질 때가 올 것이다. 이 부분은 나중에 Bidirectional RNN에서 다뤄볼 것이다. 지금 하고있는 것은 Unidirectional(단방향)이고 RNN의 주요컨셉을 알기에는 이 정도도 충분하니 계속 이걸로 가보자.

<br>

### Forward Propagation
자, 이제 RNN이 어떻게 계산되는지를 알아보자. <br>

![RNN_Forward_Propagation](https://github.com/matamatamong/img/blob/main/Visualizations/RNN_Forward_Propagation.PNG?raw=true)

<br>

위의 마지막 식을 다음과 같이 더 간략화하면 다음과 같다.

![Simplified_RNN_notaion](https://github.com/matamatamong/img/blob/main/Visualizations/Simplified_RNN_notation.PNG?raw=true)
이 식을 사용하는 이유는 두 행렬(W_aa, W_ax)을 계속 짊어지고 다니는 대신, 한 행렬로 파라미터(W_a[a^<r-1>, x^<t>]로 압축할 수 있기 때문에 쓴다.

<br>

### Backpropagation Through Time
역전파를 통해서 어떻게 학습할지를 알아보자. 보통 프레임워크가 역전파 구현해놓았긴한데 그래도 알아야한다! <br>

![Backpropagation Through Time](https://github.com/matamatamong/img/blob/main/Visualizations/Backpropagation_Through_Time.PNG?raw=true)

- 손실함수로는 이진분류에서 자주 봤던 `cross entropy`를 쓴다.
- 오른쪽에서 왼쪽으로 타임스텝을 밟는다.

<br>

## Different Types of RNNs
지금까지 `T_x` 와 `T_y` 가 같은 수를 가질 때의 RNN 아키텍처를 살펴보았다. 저 둘이 항상 같을 순 없을 것이다. 그 부분을 살펴보자. <br>
지금까지 봤던 아키텍쳐는 입력 시퀀스는 많은 인풋을 가지고 아웃풋도 많은 아웃풋을 가진 `many-to-many`이다. <br>

![Summary_of_RNNs](https://github.com/matamatamong/img/blob/main/Visualizations/Summary_of_RNNs.PNG?raw=true)

### many-to-one
만약, 영화별점분류를 하고싶다고 하자. X는 사용자의 text가 되고 y는 긍정이나 부정을 뜻하는 0이나 1일수도있고, 1에서 5까지의 숫자로 별점을 나타낼 수도 있다. <br>
이 text를 인풋으로 하나씩 넣는다면 모든 타임스텝에서 나오는 출력을 사용하는 대신 RNN이 모든 문장을 읽어들이도록 하고 마지막 단계의 예측값을 출력하도록 할 수 있을 것이다. 그럼 이 구조는 `many-to-one`구조가 된다. 많은 단어를 입력해서 하나의 숫자를 출력하기 때문이다. 

<br>

### one-to-many
`one-to-many`는 음악생성을 통해 볼 수 있다. <br>
이것은 아웃풋으로 음악에 상응하는 note 세트를 출력한다. 인풋 X는 음악 장르나 원하는 음악의 첫 음표가 될 수 있고 언제나 벡터 0일 수도있다.

<br>

### many-to-many
`many-to-many`의 인풋 길이와 아웃풋 길이는 정확히 같아야한다고 앞에서 말했다. 그런데 기계번역같은 것들은 각각 다른 길이를 가지게 된다. 기계번역은 그래서 두 가지의 부분을 구분해놨다.
- encoder
    - 번역할 문장의 인풋을 받는 구간
- decoder
    - 문장을 다른 언어로 번역해서 출력하는 구간


<br><br>

## Language Model and Sequence Generation
Natural Language Processing에서 Language modeling은 아주 기본적이고 중요한 일이다. 또한 RNN의 성능을 좋게 만드는 일이기도하다. RNN을 이용하여 language model을 만들어보자. <br>

### What is language modeling?
음성 인식 시스템을 만든다고 생각해보자. 똑같은 발음이라도 단어는 다를 수 있다. language model을 이용하여 어떤 단어를 사용해야 좋을지 probability(가능성)를 정할 수 있다. <br>
```
P(sentence) = ?
```
language model은 y^<1>, y^<2>, ~ , y^ty 까지의 시퀀스를 입력받으면 probability를 계산하는 것임.

<br>

### Language modeling with an RNN
어떻게 language model을 만들까?

#### Training set 만들기
- training set이 필요하다
    - 많은 언어 text가 필요함.
- 앞에서 했던 것 처럼 문장을 Tokenize 해준다.
    - 문장의 단어를 끊어서 사전과 매핑해주는 것
    - <End of sentence> 토큰을 마지막에 넣어주면 끝났는지 구분이 쉽다.
    - token이 사전에 없으면 <unique token>인 <unkown> 으로대체해준다.

<br>

#### RNN model

![RNN model](https://github.com/matamatamong/img/blob/main/Visualizations/RNN_model.PNG?raw=true)

- 첫 스텝에서 a^<1> 은 첫 단어의 probability를 찾는 소프트맥스 예측이다.
    - 위의 경우엔 `Cats`의 확률이 높겠다.
    - 10,000개의 사전을 가지고 있다면 10,000개의 softmax 아웃풋일 것이다.(unkown이나 end of sentence를 쓰면 10,002)
- 다음 스텝에서는 앞에서 나왔던 첫번째 단어의 예측값(y^<1>, cats)을 받아서 두번째 단어를 찾을 것이다.
- 이런 식으로 앞의 예측을 가지고 한 단어씩 왼쪽에서 오른쪽으로 끝까지 학습을 해준다.
- 비용함수를 정의한다.

지금은 잠깐만 보고 자세한 구현은 나중에 과제로 코딩하면서 구현해볼 것임. 그 때는 probability를 곱하는 것까지 해볼 것.

<br><br>

## Sampling Novel Sequences
시퀀스 모델을 학습한 후, 학습 내용을 알 수 있는 방법 중 하나가 novel sequence를 sampling하는 것이다. 어떤건지 보자. <br>
첫번째 타임스텝의 예측 y^^<1>을 가지고 오면 다음과 같이 소프트맥스 probability 벡터일 것이다.
- P(a)P(aoroan)...p(zulu)p(<eos>)
그럼 `np.random.choice` 같은 api를 이용하여 소프트맥스 분포 안에서 랜덤하게 probability 중 하나를 샘플 해준다.
- 첫번째 스텝에서 샘플한 것을 두번째 스텝에 넘기게 되고 소프트맥스는 그 샘플을 이용하여 두번째 단어의 Y^^<2>를 예측한다.
이런식으로 끝까지 반복해주면 RNN language model에서 무작위로 선택된 문장을 생성하는 방법이다.

<br>

### Character-level language model
단어 수준이 아니라 character수준의 language model을 만들 수 있다. 영어의 경우 알파벳일 것이다. 그렇다면 사전은 알파벳으로 구성된 사전이 될 것이다. character 수준의 모델을 구현하면 장단점이 있다.
- 장점: unknown 토큰에 대해 걱정할 필요가 없음
- 단점: 너무 긴 sequence가 되어버린다.
    - 계산 비용이 많이 든다.

<br><br>

## Vanishing Gradients with RNNs
기본 RNN 모델은 경사소실문제를 가지고있다. 심층신경망에서 은닉층이 많아질수록 경사소실문제를 가지고있는 것처럼, RNN도 그러하기 때문에 긴 문장이 주어질 경우에 앞부분의 단어가 뒷부분의 단어에 영향을 주게되면 기본 RNN 모델은 이러한 것을 잘 잡아주지 못한다. 중간에 있는 타임스텝들이 너무 길어서 기억을 못하기 때문이다. 이러한 문제 때문에 기본 RNN 모델은 많은 local influenece를 가지고 있다. 예측y는 가까운 주변의 예측y에 영향을 많이 받는 것이다. 물론 경사 exploding이 있을 수 있지만 그 경우에는 exploding인 것을 알기 쉽고 gradient clipping을 통해서 해결할 수도 있다. 그러나 경사소실은 더 해결하기 힘들다. 그렇다면 어떻게 해야할까?

<br><br>

## Gated Recurrent Unit (GRU)
기본 RNN의 히든 레이어를 고쳐서 긴 연결을 감지할 수 있고 경사소실 문제도 많이 줄일 수 있는 GRU를 알아보자. 

<br>

### RNN unit
기본 RNN 유닛에서 time t에 대한 activation을 구하는 식은 봤었다. 이것을 조금 시각화 하면 다음과 같다. 

![RNN_unit](https://github.com/matamatamong/img/blob/main/Visualizations/RNN_unit1.PNG?raw=true)

<br>

### GRU (simplified)
GRU 유닛은 memory cell의 c를 따온 C라는 변수를 둔다. <br>
이 C의 역할은 만약 "The `cat`, which already ate blabla.........., `was` full" 이라는 문장이 있을 때, 앞의 cat이 복수인지 단수인지를 기억하는 역할을 하고 이를 통해서 뒷부분의 `was`를 잘 고를 수 있도록 하는 것이다. GRU 유닛을 보면 a^<t>가 이 C^<t>와 같은 출력을 내는 것을 볼 수 있다. <br> 여기까지 알았다면 앤드류 응이 시각화 해준 것과 식을 보면서 GRU가 어떻게 돌아가는지 보자.

![GRU_simplified](https://github.com/matamatamong/img/blob/main/Visualizations/GRU_siplified.PNG?raw=true)

- 각각의 time step에서 C~^<t> 값으로 메모리 셀을 덮어쓴다.  C~^<t>는 C^<t>를 대체할 후보가 될 것이다. 
- 후보가 결정되면 게이트 Γu를 이용해서 그 후보로 업데이트할지 안할지를 정한다.
    - 게이트 Γu는 시그모이드를 취해줘서 대부분 0과 1에 가까운 값이된다.
    - 식을 보면 게이트가 1과 같으면 C^<t>의 새 값을 C~^<t>로 하고 게이트가 0이면 나머지가 값들이 예전 값들이 되게 되어있다.

Γu가 0에 가까워지면 C^<t>는 예전 값을 기억하고 있을 것이기 때문에 경사소실문제를 겪을 일이 줄어들고 긴 문장을 감지할 수 있게된다. 

<br><br>

### Full GRU
간단하게 GRU의 핵심을 살펴봤으니 이제 더 깊이 들어가서 전체적인 GRU를 살펴보자. 앞에서 봤던 간단한 GRU와는 조금 다를 것이다. 똑똑한 사람들이 많이 발전시켜놨기 때문이다.

![Full_GRU](https://github.com/matamatamong/img/blob/main/Visualizations/Full_GRU.PNG?raw=true)

- gate Γr이 추가되었다. 이 친구는 C^<t-1> 이 다음 후보 C~^<t>를 계산할 때 얼마나 관련있는지를 말해준다.

