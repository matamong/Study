# ML
# ML CNN - Object Detection - Detection Algorithms
CNN을 배우고 이제 Obeject Detection을 배우고 있다. 저번 공부에 Object Detection을 하기 이전에 Object Localization 을 하는 것을 배움.

## Landmark Detection
일반적으로는 이미지의 중요한 포인트에 신경망이 x, y좌표를 가지는데 이렇게 신경망이 인식햇으면 하는 중요한 곳을 `Landmark(특징점)` 라고 한다.

### e.g 얼굴인식에서
- 입 주변에 랜드마크를 다닥다닥 붙여서 입모양을 관찰하고싶을 때 이 주변에 랜드마크를 지정해줌
- 이 랜드마크를 포함하는 레이블 훈련세트를 만듦.
- 얼굴을 인풋했을 때 얼굴인지 아닌지를 인식하는 CNN을 거쳐 특징점의 개수에 따라 출력 유닛을 가진다.

컴퓨터 그래픽 효과를 많이 가지는 스냅챕이나 틱톡에서 잘 쓰임
주의할 점은 랜드마크1은 각 이미지에서 항상 동일한 역할을 해야한다.(랜드마크1은 항상 눈꼬리를 가르키고 있다던지.)

<br>

## Object Detection
Localization과 Landmark를 배웠으니 이제 물체인식을 설계 해보자. 합성곱 신경망으로 물체인식을 구현하기 위해서 `Sliding Windows Detection Algorithm` 을 배워보겠다.
<br>

### Sliding Windows Detection Algorithm
도로이미지에 있는 자동차를 인식하고 싶다고 해보자. 슬라이딩 윈도우즈 인식 알고리즘은 
- 작은 window(창)을 만들어서 이미지의 왼쪽 위에 얹는다
- 이 부분을 ConvNet에 입력한다.
- 예측값을 얻는다.
- 그다음 한칸 옆으로 가서 이 작업을 한다.
- 이것을 전체 이미지에 대해서 계속 반복한다.
- 이번엔 조금 큰 windonw(창)을 만들어서 위의 방식을 반복한다.
- 조금 더 큰 window(창)을 만들어서 또 반복한다.

<br>
이 방식에는 계산비용이 너무 든다. 매우 큰 슬라이딩 단계를 적용하면 좀 적어지겠지만 성능이 저하된다. 다행히도 이 부분을 더 효율적으로 구현할 수 있는 방법이 있다.

## Convolutional Implementation Sliding Windows
보통의 ConvNet의 구성은 다음과 같을 것이다.
- 이미지가 인풋
- 필터로 합성곱
- MaxPooL
- FC
- FC
- softmax(원하는 결과유닛수)

여기서 FC 레이어를 convolutional 층으로 바꿔버리는 것이다.
- 이미지가 인풋
- 필터로 합성곱
- MaxPooL
- 필터를 이용하여 1*1*n_c를 가지는 결과를 냄
    - 수학적으로 이것은 FC와 동일함! 각 노드가 같은 차원의 필터를 가지고 있기 때문에 임의 선형 함수를 가지고 있기 때문에 이전의 층에 대한 activation을 가지기 때문이다.
- 1*1 필터를 사용한다.
- softmax


![](https://github.com/matamatamong/img/blob/main/Visualizations/Convolution_implementation_of_sliding_windows.PNG?raw=true)

### Turning FC layer into convolutional layers
![](https://github.com/matamatamong/img/blob/main/Visualizations/Turning_FC_layer_into_convolution_layers.PNG?raw=true)


### Convloution implementation of sliding windows

![]()

Convloution implementation of sliding windows은 위 사진의 예시에서, 들어오는 입력 이미지에 대해서 독립적으로 forward propagation을 수행하는 대신, 들어온 이미지를 한 가지 계산으로 통합하는 것이다. <br>
앞서 봤던 Sliding Windows Detection Algorithm 방식은 윈도우 당 한번의 ConvNet 을 거쳐야하기 때문에 자원이 많이 들었지만 그렇게 하지않고 
- FC를 합성곱으로 만듦
- 테스트 이미지를 조금 늘려서 각각의 구역에 맞는 레이어를 얻어서 이것을 공유함
위의 것들을 이용해서 큰 합성곱 신경망에 한 번만 통과시키는 것이다.

<br>

![]()

<br>

이 정도면 효율적으로 개선했지만 바운딩박스의 위치가 정확하지 않을 수 있다. 또 이것을 해결해보자.

<br>

## Bounding box predictions
바운딩 박스를 잘 칠 수 있는 알고리즘이 없을까? 바로 있다.<br>

<br>

### YOLO Algorithm (You Only Look Onece)
그리드를 이용하여 각각의 그리드에 우리가 배웠던 이미지 구분을 하는 것이다. <br>
그렇게 하면 각각의 그리드 셀에 대한 벡터 아웃풋이 나온다.
그렇다고 그리드에 대해서 각각 합성곱을 수행하는게 아니라 한 번의 Convolutional 구현으로 이루어진다. 수행도 빨라서 인기가 많은 듯. YOLO 논문은 어려운 논문 중에 하나니까 이해 못 해도 너무 힘들어하지마셈

<br>

### Specify the bounding boxes
각 그리드 셀에는 바운딩 박스가 어떻게 할당이 될까? <br>
하나의 그리드 셀은 왼쪽 위부터 (0, 0) 오른 쪽 아래 (1, 1) 까지의 크기를 가지고 있다.
- bx :  물체의 중앙점을 정하고 중앙점으로부터 오른쪽까지의 물체의 길이
    - 0~1 까지의 길이를 가지고 있어야한다. (중앙점부터 시작해서 길이를 재므로 벗어나면 셀을 벗어나게 되는 것임 )
- by : 뮬체의 중앙점을 정하고 중앙점으로부터 위쪽까지의 물체의 길이
    - 0~1 까지의 길이를 가지고 있어야한다.
- bw: 그리드 셀 전체 너비에 비한 객체의 너비
    - 1보다 클 수 있지만 음수는 되지않는다. (객체의 너비가 너무 큰 경우가 있을 수 있음)
- bh: 그리드 셀 전체 너비에 비한 객체의 높이
    - 1보다 클 수 있지만 음수는 되지않는다.

## Intersection Over Union
물체 감지 알고맂므이 잘 동작하는지 어떻게 알 수 있을까? <br>
`Intersection Over Union(합집합 위의 교집합, IOU)` 함수를 알아보고 이것으로 물체 감지 알고리즘의 평가와 향상을 해보자. <br>

### Evaluating object localization
IOU는 교집합의 크기를 계산해서 그것을 합집합의 크기로 나눈 것이다.
따라서 이 측정값을 이용하면 두 경계 상자의 IOU를 계산하면 바운딩박스가 잘 안착했는지 알 수 있다. <br>
관례적으로 IOU가 0.5보다 크면 맞다고 판단할 것이다. 완전히 겹치면 1이겠다. 

<br>

![](https://github.com/matamatamong/img/blob/main/Visualizations/IOU.PNG?raw=true)

<br>

## Non-max Suppression
지금까지 배운 Object Detection 알고리즘의 문제 중 하나는 알고리즘이 같은 물체를 여러 번 감지할 수 도 있다는 점이다. 하지만 걱정마시라 <br>
`Non-max Suppression(비-최댓값 억제)`는 한 물체만 감지하도록 보장해준다. <br>

### Non-max Suppression example

![](https://github.com/matamatamong/img/blob/main/Visualizations/Non-max%20Suppression1.PNG?raw=true)

<br>
실제로는 두 개의 차가 있지만 그리드 셀들이 각각 본인이 자동차를 찾았다고 생각하게 될 수도 있다. (노란색과 초록색들)
<br>

그래서 `Non-max Suppression`은 감지된 것들을 정리해준다. <br>
- 먼저 각 감지의 확률을 살펴본다. 
- 가장 큰 값을 고른다.
- 그곳에서 물체를 찾았다고 하이라이트해준다.
- 나머지 직사각형들을 검사해서 가장 큰 값을 가진 직사각형과 많이 겹치는, 즉 IOU가 높은 직사각형들을 억제시켜준다.
- 그렇게 억제시켜준(어둡게 표시된)  직사각형들을 제거하고나면 하이라트된 직사각형만 남게된다.
이것이 `Non-max Suppression` 인 것이다. 이름 그대로 최대가 아닌 것들을 억제하는 것이다. 
<br>

조금 더 구체적으로 살펴보면 이렇다.

- 그리드 셀의 각각의 아웃풋이 있다.
    - (P_c, b_x, b_y, b_h, b_w)
- P_c(감지하고싶은 물체일 확률)이 0.6이하면 다 제외한다.
- 남아있는 박스에 대하여:
    - P_c가 제일 높은 것을 출력으로 예상한다.
        - 앞에서 하이라이팅했던 부분
    - 남아있는 박스 중에 IOU가 0.5보다 높은 부분들은 다 제외한다. (제외된 박스를 어둡게 표현함)

이것은 단일 물체에 대한 이야기이고 여러 물체를 감지해야한다면? <br> 
아웃풋 벡터에는 추가적으로 감지해야하는 요소들이 출력으로 나올 것이고 그렇게 되면 각각의 결과 클래스에 대해서 독립적으로 억제를 해줘야한다. 나중에 프로그래밍 과제로 구현 할 수 있다고함 굿~~

<br>

## Anchor Boxes
지금까지 배운 Object Detection 알고리즘의 또 하나의 문제는 각 그리드 셀이 하나의 물체만 감지할 수 있다는 것이다.그리드 셀이 여러개의 물체를 인식하게끔 하고싶으면 어떻게 해야할까? 앵커박스를 쓰자! <br>

### Overlapping objects
모양이 다른 앵커 박스라고 불리는 박스들을 예측에 대해서 여러개 겹친다. 그리고 그 앵커박스의 결과값을 하나로 합친 뒤,  예측과 비슷한 앵커박스 아웃풋만 살린다. <br>
이전에는 각각의 트레이닝 이미지에 있는 물체는 중심점이 있는 그리드 셀 안에 속해있었다. <br>
앵커박스를 적용하고 나면 각각의 트레이닝 이미지에 있는 물체는 중심점이 있는 그리드 셀과 IOU가 높은(겹치는게 많은)앵커박스에 속해있는 것이다. 그래서 아웃풋의 차원이 더 많아진다. <br>
앵커수보다 물체수가 더 많지 않도록 도는 물체가 똑같은 앵커박스를 쓰지 않도로 조심해야한다.
<br>
![](https://github.com/matamatamong/img/blob/main/Visualizations/Anchor_Box_Example.PNG?raw=true)

<br>

## YOLO Algorithm 구성
물체감지의 대부분의 요소를 배웠으니 이것들을 한데모아 YOLO 물체감지 알고리즘을 구성해보자 <br>
응 교수님의 정리가 더 괜찮아서 응 교수님의 정리로 대처

![](https://github.com/matamatamong/img/blob/main/Visualizations/YOLO1.PNG?raw=true)
![](https://github.com/matamatamong/img/blob/main/Visualizations/YOLO2.PNG?raw=true)

<br>

## Semantic Segmentation with U-Net
Object Detection이 아닌 Object Recognition의 하나를 알아보자. 이미지를 주면 고양이인지 아닌지를 알아보는 것 처럼 말이다. `Semantic Segmentation` 은 감지한 물체에 아웃라인을 그려서 픽셀이 Object에 속하는지 아닌지를 알 수 있게 하는 것이다. 요즘에는 광고같은 어플리케이션에 잘 쓰인다고 한다.
<br>

### Semantic Segmentation이 뭔데?
물체인식 알고리즘을 사용하면 물체에 바운딩 박스를 그리는 것이 목적일 것이다. 그런데 거기서 그치지않고 그냥 이미지의 각각 모든 픽셀들을 파악하고 싶다면 `Semantic Segmentation` 알고리즘을 쓰는 것이다. <br>
주행가능한 도로인지를 파악하기 위해서는 바운딩박스를 그리는 것만으론 충분하지않기 때문에 아예 도로를 분할해버리는 것이다.

![Object Detection VS Semantic Segmentation](https://github.com/matamatamong/img/blob/main/Visualizations/Object_Detection_VS_Semantic_Segmentaion.PNG?raw=true)

### Motivation for U-Net
이미지에서 각각의 역할을 분할을 해주면 어떨까? MRI를 보고 뇌의 종양부분을 분할해서 색을 칠해준다던지같은 것 말이다. <br> 
이러한 것을 가능하게 하는 알고리즘이 `U-Net` 이다.

<br>

### Per-pixel class labels
`Semantic Segmentation` 의 `U-Net` 알고리즘은 다음과 같이 하나의 픽셀마다 label을 붙여서 아웃풋해준다. 
<br>
![Per-pixel](https://github.com/matamatamong/img/blob/main/Visualizations/Per-pixel_class_labels.PNG?raw=true)

그런데 이렇게 되면 엄청나게 많은 아웃풋이 나오게 된다. 신경망을 어떻게 설계해야좋을까?
<br>

![Deep_Learning_Semantic_Segmentation](https://github.com/matamatamong/img/blob/main/Visualizations/Deep_Learning_for_Semantic_Segmentation.PNG?raw=true)

결과적으로는 이렇다. 이미지가 인풋이 되면 적은 set의 activation을 취하다가 크게 부풀리는 것이다. 이것을 하려면 `Transpose Convolutions` 를 알아야한다. 잠깐 한 번 살펴보고 오자.

<br>

## Transpose Convolutions
`U-Net` 알고리즘의 핵심인 `Transpose Convolutions` 을 알아보자. 보통 필터를 가지고 Convolution을 하면 너비가 작아지면 작아졌지 커지진않는다. 커지려면 채널이 커졌지! 그렇다면 어떻게 2*2의 입력을 부풀려서 4*4의 아웃풋을 낼 수 있을까? 함 해보자. 
<br>

![Transpose Convolutions1](https://github.com/matamatamong/img/blob/main/Visualizations/Transpose_Convolution1.PNG?raw=true)

- 아웃풋 결과값에 미리 패딩을 준다.
- 인풋의 한 픽셀을 필터의 모든 픽셀에 곱해준다.
- 그 값들을 아웃풋에 대는데, 이 때, 패딩이 들어간 곳은 비워둔다.

<br>

![Transpose Convolution2](https://github.com/matamatamong/img/blob/main/Visualizations/Transpose_Convolution2.PNG?raw=true)

- 다음으로 그것을 계속 하는데 만약 겹치는 곳이 있으면 값을 더해준다.

<br>

`Transpose Convolutions` 은 필터를 인풋이미지에 대보는 대신, 필터를 아웃풋에 대보는 것이다! 이렇게 함으로써 아웃풋을 부풀릴 수 있고 필터를 다 학습할 수 있기 때문에 결과가 좋게 나올 수 있게되는 것이다. <br>
`Transpose Convolutions` 를 알았으니 이제 다시 `U-Net` 으로 돌아가보자.

<br>

## U-Net Architecture Intuition
- 평범한 CNN을 한다.
- 그러다보면 너비가 작아진다.
- 너비가 작아지면 디테일한 정보들이 사라진다
- 이 때, `Transpose Convolution` 을 해서 사이즈를 부풀리는 것이다.

이것이 앞에서 살짝 맛 봤었던 `U-Net` 이다. 하지만 여기서 뭐가 더 추가된다. 바로 `Skip Connection` 을 해주는 것이다! <br>
왜 하냐고? 마지막 레이어에서 어느 부분이 인지하고싶은 물체인지를 알려면 두가지 정보가 필요하다.
- 이전 층에서 받을 수 있는 신경망 흐름, 즉 고레벨 정보
- 신경망이 깊어질 수록 얻을 수 없어지는 디테일한 정보
이것을 다 받아볼 수 있게끔 `Skip Connection` 을 해서 두 가지 정보를 다 받아볼 수 있도록 하는 것이다.

<br>

## U-Net Architecture

![U-Net Architecture](https://github.com/matamatamong/img/blob/main/Visualizations/U-Net_Architecture.PNG?raw=true)

<br>
알파벳 U같이 생긴 모양에 U-Net 이란 이름이 생겼다.

<br><br> <br>

# Kaggle
## [Porto Data Preparation & Exploration](https://www.kaggle.com/bertcarremans/data-preparation-exploration)
어제는 
- 데이터 확인
- 메타데이터 생성
을 하고 통계확인을 하고 있었던 때였다. target의 값이 0에 너무 치우쳐져있어서 `undersampling`하기로 했었음 

<br>

### 과정
- 통계 확인
    - resampling
- 데이터 퀄리티 확인
    - `NULL`, `missing value` 확인
        - 많이 없으면 mean값으로 매워버림
            - sklearn의 impute.[`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)로 빠르게 매워버리기
            - `fit_transform`은 train에만 쓰고 test에는 `transform`을 쓴다잉. train과 test를 다르게 하기 위해서!
        - 너무 많으면 `Drop` 하는 것도 나쁘지않음.
        - 하지만 이 데이터 자체도 중요한 것을 알려줄 수 있기 때문에 별도의 카테고리에 보관해둬야함. 잘 보자.


## Undersampling & Oversampling
클래스가 어느 한 쪽에 치우쳐있게 되면 모델이 불균형하게 된다.
### Undersampling
다른 클래스에 비해 상대적으로 많이 있는 클래스의 개수를 줄여버림. 균형은 유지되지만 그만큼 유용한 정보가 버려진다.
### Oversampling
데이터를 복제해버리는 것. Overfitting이 될 수 있다.
- `SMOTE(Synthetic Minority Over-sampling Technique)`
    - Overfitting을 피하기 위해서 적은 쪽의 클래스의 샘플을 가져와서 임의의 값을 추가한 새로운 샘플을 만들어버리는 것임
    - 주변 데이터를 고려하기 때문에 Overfitting을 좀 예방할 수 있다.


<br><br><br>
내일 [여기](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets) 로 가서 `resampling` 정리하기

* * *
참고 
<br>

