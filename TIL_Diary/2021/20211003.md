# Pytorch nn.transformer
ML Fullstack Bootcamp 2021에 pytorch의 `transformer`를 이용하길래 `nn.transformer` 을 이용한 언어 모델링 튜토리얼 한 번 뜯어봄. <br> <br>

pytorch의 기본 `transformer` 모듈은 `Attention is All You Need` 논문을 베이스로 함. transformer는 `sequence-to-sequence` 작업에서 아주 좋음. <br>
언어 모델링 작업은 주어진 단어(또는 sequence의 단어)가 sequence의 단어를 따를 가능성에 대한 확률을 할당하는 것이다. (저번 20211001스터디에서 확인한 부분 ) <br>

- sequence의 토큰이 먼저 임베딩 계층으로 전달
- 그 다음에 단어의 순서를 설명하기 위해 위치 인코딩 계층이 전달

`nn.TransformerEncoder`의 `self-attention` 레이어는 sequence의 이전 포지션에만 접근할 수 있으므로 입력 sequence와 정사각형의 `attention mask` 가 필요하다.

<br>

언어 모델링 작업의 경우, 미래에 있는 포지션의 모든 토큰이 마스킹되어야한다. <br>
출력 단어에 대한 확률 분포를 생성하기 위해서 `nn.TransformerEncoder` 모델의 아웃풋은 `log-softmax` 함수가 따라오는 선형 layer를 통해서 전달된다. (대충 마지막 레이어가 `log-softmax` 라는 듯) <br>


<br><br>

## PositionalEncoding 코드

```python
class PositionalEncoding(nn.Module):
  """
  Pytorch docs에 있는 기본 Attention-is-all-you-need 위치 encoding.
  시퀀스에서 토큰의 상대 또는 절대 위치에 대한 정보를 주입하는 것.
  위치 인코딩은 embedding과 동일한 차원을 가지므로 둘을 합산할 수 있음. 여기서는 서로 다른 주파수의 사인 및 코사인 함수를 사용
  """

  def __init__(self, d_model, dropout=0.1,  max_len=5000):
    super(PositionalEncoding, self).__init__()
    self.dropout = nn.Dropout(p=dropout)

    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0).transpose(0, 1)
    self.register_buffer('pe', pe)

  def forward(self, x):
    x = x + self.pe[:x.size(0), :]
    return self.dropout(x)
```

<br><br>

## generate_square_subsequent_mask 코드
```python
def generate_square_subsequent_mask(size: int):
  """
  삼각형 (size, size) mask를 생성한다. (Pytorch docs에 있음)
  """
  mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)
  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0, 0))
  return mask
```
<br><br>

## 인코딩/디코딩 하는 Transformer
```python
class Transformer(nn.Module):
  """
  인코딩과 디코딩을 모두 수행하는 기본 Transformer.
  예측 시간 추론은 greedy 형식으로 수행된다.
  
  참고: start 토큰은 0으로, end 토큰은 1로 하드코딩되어있음. 이거 변경할 시 predict() 변경해야함.
  """
  def __init__(self, num_classes: int, max_output_length: int, dim: int = 128):
    super().__init__()

    # 파라미터들
    self.dim = dim
    self.max_output_length = max_output_length
    nhead = 4
    num_layers = 4
    dim_feedforward = dim

    # 인코딩 파트
    self.embedding = nn.Embedding(num_classes, dim)
    self.pos_encoder = PositionalEncoding(d_model=self.dim)
    self.transformer_encoder = nn.TransformerEncoder(
        encoder_layer = nn.TransformerEncoderLayer(d_model=self.dim, nhead=nhead, dim_feedforward=dim_feedforward),
        num_layers = num_layers
    )

    # 디코딩 파트
    self.y_mask = generate_square_subsequent_mask(self.max_output_length)
    self.transformer_decoder = nn.TransformerDecoder(
        decoder_layer = nn.TransformerDecoderLayer(d_model=self.dim, nhead=nhead, dim_feedforward=dim_feedforward),
        num_layers = num_layers
    )
    self.fc = nn.Linear(self.dim, num_classes)

    # 가중치를 적절하게 초기화하는 것이 중요
    self.init_weights()

  def init_weights(self):
    initrange = 0.1
    self.embedding.weight.data.uniform_(-initrange, initrange)
    self.fc.bias.data.zero_()
    self.fc.weight.data.uniform_(-initrange, initrange)
  
  def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
    """
    input:
      x: (0, C)의 요소가 있는 (B, Sx) (C는 num_classes)
      y: (0, C)의 요소가 있는 (B, Sy) (C는 num_classes)
    
    output:
      (B, C, Sy) logits
    """
    encoded_x = self.encode(x)  # (Sx, B, E)
    output = self.decode(y, encoded_x)  # (Sy, B, C)
    return output.permute(1, 2, 0)  # (B, C, Sy)
  
  def encode(self, x: torch.Tensor) -> torch.Tensor:
    """
    input:
      x: (0, C)의 요소가 있는 (B, Sx) (C는 num_classes)
    
    output:
      (Sx, B, E) embedding
    """
    x = x.permute(1, 0) # (Sx, B, E)
    x = self.embedding(x) * math.sqrt(self.dim) # (Sx, B, E)
    x = self.pos_encoder(x) # (Sx, B, E)
    x = self.transformer_encoder(x) # (Sx, B, E)
    return x
  
  def decode(self, y: torch.Tensor, encoded_x: torch.Tensor) -> torch.Tensor:
    """
    input:
      encoded_x: (Sx, B, E)
      y: (0, C)의 요소가 있는 (B, Sy) (C는 num_classes)
    
    output:
      (Sy, B, C) logists
    """
    y = y.permute(1, 0) # (Sy, B)
    y = self.embedding(y) * math.sqrt(self.dim) # (Sy, B, E)
    y = self.pos_encoder(y) # (Sy, B, E)
    Sy = y.shape[0]
    y_mask = self.y_mask[:Sy, :Sy].type_as(encoded_x) # (Sy, Sy)
    output = self.transformer_decoder(y, encoded_x, y_mask) # (Sy, B, E)
    output = self.fc(output)  # (Sy, B, C)
    return output

  def predict(self, x: torch.Tensor) -> torch.Tensor:
    """
    inference time에 사용할 메소드. 한번에 하나의 토큰 x에서 y를 예측한다. 그리디 디코딩을 사용. Beam search도 대신 사용할 수 있음!

    input:
      x: (0, C)의 요소가 있는 (B, Sx) (C는 num_classes)
    
    output:
      (B, C, Sy) logits
    """
    encoded_x = self.encode(x)

    output_tokens = (torch.ones((x.shape[0], self.max_output_length))).type_as(x).long()  # (B, max_length)
    output_tokens[:, 0] = 0 # start 토큰 세팅
    for Sy in range(1, self.max_output_length):
      y = output_tokens[:, :Sy] # (B, Sy)
      output = self.decode(y, encoded_x)  # (Sy, B, C)
      output = torch.argmax(output, dim=-1) # (Sy, B)
      output_tokens[:, Sy] = output[-1:]
    return output_tokens
```

<br><br>

코드 이해하는데 시간 너무 들었고 다 이해가 안되는 것 보니 내일 `seq2seq`이랑 `attention`, `transformer` 다시 복습해야할듯. 그래도 pytorch, numpy와 계속 친해지고 있어서 좋음.