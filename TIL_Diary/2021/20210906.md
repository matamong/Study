# ML
# Coursera - Coursera - Sequence Models - Introduction to Word Embeddings
이제까지 RNN, GRU, LSTM을 배웠다. 이것들은 NLP에 적용할 수 있다.
## Word Representation
이때까지 단어들을 사전에 정의하고 one-hot 벡터로 만들어줬었다. <br>

```
I want a glass of orange ____.
```
모델의 학습 알고리즘으로 위 문장에 juice가 들어가야한다고 학습시켰다고해보자. 

```
I want a glass of apple ____.
```
위 문장에서 우리는 빈칸에 juice가 들어가야할 것을 알고있다. 하지만 학습모델은 우리처럼 `orange`와 `apple`과의 관계를 파악해서 빈칸이 juice일 것이라고 생각하지 않는다. 학습모델은 `orange`와 `apple`과의 관계성을 찾지 못하기 때문에 juice를 출력하지 않을 것이다. `orage`와 `apple`과의 관계나 `orange`와 `Woman` 과의 뜬금없는 관계나 학습모델은 그 관계의 인기도를 인지하지 못한다. 왜 그럴까? <br>
그저 one-hot 벡터를 인식하기 때문이다. 단어사전에서 어떤 것이 일치하는지만 알 수 있고 어떤 단어와 어떤 단어가 비슷한지를 알 수 있는 방법이 없다.

<br>

### Featurized representation: world embedding
위의 문제에서, 각각의 단어들의 특징을 수치화해주면 말이 달라진다.

![Featurized representation_world embedding]()

위 그림과 같이 단어에 각각의 feature와 그에 대한 value를 정의하고 책정한다. <br>
만약, 300개의 특징과 300개의 값이 있다고 하면 300차원의 벡터가 단어 `Man` 을 나타내기 위해 존재한다. <br>
학습 알고리즘은 이러한 특징값의 비슷한 것들을 학습하고 `orange`와 `apple`이 비슷한 것이라는 것을 인지할 수 있다. 이것이 Word Embeddings이다. feaureized 과정이 보기보다 어렵긴하지만 특징을 학습시키기에 좋다. <br> 

<br>

![Visualizing word embeddings]()
이렇게 embedding한 300D 데이터를 2D로 만들어서 시각화 해주면 관계를 파악하기 쉬워진다. `t-SNE` 알고리즘을 사용한다.

<br><br>

## Using Word Embeddings
Word Embeddings을 NLP에 어떻게 사용할까? 이름 엔티티를 인식하는 예를 보며 알아보자. <br>
- Sally Johnson이 이름일것이다.
- 단어들을 one-hot 벡터로 만들어줘야하는데 여기서 featurized embedding vector를 사용한다.
    - x^<1>, x^<2>...

이렇게 입력을하고 학습을 하게 되면 비슷한 문장에서 특징의 관계성을 통해 이름을 찾는 것이 더 쉬워진다. <br>
이를 통해 굉장히 큰 학습 데이터가 unlabled text 를 가지고 있다고 해도 embedding들을 학습하면서 비슷한 것들을 grouping해줄 수 있다. 그리고 이것을 transfer해서 작은 학습데이터셋이 있는 곳에 적용시킬 수 있다. <br> 
인터넷에서 무료로 얻을 수 있는 글들을 학습시키기 편해지는 것이다.

### Transfer learning and word embeddings
word emdeddings를 통해서 transfer learning을 어떻게 할 수 있는지 알아보자.
- 큰 text corpus를 가지고 word embedding를 학습한다. (1~100B words)
    - 아니면 pre-trained embedding을 온라인에서 다운로드 할 수 있음
- 적은 학습데이터셋을 가지고 있는 새로운 작업에 embedding을 transfer한다. (100k words)
- 새로운 데이터를 가지고 word embedding을 계속해서 finetune한다. (선택적)

<br>
transfer learning은 데이터가 굉장히 많은 A에서 비교적 적은 데이터셋을 가진 B로 갈 수 있을 때 잘 활용될 수 있다. 물론 NLP 작업에 많은 도움이 되지만 몇몇 language modeling이나 기계번역에는 잘 쓰이지않는다.

<br><br>

## Properties of Word Embeddings
```
Man -> Woman
and
King -> ___
```
빈칸에 들어가야 할 단어는 뭘까? 인간은 당연히 Queen을 연상할 것이다. 학습모델이 이러한 유추를 가능케할 수 있을까? <br>
Embedding을 이용해서 이를 파악할 수 있다. 바로 featurized value의 차를 이용하는 것이다. <br>
`e_man과 e_woman의 차`와 `e_king과 e_?의 차 `가 비슷하면 이게 가능한 것이다.

![Analogies]()
![Analogies using word vactors]()

<br>

![Cosine similarity]()

- similarity를 계산할 때 자주 사용되는 것이 Cosine 유사도이다.
    - u와 v가 비슷하다면 값이 커질 것이다.
    - 원하면 유클리디안 거리를 사용할 수 있음.

<br>
이렇게 Embedding을 이용하면 유추도 할 수 있게된다.

<br><br>

## Embedding Matrix
10,000개의 사전이 준비되었고 300D의 차원을 가진 Embedding이 있다고 해보자

![Embedding matrix]()

- Embedding matrix는 `E`로 표현한다.
- `O_6257` 은 사전이 모두가 제로이지만 하나의 위치에만 1값이 들어가 있는 one-hot 벡터일 것이다.
- E * O_6257은 (300,1)일 것이다. 이것은 e_6257 이라고 나타낸다.
    - E: (300, 10,000)
    - O_6257 : (10,000, 1)
    - 이것은 Orange에 대응하는 300 차원의 column을 찾아내는 것이다.

<br><br>

# Coursera - Coursera - Sequence Models - Learning World Embeddings: Word2vec & Glove

## Learning Word Embeddings
word embedding을 학습할 수 있는 구체적인 알고리즘을 배워보자. <br>
이것들은 어려운 것에서부터 점점 간단해졌기 때문에 간단한 것부터 먼저 보고 이해하기보단 어려운 것부터 살짝 살펴보자. <br> 

### Neural language model

![Neural language model]()

신경망이 입력을 받으면 시퀀스에 따라 마지막 빈칸을 채우기를 바란다고 해보자. 밑에 있는 숫자는 사전(10,000)의 인덱스이다. 
- I에 대응하는 one-hot 벡터(10,000, 1)를 만들 것이다. -> `o_4343`
- embedding matrix E(300, 10,000) 와 one-hot 벡터를 곱한다. -> `E`
- 이렇게 해서 300차원의 embedding 벡터인 `e_4343` 을 얻는다.
- 다른 단어들에도 이 방식을 적용한다.
- 이것들을 신경망에 먹인다.
    - 300*6 = 1800차원의 벡터
    - 항상 빈칸에 가까운 단어들만 취할 수도 있음.
        - 300*4= 1200
    - w^[1], b^[1] 의 파라미터를 가진다.
- softmax(10,000)를 거친다.
    - w^[2], b^[2] 의 파라미터를 가진다.
- 이 알고리즘은 E,w^[1], b^[1],w^[2], b^[2] 파라미터를 가진다.

하지만 더 긴 문장이 주어졌을 때, 최근에 사용됐던 가까운 단어들을 사용해야만 한다. 
- 최근 앞 뒤에서 사용된 단어
- 마지막 한 단어
- 근처에있는 한 단어
도 사용해보자.

<br><br>

## Word2Vec
좀 더 간단하고 효율적인 알고리즘 Word2Vec을 알아보자.<br>
Word2Vec은 자연어 처리에서 단어를 밀집 벡터로 만들어주는 단어 임베딩 방법론이다. <br>
최근에는 자연어 처리를 넘어 추천 시스템으로도 사용된다고 하니 잘 알아봅세다.

### Skip-grams

Skip-grams모델은 context와 target 쌍을 이용해서 지도학습문제를 해결하는 것이다.
- 무작위로 context 단어를 선택한다.
- context 단어의 widow 안에서 무작위로 다른 타겟 단어들을 선택한다.
즉, 중심 단어에서 주변 단어를 예측하는 것이다.
지도학습문제를 해결하기엔 힘들지만 word embedding을 위하기엔 아주 좋다.

![Skip-grams Model]()

- contex c에서 target t를 지도학습하고 싶다.
- O_c로 만들고 E*O_c를 해서 embedding 벡터 e_c를 얻는다.
- softmax를 해서 y^^을 출력한다.
    - `Θ_t` : 아웃풋 t와 연관된 파라미터
    - target y는 target의 인덱스가 1이 되는 one-hot 벡터가 된다.
- bias는 여기서 보여주지않았음.
이렇게 해주면 꽤 괜찮은 embedding 벡터를 가질 수 있다.

<br>

![Problems with softmax classification]()

이 모델은 문제가 하나 있다. softmax를 거칠 때 사전의 크기가 크기 때문에 계산이 힘든 것이다. 10,000크기의 사전도 이미 힘들다. 그래서 다음과 같은 기능을 쓴다.
- Hieararchical softmax
    - 쓸 때는 자주 쓰이는 단어들을 위쪽에 분포시키고 자주 쓰지 않는 단어를 아래쪽에 분포시켜서 효율을 높여줌.
- negative sampling


Skip-gram보다 좀 더 간단하고 더 효율적인 것이 있으니 넘 자세히 파진말라고하네..!

<br><br>

## Negative Sampling
Skip-grams 모델과 비슷하지만 Negative Sampling을 적용해서 더 효율적인 학습알고리즘을 만들어보자. <br>

### train set 만들기
- context와 target을 샘플링해서 1로 labeling 해준다.
- context의 window에 맞게 k번을 랜덤으로 target을 샘플링해서 0으로 labeling 해준다. (k = 5~20)
    - 데이터가 많을 때는 k = 2~5, 적을 때는 k = 5~20
이것이 training set을 만드는 방법이고 `context,target` 쌍을 입력 x로 하고 target y의 label을 예측할 것임

### SGNS Model

![SGNS Model]()


- O_c로 만들고 E*O_c를 해서 embedding 벡터 e_c를 얻는다.
- 10,000개의 logistic regression 구분문제를 가진다.
    - softmax대신, y=1의 경우, 입력 c,t쌍이 주어지는 logistic regression 모델로 만든다.
    - 각각은 target이 사전에 있는 단어인지 아닌지를 구분하는 문제를 가지고 있을 것이다.
- 매번 반복할 때마다 10,000개를 다 학습하는게 아니라 k+1 혹은 k-1개만 학습한다.
이렇게 해서 10,000번의 소프트맥스를 가지는게 아니라 10,000개의 binary classification 문제를 가져서 계산하기 쉽게 만들었을 뿐 아니라 매번 반복할 때마다 k+1 혹은 k-1개만 학습해서 더 자원이 적게 든다. <br>
negative word들을 통해서 학습을 하는데 이런 negative word를 어떻게 샘플링해야할까? <br>
논문의 저자는 다음과 같은 식이 최선이라고 하더라.

![Selecting negative examples]()

<br>

pre-trained word vector도 있으니까 그거 찾아서 쓰는 것이 좋다.ㅎ

<br><br>

## GloVe Word Vectors
Word2Vec은 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못한다. 어떻게 전체적인 통계 정보를 반영하면서 유사도를 측정해볼 수 있을까? <br>
GloVe 연구팀이 이 부분을 파고들었다. 

> 이 때문에 GloVe 연구팀은 임베딩된 두 단어벡터의 내적이 말뭉치 전체에서의 동시 등장확률 로그값이 되도록 목적함수를 정의했습니다. (their dot product equals the logarithm of the words’ probability of co-occurrence) “임베딩된  단어벡터 간 유사도 측정을 수월하게 하면서도 말뭉치 전체의 통계 정보를 좀 더 잘 반영해보자”가 GloVe가 지향하는 핵심 목표라 말할 수 있을 것 같습니다. <br> *출처: https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/09/glove/*

![Glove_J_function]()

임베딩 벡터를 만들 때, 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만든다. <br>

교수님이 선지식을 짜르고 시작해버려서 이 부분은 나중에 더 깊이 파봐야할 듯.

<br><br><br>

* * *
참고 <br>

https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/09/glove/ 

<br>

https://wikidocs.net/22660