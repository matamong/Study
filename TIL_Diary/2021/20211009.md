# Kaggle - PetFinder
## Regression -> Classification
`Pawpularity`를 [0, 1]에서 [0, 100]으로 Normalization하고 `RMSE` loss (regression loss) 대신 `BinaryCrossentropy` loss (classification loss)를 사용한다. 그 후에 예측을 Denormalization한 뒤 metric을 계산한다면 Classification의 성능이 더 우수하다고한다. 즉 이 대회의 중점은 **regression 문제를 classification으로 보고 해결**하는 것.

- https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275278

<br>

## No metadata
Input으로 metadata를 넣지않고 오직 이미지만 넣음. 모델이 더 느려지기만 할 뿐이라는 경험들이 있음.(현재 퍼블릭리더보드 2위, 4위 경험담)
- https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/276522

<br>

## No MixUp?
캐글 2위가 Mix-up은 효과가 별로 없었다고 하는 듯.


## Swin Transformer 모델
- `timm` 라이브러리가 제공하는 `pre-traiend Swin Transformer(ImageNet22k)` 을 이용할 수 있음. 

<br><br><br><br>

# SOTA
SOTA는 State-Of-The-Art 의 약자로 사전학습 된 신경망 중에 최고 수준의 신경망을 말한다.

<br><br><br><br>

# timm Library
`timm` 라이브러리는 SOTA CV 모델, 비전 보델, 레이어들, 유틸리티들, optimizer들, 스케쥴러들 그 밖의 다양한 것들의 콜렉션이다.
- https://github.com/rwightman/pytorch-image-models

<br><br><br><br>

# Grad-CAM
Gradient-weighted Class Activation Mapping의 약자로 마지막 ConvNet 레이어로 흐르는 대상의 그라디언트를 사용해서 이미지의 중요한 영역을 강조하는 히트맵생성해줌.

<br><br><br><br>

# logits.sigmoid().detach().cpu().numpy()
모듈ㅇ
- `logit` : Activation function을 거치지않은 값임
- `logits`에 sigmoid를 취하고 
- `detach()`를 이용해서 추적해놓은 연산 기록을 분리한 tensor를 반환한 뒤
- `cpu()`를 이용해서 GPU메모리에 있던 tensor를 cpu 메모리로 복사
- `numpy()`를 이용해서 tensor->numpy로 변환.

<br><br><br><br>

# ttach library
training set에 augmentation을 하는 것 처럼 test image에도 랜덤 augmentation을 적용하고 적용된 각 이미지의 예측값 평균을 내서 최종 예측을 내줌. 편향을 줄여줄 수 있다.

- https://github.com/qubvel/ttach

<br><br><br><br>

# CUDA toolkit
코랩이랑 캐글노트북이 너무 느려서 내 GPU 좀 사용해보려고 했더니 환경설정이 사람을 약간 미치게하는군..반나절을 환경설정을 해버렸다... 🤦

```
conda create -n 가상환경이름 python=버전
conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
```

```
print(torch.cuda.is_available())
```