# ML
# Coursera - Hyperparameter Tuning
## **Batch Norm at Test Time**
테스트 할  때 배치정규화를 알아보자. <br>
배치 정규화는 한 번에 하나의 미니배치 데이터를 처리한다. 그런데 테스트할 때는 한번에 샘플 하나를 처리한다. 그럼 테스트할 때는 어떻게 해야할까?

- 테스트할 때는 샘플이 하나일텐데 그 샘플의 평균과 분산을 구하는 것은 말도안되는 일이다.
- 그래서 μ와 σ²(층의 평균과 분산을 하는 식)을 훈련세트로부터 추정해준다. 
- 여러가지 방법이 있는데 실제로는 지수가중평균을 사용한다.
    - μ와 σ²가 학습하면서 가진 값을 추적하여 대충 결정한 뒤 사용하는 것이다.
- 이렇게 함으로써 은닉층의 z값을 조정할 때 훈련세트로부터 대충 추정해준 μ와 σ²값을 사용한다.
- 물론 딥러닝 프레임워크로 구현가능하다.ㅎ

<br><br>

## Multi-class Classification
지금까지 봤던 분류문제는 이진분류였다. 그럼 여러분류를 하려면 어떻게 해야할까? Softmax 활성화 함수를 이용할 수 있다.

### Softmax
Hardmax(z벡터를 받아와서 가장 큰 원소만 1로 만들고 나머지는 0으로 만들어주는)와 반대되는 이름인 Softmax는
입력받은 값을 정규화하여 0~1사이의 값으로 출력하며 출력 값들의 총합은 항상 1이되게해주는 특성을 가진 함수이다. 이 때문에 주로 출력증에서만 사용된다. (시그모이드랑 비슷하네)


#### 식
![](https://miro.medium.com/max/2000/1*eqQuFgXPUP5L6J_vVH19wg.png)

- 시그모이드와 마찬가지로 e를 쓰면 아무리 작은 값의 차이라도 확실히 구별될 정도로 커지고 또한 미분하기에도 좋다.
- 단순하게 k번일 확률을 전체확률로 나누는 것이다.

<br>

#### 특징
- 시그모이드나 Relu 활성화 함수 등은 실수를 받아서 실수를 내놓는다. 하지만 소프트맥스 활성화 하수의 특이점은 정규화를 하기 위해서 입력값과 출력값이 모두 벡터이다. 
- 2개의 클래스만 구분하는 경우에는 로지스틱회귀가 하나의 출력값을 계산하는 방식과 같아진다. 소프트맥스회귀는 로지스틱 회귀를 일반화한 것이기 때문이다.

<br>

#### 어떻게 소프트맥스를 이용하여 학습시킬까?

##### 소프트맥스의 비용함수
소프트맥스 분류에서 자주 사용하는 비용함수는 다음과같다.
![](data:image/png;base64,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)

<br><br><br>

## Deep Learning Frameworks
점점 복잡하고 큰 모델을 만들수록 처음부터 구현한다는 것은 효율적이지 못 하다. 그래서 프레임워크를 써볼 것이다.

<br>

### 딥러닝 프레임워크들
- caffe/caffe2
- CNTK
- DL4J
- Keras
- Lasagne
- mxnet
- PaddlePaddle
- Tensorflow
- Theano
- Torch

### 딥러닝 프레임 워크를 고르는 법
- 프로그래밍이 쉬워야된다.
- 실행 속도가 빨라야한다.
- 오픈소스일수록 좋다. 


### Tensorflow
프레임워크 중 하나인 Tensorflow를 잠깐 만져보자.
<br>
```python
import numpy as np
import tensorflow as tf
```

#### Minimize
텐서플로우의 장점 중 하나는 그저 전방향전파만 하면 다 알아서 해준다는 것이다. 즉, 비용함수의 값만 코딩하면 텐서플로우는 알아서 역전파나 경사계산을 한다는 것이다. <br>

##### Minimize variable
비용함수가 `cost = w** - 2-16 * w + 25 ` 과 같다고 쳤을 때 이것을 Minimize하는 w값이 뭔지를 텐서플로우를 통해 알아보자.

<br>

```python
w = tf.Variable(0, dtype.float.32)
optimizer = tf.keras.optimizers.Adam(0.1) # 0.1 은 Learning_rate이다.

def train_step():
    with tf.GradientTape() as tape:
        cost = w** - 2-16 * w + 25 
    trainable_variables = [w]
    grade = tape.gradient(cost, trainable_variables)
    optimizer.apply_gradients(zip(grads, trainable_variables))

```
- `GradientTape()` : forward prop 단계에서 비용 함수를 계산할 때의 작업 순서를 기록해두었다가 카세트테이프를 역순으로 돌리는 것처럼 역순으로 작업 순서를 다시 방문하고, 그 과정에서 역전파 및 기울기를 계산할 수 잇게해준다.
- `train_step()` 함수를 계속 돌리면 비용함수를 minimize하는 w값이 나오게 된다.
    - `w` : 5

<br>

##### Minimize function
그냥 w말고 함수를 minimize하고싶을 때는 어떻게 해야할까?

```python
w = tf.Variable(0, dtype.float.32)
x = np.array([1.0, -10.0, 25.0], dtype=np.float32)
optimizer = tf.keras.optimizers.Adam(0.1) # 0.1 은 Learning_rate이다.

def trainin(x, w, optimizer):

    def cost_fn():
        return cost = x[0]*w ** 2 + x[1]  * w + x[2]
    for i in range(1000):
        optimizer.minimize(cost_fn, [w])
```
- x를 비용함수의 계수를 조정하는 데이터라고 생각하면 되겠다.
- `optimizer.minimize` 는 최적화함수를 1스텝 취하는 것이다. 
    - 이 때, w값은 당연히 변함



<br>
(앤드류 응이 최근에 추가한 것으로 보이는 강의인데 뭔가 반가웠다.)

<br><br><br>

# Coursera - Structuring Machine Learning Projects - Introduction to ML Strategy
프로젝트의 방향성을 정하는 것은 아주 중요하다. <br>
그렇기에 **전략이 필요하다.**

## **Orthogonalization(직교화)**
머신러닝의 직교화란 각 단계에서 독립적인 튜닝을 할 수 있도록 해주는 것. 방향성과 함께 어플리케이션에 중요한 역할을 할 수 있을 것이다.
- e.g) 오버피팅되었다면 정규화를 딸깍하고 할 수 있게.

## 목표를 세우자
### Single Number Evaluation Metric(실수 평가 측정 지표?)
지표가 있으면 빠르게 결과를 확인 할 수 있다. 그러므로 실수평가측정지표를 사용하는 것을 권장한다. <br>
Precision 과 Recall등의 지표나, 평균값을 사용할 수 있다.

#### Precision(정밀도)
어제 kaggle공부에서 정리한 Precision. <br>
e.g) 맞다고 올바르게 판별하는 확률

#### Recall(재현도)
어제 kaggle공부에서 정리한 Recall <br>
e.g) 실제로 얼마나 올바르게 인식했는가?

<br>
Classifier들의 정밀도와 재현도를 보고 좋은 성능을 내는 것을 골라야겠다. 앤드류응은 정밀도와 재현도를 조화롭게 합친  **F1 Score** 를 추천했다.(어제 kaggle 공부에서 정리함) <br>

<br><br>

### Satisficing and Optimizing Metric
항상 실수평가측정을 하는 것이 쉬운 것은 아니다 그럴 땐 다음을 살펴보자.

#### Optimizing
정확도를 최대치로 하고싶은 것.

#### Satisficing
어느 정도 만족할만하면 그 이상은 그렇게까진 신경안쓰는 것.

<br><br>

### Train/Dev(Valid)/Test Distributions
Train/Dev/Test 을 설정하는 것은 아주 중요하다. 이것을 어떻게 설정하느냐에 따라서 느리게 진행될수도 빠르게 진행될수도있다. 
<br>
다음과 같이 피쳐들이 있다고 생각해보자.
```
- 지역
    - US
    - UK
    - Other Europe
    - South America
    - India
    - China
    - Other Asia
    - Autralia
```
- 대륙중에서 4가지를 골라 Dev로 나머지 4개를 Test로 하는 것은 정말 안 좋은 아이디어다.
- Dev와 Test는 같은 분포를 가져야한다.
- 모든 8개의 데이터에서 무작위로 뽑아야한다

<br>

이 과정을 잘못하면 몇개월간 삽질을 하게된다. 너무나 당연한 이야기!!

<br>

### Size of the Dev and Test Sets
분포가 같아야하는건 알겠는데 얼만큼 나눠야할까? <br>

- 오래된 방식
    - `train`과 `test` 는 7:3이 
    - `train`, `dev(valid)`, `test` 는 6:2:2
    - kaggle에서는 데이터가 많이 없어서 요렇게 하는게 좋을듯.

- 요즘 방식
    - 요즘엔 데이터가 엄청 많다. 데이터를 많이 가지고 있으므로 다음과 같이 한다.
    - `train`, `dev(valid)`, `test` 는 98%, 1%, 1%
    - `train` 은 정말 많은 데이터를 원하고있기 때문이다.

<br><br>

### 언제 Dev/Test 셋과 Metrics를 바꿔야할까요?
3%의 에러를 내는 A알고리즘과 5%의 에러를 내는 B알고리즘이 있다고 하자. 겉보기에는 3%의 에러를 내는 A알고리즘이 더 나아보일 것이다. 하지만 A알고리즘에서 에러는 적지만 포르노그래픽같이 보여줘서는 안될 것들을 구분하지 못하고있다면? <br>
이 때, Metric을 바꿔야할 것이다. 더 이상 기존의 Metrics로 뭐가 좋은 알고리즘인지 구분하지 못하기 때문이다. <br>
(이 때, Metric을 바꾸는 한가지 방법은 `weight` 변수를 넣어서 포르노그래픽같이 보여주고 싶지 않은 것에 큰 변수를 넣어서 에러율을 올려주는 방법이 있을 것이다.)
<br>
어플리케이션을 배포하고 유저가 예측값에 불만족할 때, 이 때에도 바꿔야할 것이다.


<br><br>

## 휴먼레벨퍼포먼스와 비교해보자.
### 왜 휴먼레벨퍼포먼스?
인간이 할 수 있는 것들(언어를 읽거나, 사진을 보거나) 까지는 여러가지 전략을 통해 발전할 수 있다. 하지만 알고리즘이 이미 인간레벨성능을 뛰어 넘었을 경우엔 전술을 적용하기가 힘들다.

<br>

### Avoidable bias
Bays error와 트레이닝 오류의 차이를 `avoidable bias` 라고 해보자(앤드류 응). <br>
`avoidable bias` 가 적으면 `varience`를, `avoidable bias` 가 크면 `bias`를 건드려야 효율적일 것이다. <br>
- `avoidable bias` 
    - 더 큰 모델을 학습시키자.
    - 오래 학습시키거나, 더 좋은 최적화 알고리즘을 써보자.
    - RNN/CNN 을 도입해보자.
- `varience`
    - 더 많은 데이터를 학습시키자.
    - 정규화 (L2, Dropout, Augmentation 등등)
    - NN 설계를 다시 하고, 하이퍼파라미터를 찾아본다.
Human-level Performance 를 알면 Bays error 를 알 수 있고 bias나 varience를 어떻게 해야할지에 대해서 감이 올 것이다.

<br>

### 인간레벨성능을 능가하는 퍼포먼스
인간레벨성능을 능가하면 머신러닝의 발전을 느려질 수 밖에 없다. Bay error를 알 수 없게 되고 뭘 어떻게 발전해야할지 모호해지기 때문이다. <br>
방대한 데이터와 structure data를 다루는 환경에서 이러한 일이 일어나기 쉽다. (e.g 광고추천, 추천시스템, 대출승인 등등..) 
<br> 
CV나 메디컬 쪽도 인간레벨성능을 뛰어넘고는 있긴하다.



<br>

<br><br><br><br>

* * *
참조 <br>
https://towardsdatascience.com/softmax-function-simplified-714068bf8156