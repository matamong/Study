# Vision Transformer (ViT)

2017년에 transformer가 등장한 이후로 NLP분야에선 **"LSTM is dead. Long live Transformers!"** 란 말이 나올정도로 transformer가 굉장히 대두돼고  Transformer 위주의 연구가 진행되고있다. 이런 Transformer의 성공이 CV분야에도 많은 영향을 끼치게되었다. CV 분야에서는 CNN에 `self-attention`을 어떻게 적용할까라는 연구를 하다가 이제는 Transformer 모델 자체를 이용하려고 하는 연구들이 진행되고있는데 ViT는 후자이다.

<br><br>

## CNN vs Transformer

### CNN

- CNN은 convolutional filter를 이용하여 몇 개의 layer를 통과해야 이미지 전체의 정보가 통합된다.
- 학습 후 가중치가 고정된다.

<br>

### Transformer
- Transformer는 이미지를 한 번에 1차원 vector로 만든 뒤 self-attention을 적용하여 Query, Key, Value를 계산한다. Query와 Key의 유사도를 통하여 가중치를 뽑아낸 뒤 Value에 곱해주면 self-attention의 output이 나온다. 즉, 레이어 하나만 거쳐도 멀리 떨어져있는 정보의 교환이 이루어질 수 있다.
- input에 따라 가중치가 유동적이다. 모델의 자유도가 높다.
- 학습데이터가 충분하지 않을 경우 CNN모델보다 성능이 감소할 수 있다.

<br><br>

## What is Vision Transformer
2020년 10월 경 나온 논문. CNN을 활용하지않고 오직 Transformer 구조를 활용해서 image classification을 수행한다.

<br><br>

![ViT]()

<br>

- 이미지를 몇 개의 patch로 나눈다.
    - NLP분야로 비유한다면 인풋 이미지가 전체 문장이면 patch가 단어라고 생각. 이미지를 시퀀스로 만드는 것.
- 각각의 patch들을 1차원 벡터로 풀어낸다.
- 이것을 Linear Projection을 거쳐서 embedding vector로 표현한다.
    - fully connected 레이어를 통과한다고 생각하자.
- 각 patch의 embedding에 `Classification token`과 `Position embedding` 이 추가된다.
    - `Classification token` 은 BERT의 cls토큰과 동일한 역할을 한다.
    - `Position embedding` 은 patch 위치정보를 나타낸다.
    - 이 두 개는 학습을 통하여 결정되는 파라미터이다.
- 이제 이것들은 Transformer의 인코더로 들어가게 된다. ViT base모델의 경우 이 인코더를 12층을 쌓는다.
    - ViT의 인코더는 원래의 Transformer 인코더와는 다른 형식이다. 깊은 레이어는 학습에 어려움을 준다는 이 단점을 극복하기 위해 연구 한 결과 layer Norm의 위치가 중요하다는 연구가 나왔고 ViT는 이 연구의 인코더를 사용한다.
- 인코더의 아웃풋은 각각의 patch에 대한 embedding들이 계산들이므로 이미지 전체의 embedding을 구하기 위해서는 `Classification token`을 도입하여 이것의 최종적인 embedding이 이미지 전체의 embedding인 것으로 가정한다(BERT식).

<br><br>

## Visiont Transformer 성능 개선 테크닉
Pre-trained 이미지 resolution(해상도)를 낮추고 Fine-tuning단계의 이미지 resolution을 올리면 모델의 성능이 올라가기 때문에 ViT에서는 이렇게 한다.

<br><br><br><br>

# Data efficient image Transformer (DeiT)
ViT의 "학습데이터가 충분하지 않을 경우 CNN모델보다 성능이 감소할 수 있다"는 단점을 보완하기 위해 Facebook에서 낸 논문.

<br><br>

## Knowledge distilation
먼저 `Knowledge distilation` 을 알아보고 가자. 이것은 **잘 학습된 모델의 지식을 transfer하여 좋은 성능을 내는 작은 모델을 구축**하는 것을 목적으로 한다. 자원이 부족할 때, 큰 모델만큼 성능을 내는 작은 모델을 구축하는 것이다. 큰 모델을 `Teacher model` 이라하고 작은 모델은 `Student(distiled) model`이라고 한다.

## What is DeiT

Vision Transformer와 동일한 구조이지만 `Knowledge distilation` token이 추가되며 이것은 학습을 통하여 결정되는 파라미터이다. DeiT는 `Teacher model`로 CNN인 `RegNet`모델을 이용한다. 그리고 Data augmentaion을 적극적으로 활용한다. DeiT는 일반적으로 Vit, EfficientNet 보다 좋은 성능을 낸다.


<br><br><br><br>

# **Swin Transformer**: Hierarchical Vision Transformer using Shifted Windows 
2021년 3월에 microsoft research에서 낸 논문이다. 기존 ViT에 대해 제기되었던 문제점들이 있었는데 그것은 다음과 같았다.
- 기존 ViT 모델이 분류문제를 위해 제안
- 텍스트와 달리 이미지를 위한 특성이 ViT에 없음
- token의 수가 증가함에 따라 연산량이 증가

<br>

이같은 문제가 대두되어서 Swin은 다음과 같은 목적을 가지고 설계되었다.
- 다양한 문제에 대하여 뼈대로 사용할 수 있는 모델을 제안
- 이미지의 특성을 반영할 수 있게
- 기존 ViT모델보다 더 적은 연산량을 가질 수 있게

<br>

## What is Swin?

- 로컬 윈도우(패치를 모은 것)를 모델에 적용한다.
    - 기존 ViT보다 더 적은 복잡도로 학습할 수 있다.
- class token을 사용하지않고 token들의 평균값을 사용하여 분류를 수행한다.

<br>

### 모델 구조

<br>

![Swin Transformer]()

<br>

- **Patch Merging** : 각 스테이지가 진행됨에 따라 해상도를 줄여준다.
    - 주변 패치의 정보를 가져다가 하나의 차원으로 축소해준다.
- **Swin Transformer Block** : 
    - **W-MSA**(Window Multi-Head Self Attention) : 로컬 윈도우 안에서 self attention
    - **SW-MSA**(Shifted Window Multi-Head Self Attention) : 로컬 윈도우 간의 self attention
    - 병렬적으로 처리된다.

<br><br>

## Experiemnets
- 다양한 Augmentation, Regularization이 필요하다.
- AdamW optimizer 사용
- Classification /  Object Detection / Semantic Segmentation 에 사용
- 

<br><br><br><br>

# Efficient Net 
보통 ConvNet들의 스케일을 업할 때 다음과 같은 scaling을 한다.

- Resolution : 고해상도의 이미지 사용
- Width : 레이어를 넓히기
- Depth : 신경망의 깊이를 조절

EfficientNet은 이것들을 어떻게하면 효율성을 지키면서 최적으로 스케일 업할지를 제시했다. 저 세가지 factor들을 동시에 키워주는 것이다. 그것이 바로 compound scaling 방법이다. 기존 ConvNet들과 비슷한 성능을 보이면서도 이름답게 파라미터 수를 굉장히 줄여준다.


<br><br><br><br>

* * *
[[DMQA Open Seminar] Transformer in Computer Vision - ‍김성범[소장 / 인공지능공학연구소 ]](https://youtu.be/bgsYOGhpxDc) <br>

[[Paper Review] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows - 
고려대학교 산업경영공학부 DSBA 연구실](https://youtu.be/2lZvuU_IIMA) <br>

[Swin-Transformer GitHub](https://github.com/microsoft/Swin-Transformer) <br>