# ML
# Coursera - Sequence Models - Application Using Word Embeddings
## Sentiment Classification
감정구분을 해보자. 감정구분을 하기 위해선 라벨링된 학습데이터가 필요한데 word embedding을 통해서 이 부분을 조금 해소할 수 있음. <br>


### Simple sentiment classification model

![Simple sentiment classification model]()


- O_n * E = `e_n`을 만들어준다.
    - `e_n`이 300차원의 one-hot 벡터라고 해보자.
- `e_n` 들을 더하거나 평균을 내준다.
    - 이것은 300차원을 가지고 있을 것이다.
- softmax를 거쳐서 예측을 하게 한다.
하지만 여기서 더하거나 평균을 내주게되면 부정적임에도 긍정적인 단어가 많이 들어갔을 때 구분을 잘 못하게 된다. <br>
그래서 더하거나 평균을 내는 대신 RNN을 쓸 수 있다.

### RNN for sentiment classfication

![RNN for sentiment classfication]()
- O_n * E = `e_n`을 만들어준다.
    - `e_n`이 300차원의 one-hot 벡터라고 해보자.
- 이것을 RNN에 입력한다.
- softmax를 거쳐서 예측을 하게 된다.
이것은 `many-to-one` 모델이 될 것이다. 이렇게 하면 더하거나 평균을 냈던 모델보다 조금 더 구분을 잘 할 수 있게된다.

<br><br>

## Debiasing Word Embeddings
머신러닝을 학습시킬 때 편견(bias)이 들어갈 수 있다. 여기서의 편견은 varience, bias 같이 기술적인 예측의 결과에 대한 것이 아니라 진짜 한쪽으로 치우친 편견같은 것을 말한다. Word Embedding에서 이러한 편견을 어떻게 해소할 수 있는지 알아보자. <br>

```
Man:Woman as King:Queen
```
우리는 Word Embedding으로 유사도를 측정할 수 있다고 공부했다. 그래서 위와 같은 문장과 같은 학습 데이터를 꾸준히 학습하고 유사도를 배웠다고 생각해보자.

<br>

```
Man:Computer_Programmer as Woman:____
```
학습시킨 모델은 위와 같은 문장에서 빈칸을 어떻게 예측했을까? 이것에 대해 논문을 냈던 저자들은 놀라운 예측을 얻었다고했다. 바로 빈칸에 들어갈 단어로 `Homemaker(가정주부)` 를 예측해버린것이다. 사람들의 편견이 내포된 글을 학습했더니 컴퓨터가 성차별주의자가 되어버린 것이다. 이렇게 모델은 성별, 나이, 지역, 인종, 소득 등의 편견을 reflect하고 있었다. 머신러닝은 AI면접이나 중요한 결정을 할 때 쓰이고 있으므로 이러한 예측은 치명적일 수 있다. 그러므로 우리는 이러한 편견을 걷어내고 모델이 `Homemaker(가정주부)` 가 아닌 `Computer_Programmer` 를 예측하게 해야한다. 자, 그럼 어떻게 해야할까? <br>

<br>

### Addressing bias in word embeddings

![Addressing bias in word embeddings]()

- bias의 방향을 구분한다.
    - 위에서는 젠더 bias를 구분했다.
    - embedding vecter `e_he`와 `e_she`의 차, `e_male`과 `e_female`의 차 등등, 젠더에 관한 **`e_n` 들의 차를 구해서 평균을 낸다.**
    - bias와 상관없는 방향은 non-bias 방향으로 한다.
- 확정적이지 않은 모든 단어에 대해 bias를 제거하는 것을 투영한다. 즉 중화시키는 것.
    - 할머니, 할아버지, 소녀, 소년과 같이 본질적으로 젠더로 정의되어있는 것이 있지만, 의사, 베이비시터와 같은 것들은 non-bias direction에 투영시켜 중화시키는 것.
- 쌍들을 균등화시킨다.
    - 할머니-할아버지, 소녀-소년
    - 할머니와 베이비시터와의 거리는 할아버지와 베이비시터와의 거리보다 멀 수 있는데 이것이 편견을 비칠 수 있으니 할머니-할아버지 쌍의 유사성을 똑같이 해주기 위하여 bias 방향에서 같은 포인트를 갖게 해준다.

모든 논문의 알고리즘은 이렇게 간단하지않다. 다음에 찾아보셈. 그리고 이건 프로그래밍 과제에서 다룰 것임!

<br><br><br>

# Coursera - Sequence Models - Application Using Word Embeddings
## Programming Assignment
## Operation on word vectors
- GloVe벡터를의 코사인 유사도 계산해봄
    - `e_a - e_b` 와 `e_c - e_d` 의 차를 코사인 유사도로 계싼.
- 코사인 유사도를 이용해서 유사도 측정해봄.
- neutralizse() 함수를 만들어서 중성화해봄.

## Emoji
Word vector를 이용해서 텍스트에서 이모지를 추천해봄
- sigmoid, tanh의 경우 가중치를 Xavier로 초기화 효율적
- ReLu의 경우 가중치를 He 초기화 효율적, 요즘에는 이걸로 많이 초기화한다고함.

<br><br><br>

# Kaggle
## Adaboost(Adaptive boost)
- 모델의 단점(shortcoming, 여기서는 오차)을 가중치에 반영해서 데이터를 더 많이 샘플링되게 함.
- ensemble로 모델과 가중치를 잘 합쳐서 복잡한 형태의 분류모델도 만들 수 있음
## Gradient Boosting
- 모델의 단점(오차)을 gradient에 반영하고 데이터는 그대로 쓴다.
- 현모델 : 지금까지 이만큼 맞췄으니 다음 모델은 못 맞춘 만큼만 맞춰라
- loss 함수의 복잡도에 영향을 받는 점이 Adaboost와 다른 점이다.
## Gradient Boosting Machine
누적해온 모델의 결과물의 합과 정답과의 잔차(residual)을 이용해서 알고리즘의 성능을 개선시켜 나가는 것이 핵심목적이다.
잔차만큼을 학습하라고 하는 것은 gradient의 반대방향으로 학습을 시켜라라는 것이 된다. 이것은 경사하강법이랑 비슷한 개념임. 단순한 모형인데도 복잡한 회귀선을 찾아낼 수 있음. 가장 보편적으로는 제곱 손실함수와 같이 사용된다. <br>
- 하지만 이 GBM은 Overfitting 문제가 있다. 앞에서 못 맞춘 답을 뒤에서 다 맞춰버리겠다는 구조이기 때문이다. 
    - 이것을 Regularization으로 해결하려고 함
        - Subsampling : 데이터를 바꾸지않았는데 일부러 전체 데이터의 몇%를 랜덤으로 샘플링한다. 그리고 거기서 함수를 찾은 뒤 다시 원래 데이터에 적용을 해서 다시 한 번 잔차를 계산함. 그리고 원래 데이터에서 다시 랜덤으로 샘플링하기 때문에 뒤로 갈수록 데이터가 적어지는 현상은 일어나지 않으니 이것 기억하기.
        - Shrinkage: 뒤로 갈수록 영향력을 줄여주는 것.
        - Early Stopping: validation error가 일정수준 이상 증가할 것 같으면 미리 중지시켜버리는 것.
## XGBoost
Decision Tree가 발전해온 역사가 있다.
- Decision Trees
    - 단일 의사 결정
- Bagging
    - ensemble
- Random Forest
    - Bagging을 하면서 split할 때 변수를 임의로 선택
- Boosting
    - 모델을 쫘라락 나열해서 이전의 모델에서 에러를 줄이는 것.
- Gradient Boosting
    - Gradient를 이용해서 Boosting하는 것.
- XGBoost
    - Gradient Boosting을 더 빠르게, 큰 용량에 대해서도 잘 작동시키게 병렬처리로 만드는 것. GB보다 overfitting이 더 안일어난다.

<br>

### Split Finding Algorithm
#### Approximate algorithm
원래 GBM에서는 모든 split을 다 탐색해서 제일 좋은 split을 찾았는데(Basic exact greedy algorithm), 데이터가 한꺼번에 메모리에 로딩되지않거나 분산환경에서는 힘들었다. 그래서 전체 데이터의 영역을 분할해서 구분을 시켜놓고 그 버켓들에 대해서 split 포인트를 찾음.

#### Sparsitiy-Aware Split Finding
missing value나 0이 많을 때 한 쪽(왼쪽 or 오른쪽)으로 보내버림


<br><br><br>

* * *
참고 <br>
https://youtu.be/d6nRgztYWQM 

<br>

https://youtu.be/VHky3d_qZ_E