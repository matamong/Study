# Sequence to Sequence Learning and Attention
'Attention Is All You Need' 논문은 Transformer와 sequence-to-sequence 아키텍처에 대해 설명한다. 오늘은 먼저 seq2seq과 attention을 살펴보고 tarnsformer에 대해서는 살짝만 복습해볼 것임. <br>

## sequence-to-sequence이 뭔데?
문장의 sequence와 같이 주어진 요소 sequence를 다른 시퀀스로 변환하는 신경망이다. seq2seq 학습은 시퀀스를 한 도메인(예: 영어 문장)에서 다른 도메인(예: 프랑스어로 번역된 동일한 문장)의 시퀀스로 변환하는 훈련 모델에 관한 것이다. seq2seq 모델은 성능이 좋은 편이라 번역, 비디오 캡션, 음성인식 등의 복잡한 작업에 많이 쓰인다. <br>

<br>

### Seq2Seq이 작동하는 방식
입력과 출력의 정해진 길이가 다르면 어떻게 해야할까? 예를 들면 번역을 할 때 번역을 하고싶은 문장과 번역을 한 문장의 길이가 다른 상황일 때 말이다. 이럴 경우 기존 LSTM으로는 문제를 해결할 수 없고 Seq2Seq으로 다룰 수 있다. <br><br>

![]()

Seq2Seq 모델은 세 파트로 이루어져있다.
- encoder
- intermediate (encoder) vector
- decoder

<br>

#### **Encoder**
- recurrent 유닛들의 stack이다. LSTM나 GRU cell이 될 수 있겠다.
    - 각각은 입력 시퀀스의 단일 요소를 받아서 해당 요소에 대한 정보를 수집하고 앞으로 전파(forward propagation)한다.
    - RNN에서 볼 수 있는 평범한 유닛들의 stack인 듯.

<br>

#### **Encoder vector**
- 모델의 encoder파트가 만들어내는 마지막 은닉상태이다.
- 이 벡터는 decoder가 정확한 예측을 할 수 있도록 모든 입력 요소에 대한 정보를 캡슐화하는 것을 목표로 한다.
- 모델의 decoder 부분의 이니셜 은닉상태로 작동한다.

<br>

#### **Decoder**
- recurrent 유닛들의 stack이다
    - 각각은 time-step `t`에서 output `y_t`를 예측한다.
- 각 recurrent 유닛은 이전 단위의 은닉 상태를 받아서 자체 은닉상태도 같이 출력한다.

<br>

#### **요약**
Encoder가 입력 시퀀스를 받아서 더 높은 차원 공간(n차원 벡터)에 매핑을 해준다. 그 abstract vector는 Decoder에 들어가고 아웃풋 시퀀스가 되는 것이다. 이런 식으로 다른 길이의 시퀀스를 매핑한다.

<br>

<br><br>

## Attention
Seq2Seq이 시퀀스를 매핑해줘서 다른 길이의 시퀀스를 잘 다룰 수 있게 됐는데 이것을 조금 더 보완을 해야했다. 여태까지 인코더가 모든 문장을 읽고 기억한 뒤 actvation에 저장했다. 이 방법은 모든 문장을 다 기억해야하기 때문에 문장이 길면 길수록 성능이 떨어지게된다. 여기서 사람처럼 sequence의 부분 부분을 볼 수 있는 attention이 나오게된다. attention을 간단히 말하면 각 스텝에서 문맥과 중요한 키워드를 기록하고 이를 디코더에 제공해준다. 디코더는 이 키워드를 받아서 일을 더 쉽게 할 수 있는 것이다. 그래서 문장이 길 때도 성능이 좋다.

<br><br>

## Transformer
`Attention Is All You Need` 에서 트랜스포머가 소개된 것 처럼 트랜스포머는 attention 메커니즘을 사용한다. attention 메커니즘은 LSTM처럼 인코더와 디코더를 사용해서 하나의 시퀀스를 다른 시퀀스로 만드는 것이다. 그러나 다른 것이 있는데 recurrent 신경망(LSTM, GRU 등)을 사용하지않는다. Recurrent 신경망은 지금까지 시퀀스에서 시기적절한 dependencies를 잡을 수 있는 가장 좋은 방법 중에 하나였지만 논문에서는 RNN을 배제시키고 오직 attention 메커니즘을 사용했을 때 성능이 좋았기 때문에 RNN을 배제시킨 것이다. <br>
*더 자세한 Transformer 복습은 다음 시간에!*

<br><br><br><br>

# np.ravel()
3차원을 1차원으로 평평하게.

<br>

# Kaggle
PetFinder에서 EDA를 끼적여본 결과 feature가 생각보다 결과에 영향을 많이 끼치지않는 것 같아서 내일 feature scaling을 파볼예정.

<br><br><br><br>

* * *
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346 <br>

https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04 <br>