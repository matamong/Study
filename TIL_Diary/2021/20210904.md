# ML
# Kaggle Study
## Porto Competition
### Feature Engineering
- **Dummy Variables**
    - 수치가 연관되어있지않은(e.g categorical) variable을 모델이 알아듣기쉬운 데이터로 바꿔주는데 이것을 Dummy varaible로 한다.
    - `pd.get_dummies()` 
    - 여기서 첫번째 column은 drop해주기도 하는데 이것은 모델에 따라 다름
        - drop 안해주면 dummy variables는 상관관계를 갖게 되고 이것은 몇몇 모델에 큰 영향을 끼칠 수도 있음.
            - e.g) cardinality(중복 수치)가 작을 때, 반복모델의 경우 수렴에 문제가 생기거나 변수 중요도가 왜곡됨
        - dummy가 적으면 drop하는 것을 추천.
            - e.g) 만약에 `gender` variable이 있을 때, `male`과 `female` 두 개의 dummy는 필요가 없음. female이 0이냐 1이냐로 gender를 확인할 수 있기 때문임. 하지만 variable이 수백개가 된다면 drop 안하는 것이 좋음.
- **Interaction variables**
    - accuracy 예측을 도와줄 때 좋음
    - 빵 구울 때 온도만 알거나 시간만 아는 것은 소용없으니 온도와 시간의 interactin을 통해서 빵이 잘 구워질지 좋은 평가를 내는 것과 비슷하다.
    - Porto에선 PolynomialFeatures를 이용하여 interaction만들었음

<br>

### Feature selection
- **variance가 낮거나 0인 피처 제거**
    - variables의 variance가 낮거나 0일 경우 피처가 거의 일정하기 때문에 모델의 성능이 향상되지가 않는다.
    - 낮은 variable이 엄청나게 많지않은 이상 classifier가 피처를 스스로 선택하게 놔둘 수 있음
    - sklearn의 `VarianceThreshold`로 쉽게 가능
- **`RandomForest` / `selectFromModel` 을 이용하여 피처 선택**
    
    - RandomForest의 feature importance를 기반으로 피처 선택할 수 있음.
        - RandomForest 직접 fit하고 훈련시킨뒤 모델에서 `feature_importances_`를 사용하여 찾음.
        - feature importance에 threshold를 지정해서 중요도의 레벨을 조정할 수 있음.
    - 그런다음 Sklearn의 `SelectFromModel` 로 그 피처들을 모델에서 빼옴

<br>

### Feature scaling
- `StandardScaler()` 사용했음
    - 꼭 `target`은 드랍하고 fit_transform하자.
- 몇몇 모델은 이 구간을 마치고 성능이 더 향상될 수 있음.

<br><br>

#### **PolynomialFeatures**
데이터의 분포가 곡선이어서 일반적인 선형회귀로 해결할 수 없을 때, 각 특성에 제곱을 추가해주어서 특성이 추가된 비선형 데이터를 선형회귀모델로 훈련시키는 방법이 있다. 이것을 다항회귀라고한다. <br>
어떻게 그렇게 할 수 있을까? 바로 주어진 차수를 가진 기능의 모든 다항식 조합으로 새 행렬을 생성한다.
- polynomial features 너무 많아지게 될 수 있는데 이 때, regularization 을 사용할 수 있다.
- 다항회귀를 사용할 때 `overfitting` 을 조심해야한다.
- sklearn의 `PolynomialFeatures` 사용
    - interaction 만들 수 있음


<br><br><br>


# Coursera - Coursera - Sequence Models - Recurrent Neural Networks
## Long Short Term Memory (LSTM)
GRU 보다 조금 더 파워풀한 LSTM을 살펴보자.(참고로 논문은 엄청 어렵다고한다.) GRU와 식을 비교해보자.<br>

![GRU and LSTM](https://github.com/matamatamong/img/blob/main/Visualizations/GRU_and_LSTM.PNG?raw=true)
- 더 이상 `C^<t-1>` 와 `a^<t-1>` 이 같다고 생각하지않아서 딱 `a^<t-1>` 을 명시해준다.
- 게이트를 업데이트할 때 `Γu` 로 한 번에 업데이트해주는 것이 아니라 `Γu`와 `Γf(forget)` 로 나눠서 업데이트해준다. 
- 새로운 출력 게이트를 가진다.
- 정리하자면 `update` / `forget` / `output` 게이트를 가지는 것이다.

<br>

![LSTM in pictures](https://github.com/matamatamong/img/blob/main/Visualizations/LSTM_in_pictures.PNG?raw=true)
- `a^<t-1>`, `x^<t>` 가 모여서 `forget gate`, `update gate`, `output gate`를 연산한다. 이 때, c^~<t>를 계산하기 위해서 `tanh` 를 거친다.
- 이러한 복잡한 방법을 거쳐서 값이 모여지고 `element-wise` 곱을 하는 등으로 c^<t-1>을 통해 c^<t>를 얻게된다.
- 그리고 이 유닛은 연결되어진다.

<br>

![LSTM in pictures2](https://github.com/matamatamong/img/blob/main/Visualizations/LSTM_in_pictures2.PNG?raw=true)

- peephole 연결이라는 것도 있는데, 게이트에 c^<t-1>도 끼워 넣는 것이다. 
    - 만약에 100차원의 벡터들, 즉 100차원의 은닉 메모리 셀 union이 있다고 치자. 5번째의 요소는 오직 상응하는 게이트의 다섯번째 요소에만 영향을 미치는 것이다. 이 경우 모든 요소에대해 관계가 있는 것이 아닌 `1:1` 관계가 된다.

<br>
언제 LSTM을 쓰고 언제 GRU를 써야할까? GRU는 비교적 심플하고 LSTM은 살짝 더 성능이 좋은 것 같은?

<br><br>

## Bidirectional RNN (BRNN)
모델을 조금 더 파워풀하게 만들 수 있는 아이디어를 더 살펴보자. <br>

![Getting information from the future](https://github.com/matamatamong/img/blob/main/Visualizations/Getting%20information%20from%20the%20future.PNG?raw=true)

이 구조에서 문제가 되는것은 `Teddy`가 사람의 이름인지를 구분하는 것이다. `Teddy`이전의 단어들을 읽고 예측하기란 충분하지않다. 이 구조는 unidirectional이며 유닛은 RNN, GRU, LSTM 블록이 될 수 있다.

<br>

### Bidirectional RNN (BRNN)
BRNN은 어떤 구조를 가지고 있을까? 앤드류 응의 드로잉을 보며 알아보자.

![Bidirectional RNN](https://github.com/matamatamong/img/blob/main/Visualizations/Bidirectional%20RNN.PNG?raw=true)

- `Backward recurrent` 를 유닛 중간에 집어넣는다.
    - 이 역전파 유닛은 오른쪽에서 왼쪽으로(역으로) 진행된다.
- 평범하게 블록을 계산한 뒤, 역전파 유닛으로 뒤로 계산하고 예측값을 내놓는다.
```
예측값 = g(W_y[t에 대한 전방향전파 activation, t에 대한 역방향 activation] + b_y)
```
- e.g) 타임셋이 4인 구조에서 타임 셋 <3>의 예측값을 알아보자.
    - previous
        - X^<1>의 정보는 a^<1>전방향 유닛을 탄다. 
        - 그것은 또 X^<2>의 정보를 받은 a^<2>전방향 유닛을 탄다.
        - 이 정보는 예측값의 정보로 쓰인다.
    - current
        - X^<3>의 정보는 받은 a^<3> 전방향 유닛과 역방향 유닛을 타고 예측값의 정보로 쓰인다.
    - future
        - X^<4>의 정보가 a^<4> 역방향을 타서 a^<3> 역방향 유닛으로 흘러들어가 예측값의 정보로 쓰인다.
    
결과적으로 과거, 현재, 미래의 정보를 다 쓸 수 있는 것이다! 마찬가지로 이 유닛은 GRU, LSTM 다 적용할 수 있다. <br>
하지만 여기서 단점은 미래의 정보까지 다 있어야 예측을 할 수 있어진다는 점이다. 음성인식 모델을 구현할 때 이것을 쓰면 사람이 말을 다 끝낼 때까지 기다렸다가 계산을 해야할 것이다.

<br><br>

## Deep RNNs
기존 RNN보다 더 깊은 신경망을 구축해보자. <br>
RNN은 깊은 신경망을 구축할 때 레이어를 쌓아올린다. <br>
그래서 깊은 신경망의 유닛은 두 가지의 입력을 가지게 된다.
- bottom input
- left input
그러므로 activation 값은 다음 예시와 같이 얻을 수 있다.
```
a^[2]^<3> = g(W_a^[2][a^[2]^<2>, a^[1]^<3>] + b_a^[2])
```
w_a[2]와 b_a[2]는 두번째 레이어에서 항상 사용되는 것을 볼 수 있다. <br>
세개만 쌓아도 deep하기 때문에 더 deep한 신경망을 구축하고 싶다면 예측값이 나왔을 때 끊어서 연결시키지 않는 방법으로 구조를 짠다.

![Deep_RNNs_example](https://github.com/matamatamong/img/blob/main/Visualizations/Deep_RNNs_example.PNG?raw=true)


<br><br><br>


* * *
참조 <br>
https://www.kaggle.com/learn-forum/129406