# 20210820
# ML
## Cousera - Optimization Algorithms
잘 작동되는 모델을 찾으려고 훈련을 반복해야하는 숙명에 놓여있는 머신러닝. <br>
훈련의 반복이니까 모델을 빠르게 학습시키는 것도 중요하다. <br>
엄청나게 많은 큰 데이터를 훈련하는 것은 엄청 느릴 것인데 그때문에 **최적화 알고리즘을 사용하여 효율성을 좋게만들어야한다.** <br>
그러므로 다음과 같은 알고리즘을 활용할 수 있다.

<br>

### **`Mini-batch Gradient Descent`**
#### 배치 경사하강법의 단점
- 벡터화는 m개의 샘플을 상대적으로 빠르게 훈련시킬 수 있음 그러나 m이 500만개 정도로 엄청 큰데 경사하강법을 사용해야한다면?
- 경사하강법을 사용하면 500만개의 전체 훈련 샘플을 처리한 뒤 다음 단계를 밟고 또 전체를 처리하고 밟고를 반복해야한다. 느리다!
- 500만개의 거대한 훈련샘플을 모두 처리하기 전에 경사하강법이 진행되도록 하면 어떨까? 더 빠르지않을까? 어떻게 할 수 있을까?
- 500만개의 훈련샘플을 한 1000개 정도로 나누고 이것을 **Mini-batch** 라고 해보자. 5000개의 미니배치를 얻을 수 있겠다. 
    - Mini-batch `t` : X^{t}, Y^{t}
- 이제부터 모든 훈련샘플을 동시에 진행시키는 경사하강법은 `Batch Gradient Descent Algorithm` 으로 부르고 지금 하는 것을 `Mini-batch Descent Algorithm` 으로 부르겠다.

### 미니배치 경사하강법의 등장
- 쪼갠 미니배치 하나를 진행한다. 물론 벡터화해서. 그럼 1000개의 훈련샘플로 나온 값이 나오겠지
- 이것을 가지고 비용함수를 구해준다. 물론 비용함수를 구할 때도 오차를 전체평균을 내는 것이 아니라 1000개의 값들의 평균을 내주겠지. 당연히 `Regularization` 도 이 때 할 수 있다. 물론 1000개의 값들에 대한 `Regularization` 이다.
- 자, 이제 비용함수 J^{t}에 대한 경사를 계산하기 위해서 역전파를 하자.
- 그리고 계산한 dw, db를 사용해서 업데이트 해주자.
- 이것을 미니배치마다 계에에에속 반복한다.

### 차이
- 1epoch에 배치 경사하강법은 훈련세트를 거칠 때 오직 한번의 경사하강스텝을 밟게된다.
- 1epoch에 미니배치 경사하강법은 5000개의 경사하강스텝을 밟았다.
- 배치 경사하강법보다 미니배치 경사하강법이 훨씬 더 빠르게 실행된다.
- 배치 경사하강법이 고고하게 비용함수의 최적값을 찾아 내려간다면 미니배치 경사하강법은 노이즈가 많지만 평균적으로 잘 찾아간다.
- 배치 경사하강법은 반복에 대해서 너무 느리다면 미니배치 경사하강법은 작은 단위를 반복하는 것이다 보니 벡터화의 효율을 기대하기 어렵다.
    - 그래서 1~m() 사이의 배치를 지정하는게 좋다.

### 팁
- 작은 훈련세트이면 그냥 배치 경사하강법 쓰자. 
    - 샘플이 2000개 정도까지면 그냥 쓰자
- 더 큰 훈련세트이면 미니배치 경사하강법 스자
    -  64 ~ 515 배치 정도로 나누자.
    - 가능하면 2의 제곱으로 쓰자.(64, 128, 256, 512) 컴퓨터메모리가 잘 돌아간다.
    - X^{t}, Y^{t}가 CPU/GPU 메모리에 맞는지 확인하자.
    - 2제곱의 수들을 시도해보고 가장 효율적인걸 고르자.

<br>

**더 알아보기 +++**
### Stochastic Gradient Descent (확률적 경사하강법)
한번에 하나의 샘플을 트레이닝하는 것을 확률적 경사하강법이라고 한다. 미니배치할때 사이즈를 1로 둔거라고 생각하면 될듯. 오차율이 크고 벡터화 장점도 없고해서 잘 안쓰이는 듯. 
<br>

### 하지만 이것들보다 더 효율적인 방법이 있다. 


여기까지 하고 오늘은 쉬는날이니 쉬어주겠음. <br>
효율적인 방법이 뭔지 너무 궁금하지만 쉬는 날에는 쉬어줘야하는 법. 집안일도 하고 컨디션 관리도 하고 남는 시간에 기초수학 유튜브도 볼 것.
