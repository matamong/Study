# Full Stack Deep Learning Lecture 7: Troubleshooting Deep Neural Networks

사이드 프로젝트에 앞서 DNN 트러블 슈팅하는 법 배움.

<br><br>

## 버그 잡는 DL 모델 구현 요약
- **1. 간단하게 시작해라**
    - 가능한 간단한 모델과 데이터를 선택
    - e.g) LeNet에 내 데이터 subset
- **2. 구현 & 디버그하기**
    - 모델은 한 번 돌리고 single batch를 overfit해라 
    - 알려진 결과를 다시 생산해라.
- **3. 평가하기**
    - 다음에 무엇을 해야할지 `bias-variance` 를 분해하자.
- **A. 모델/데이터를 향상시켜라**
    - `underfit` 됐다면 모델을 크게 만들고 overfit 됐다면 데이터를 넣거나 `regularize` 를 해라
- **B. 하이퍼파라미터 조정**
    - `coarse-to-fine` 무작위 검색들을 사용하라.


<br><br>

## **1. 간단하게 시작해라** 

### **1-1. 간단한 아키텍처 선택하기**


|           	|                    Start here                   	|       Consider using this later       	|
|:---------:	|:-----------------------------------------------:	|:-------------------------------------:	|
|   Images  	|             LeNet-like architecture             	|                 ResNet                	|
| Sequences 	|  LSTM with one hidden layer (or temporal convs) 	| Attention model or WaveNet-like model 	|
|   Other   	| Full connected neural net with one hidden layer 	|           Problem-dependent           	|


<br>

인풋이 다수인 모델에 대해서는 다음과 같이 간단한 접근을 할 수 있다.
- 각 Input에 대한 적당한 아키텍처로 처리
- Flatten 하여 합쳐버림
- FC를 이용하여 예측

<br><br>

### **1-2. 합리적인 기본값 사용**

- Optimizer
    - `Adma` with lr 3e-4
- Atctivation
    - `ReLu`
    - `tanh` (LSTM)
- Initializtion
    - `He normal` (ReLu)
    - `Glorot normal` (tanh)
- data normalization
    - 처음엔 하지말기. 버그가 많다.
- ragularization
    - 처음엔 하지말기. 버그가 많다.

<br><br>

### **1-3. Normalize inputs(Data)**
- 평균을 빼고 분산으로 나누기.
- 이미지에 있어서는 [0, 1]이나 [-0.5, 0.5] 로 조정하는 것이 좋다.
    - e.g) 255로 나누기
    - 라이브러리가 하게 놔두지말자. 버그가 많이 난다는디.

<br><Br>

### **1-4. 문제를 간단하게 만들자.**
- training set을 작게 시작하자.
    - e.g) 10,000
    - 작은 training set에서 문제가 생기면 큰 training set도 마찬가지이기 때문에 작은 것으로 먼저 시작해보자.
- objects, classes, image size 등의 수를 고정해서 사용하자.
- 간단한 synthetic training set을 만들자.

<br><br>

## **2. 구현 & 디버그하기**

### **DL 버그들 TOP 5**

- 잘못된 tensor들의 shape.
    - 쥐도새도 모르게 안 맞을 수 있음.
- 잘못된 Pre-processing input.
    - E.g) normalize 깜빡하거나 너무 과도하게 했거나
- 잘못된 loss function Input.
    - E.g) logits을 예상하는데도 loss에 softmax 된 아웃풋 집어넣기
- train 모드 잘못 셋팅.
    - E.g) train/eval을 뒤바꿔서 넣거나
- 산수가 불안정 - inf/NaN.
    - exp, log, div 계산할 때 자주 보임

<br><br>

### **모델 구현할 때 전반적인 조언**

- 가벼운 구현을 먼저 해라.
    - 코드를 적게 해서 버전 1을 만들어라.
    - 경험상 200라인 보다 적게.
    - 하지만 테스트 된 것은 상관없음.
- 기존에 만들어진 컴포넌츠를 사용해라
    - `Keras`
    - tf.nn.relu(tf.matmul(W,x)) 이렇게 다 구현하지 말고 `tf.layers.dense(...)` 를 써라.
- **완성된 데이터 파이프라인은 나중에 구축해라.**

<br><br>

### **2-1. 모델이 돌아가게 만들어라**

#### 자주 일어나는 이슈와 추천 솔루션
- Shape 안 맞음, Casting 문제
    - 모델 생성 단계서부터 추론해보기.
- Out Of Memory
    - 메모리 많이 들어가는 것 하나씩 스케일 백해보기.
    - E.g) batch 사이즈 줄이기, 채널 수 줄이기 등등.
- 다른 것
    - 구글링하자.

<br><br>

#### DL 코드 디버거
- Pytorch : 쉽다, ipdb를 쓰면 됨
- tensorflow : 어렵다. 
    - 그래프 쓰거나
    - 루프 돌거나
    - tfdb를 씀

<br>

이외에도 다양한 이슈와 해결이 있는데 이건 거기 pdf에 잘 정리되어있음.

<br><br>

### **2-2. single batch를 Overfit하자.**
single batch 하나를 잡아서 loss가 0에 가깝도록 해보자. 여기에 다양한 이슈가 나올 수 있다.

<br>

#### 자주 일어나는 이슈와 원인들
- **에러가 오른다.**
    - loss함수/gradient 의 sign이 뒤집힘.
    - Learning rate가 너무 높다.
    - Softmax가 잘못된 차원에 전해짐.
- **에러가 터져버림**
    - 산수 이슈. exp, log, div 계산을 체크해야함
    - Learning rate가 너무 높다.
- **에러가 진동함**
    - 데이터나 label이 손상됨.
    - Learning rate가 너무 높다.
- **에러가 변하지 않는다.**
    - Learning rate가 너무 낮다.
    - Gradient가 모든 모델에 흐르지 않는다.
    - 너무 많은 regularization
    - loss 함수 자체에 잘못된 입력
        - E.g) logits에 softmax, 아웃풋에 ReLu넣어버리기


<br><br>

### **2-3. 알려진 결과와 비교하자.**
- MNIST와 같이 잘 알려진 벤치마크를 사용하자.

<br><br>

## **3. 평가하기**
다음 스텝을 어떻게 해야할지를 알기위해 평가를 함. <br>
`bias-variance` 를 분해해서 평가.

- 휴먼레벨
- 트레이닝 에러
- val 에러
- test 에러

를 이용하여 bias, variance, underfit, overfit을 확인한다. <br>
같은 분포에서 확인되는 에러여야하지만 만약 같은 분포가 아니라면?
- validation set을 두개 쓴다.
    - training 분포에서 하나
    - test 분포에서 하나

앤드류 응 교수님이 가르쳐준 error analysis 를 이용하여 평가.

<br><br>

## **A. 모델/데이터를 향상시켜라**

<br>

### under-fitting 해결 (bias 줄이기)

<br>
A부터 순서대로 하는 것을 추천. 뒤로 갈 수록 비용이 많이 든다.
<br>

- A. 모델을 크게 만들어라.
    - layer 더하기, 유닛 더하기
- B. regurlarization 줄이기
- C. Error analysis
- D. 다른 모델 아키텍처 선택하기
    - LeNet -> ResNet
- E. 하이퍼파라미터 조정
    - Learning rate 조정 등..
- D. 피처 더하기

<br><br>

### over-fitting 해결 (variance 줄이기)
<br>
A부터 순서대로 하는 것을 추천. 뒤로 갈 수록 비용이 많이 든다.
<br>

- A. training Data를 늘려라
- B. normalization 을 더해라
    - e.g) batch norm, layer norm
- C. Data augmentation을 더해라
- D. regularization을 높여라
    - e.g) dropout, L2, weight decay
- E. Error analysis
- F. 다른 모델 아키텍처 선택하기
- G. 하이퍼파라미터 조정

<br>
이 밑에서 부터는 추천을 하지 않았다.
<br>

- H. 조기 종료
- I. 피처 삭제
- J. 모델 사이즈 줄이기

<br><br>

### 분포 다른 것 해결
<br>
A부터 순서대로 하는 것을 추천. 뒤로 갈 수록 비용이 많이 든다.
<br>

- A. test-val 에러를 하나하나 분석 & training 데이터 더 모으기
- B. test-val 에러를 하나하나 분석 & training 데이터 복제하기.
- C. 학습에 `도메인 적응 기술` 적용
    - 레이블이 지정되지 않은 데이터 또는 레이블이 지정된 제한된 데이터만 사용하여 'source' 분포를 학습하고 다른 'target'으로 일반화하는 기술
    - test 분포에서 label이 지정된 데이터에 대한 액세스가 제한될 때 사용한다.
    - 비교적 비슷한 데이터에 대한 액세스가 풍부할 때 사용한다.

<br><br>

### 데이터셋 Re-balance (가능하면 함)
<br>
val set이 적을 때, 하이퍼파라미터 조정을 너무 많이 했을 때 데이터를 더 모으거나 re-sampling해야함.
<br>

<br><br>

## **하이퍼 파라미터 조정**
조정해야 할 파라미터가 많은데 어떻게 골라야할까? <br>
경우에 따라 다르지만 경험상 다음과 같다고 한다. <br>

<br>

- 변화가 큰 것들
    - Learning rate
    - Learning rate schedule
    - loss 함수
    - 레이어 사이즈
<br>

- 평범한 것들
    - Weight 초기화
    - 모델 깊이
    - 레이어 파라미터
    - regluarization 의 weight
<br>

- 큰 변화 없는 것들
    - Optimizer 선택
    - Optimizer의 다른 파라미터들
    - Batch size
    - Nonlinearity

<br><br>

나머지는 앤드류 응 교수님의 하이퍼파라미터 수업을 다시 복습하면 될 듯.