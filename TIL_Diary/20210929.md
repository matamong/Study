# **[HuggingFace 🤗](https://huggingface.co/)**
NLP Transformer 라이브러리를 제공하는 오픈소스. 모델을 구성하고 학습하고 평가할 때 쓸 수 있음. 데이터셋도 많음. 페이스북, 아마존, 구글같은 회사들도 사용 중.

<br><br>

# **NLP Metrics**

<br>

## **Bleu Score**
NLP metric 중에 유명하다. 단점도 많고 완벽하진 않아도 계산이 간단하고 장점이 있다. 아직까지 잘 쓰는 듯. <br>
predict 문장이 인간이 만든 target 문장에 가까울 수록 좋다는 생각에 근거한다.
- 0~1 값을 가지고 있다.
    - 인간들이 똑같은 문장을 만들 순 없으니까  0.6 ~ 0.7 정도면 잘 나온 것.
    - 만약 1에 가까운 수치가 나온다면 overfitting이 되었을 수 있음.

<br>

더 알아보기 전에, `n-gram`과 `Clipped Precision`을 알아보고 오자.

<br>

### **n-gram**
일반 텍스트 처리에서 자주 사용됨. <br>
ex) The ball is blue
- 1-gram (unigram): `'The'`, `'ball'`, `'is'`, `'blue'`
- 2-gram (bigram): `'The ball'`, `'ball is'`, `'is blue'`
- 3-gram (trigram): `'The ball is'`, `'ball is blue'`
- 4-gram: `'The ball is blue'`

n-gram은 차례대로 해야한다.

<br><br>

### **Precision**
이 메트릭은 target 문장에서도 발생하는 예측 문장의 단어 수를 측정한다.
- target : He eats an apple
- 예측 문장 : He ate an apple

위와 같이 target과 예측 문장이 있다면 일반적으로 다음과 같은 식을 이용하여 Precision을 계산한다. <br>

```
Precision = 알맞게 예측한 단어 수 / 총 예측 단어 수
```
<br>

이 식을 이용하여 위의 예제문장을 계산하면 다음과 같이 계산된다.
```
Precision = 3 / 4
```
하지만 이런 식으로 Precision을 이용하는 것은 충분하지않다. 아직 두가지의 경우를 다루어야하기때문이다.

<br>
만약, 다음과 같은 target문장과 예측 문장을 Precision으로 계산하면 어떻게 될까?

- target 문장 : He eats an apple
- 예측 문장 : He He He

```
Precision = 3 / 3 = 1
```

완벽하게 1이 나와버린다.

<br><br>


#### **Clipped Precision**
Clipped Precision은 앞에서 말 한 Precision의 단점을 보완하기 위하여 아래와 같이 계산한다.

- 예측 문장의 각 단어를 모든 target 문장과 비교하고 일치하는게 있으면 맞는 것으로 간주한다.
- 맞는 단어는 target문장에 안에 있는 해당 단어의 수까지로만 제한해버린다. 이렇게 하면 위에서 봤던 중복문제는 해결된다.

<br>

아래와 같은 문장이 있다고 해보자.
- target 문장 1: He eats a sweet apple
- target 문장 2: He is eating a tasty apple
- 예측 문장: He He He eats tasty fruit

<br>
<br>

|  Word 	| Matching Sentence 	| Matched Predicted Count 	| Clipped Count 	|
|:-----:	|:-----------------:	|:-----------------------:	|:-------------:	|
|   He  	|        Both       	|            3            	|       1       	|
|  eats 	|      Target1      	|            1            	|       1       	|
| tasty 	|      Target2      	|            1            	|       1       	|
| fruit 	|        None       	|            0            	|       0       	|
| total 	|                   	|            5            	|       3       	|

- `'He'` 는 각 target 문장에서 한번씩만 있다.
- 예측 문장에서 `'He'`가 세번 나온다고 해도 target 문장의 최대 출현수인 한 번으로 클립한다(`cliped`).

<br><br>

```
Clipped Precision = 클리핑 된 알맞은 예측 단어 수 / 총 예측 단어 수
```
```
Clipped Precision = 3 / 6
```

<br><br>

### **Bleu Score가 계산되는 법**
간단한 예를 위해 target 문장 하나만 취해서 문장을 예측하는 NLP 모델이 있다고 해보자.
- target 문장 : The guard arrived late because it was raining
- 예측 문장 : The guard arrived late because of the rain

첫번째 단계는 1-gram에서 4-gram까지의 Precision Score를 계산하는 것이다.

<br>

#### **Precision 1-gram(Cliped)**

<br>

```
Precision 1-gram = 1-grams로 올바르게 예측한 수 / 1-grams의 총 예측 수
```
- target 문장 : `The` `guard` `arrived` `late` `because` it was raining
- 예측 문장 : `The` `guard` `arrived` `late` `because` of the rain

<br>

```
Presisoin 1-gram(p1) = 5 / 8
```
<br><br>


#### **Precision 2-gram(Cliped)**

<br>

```
Precision 2-gram = 2-grams로 올바르게 예측한 수 / 2-grams의 총 예측 수
```
- target 문장 : `The guard` `arrived late` because it was raining
- 예측 문장 : `The guard` `arrived late` because of the rain

위와 같이 카운트도 되고

- target 문장 : The `guard arrived` `late because` it was raining
- 예측 문장 : The `guard arrived` `late because` of the rain

위와 같이 카운트할 수 있다.

<br>

```
Presisoin 2-gram(p2) = 4 / 7
```
<br>

#### **Precision n-gram(Cliped)**
위의 방식으로 1-gram에서 4-gram까지 쭉 계산하면 아래와 같은 값이 나온다.
- 1-grams : 5 / 8 
- 2-grams : 4 / 7
- 3-grams : 3 / 6
- 4-grams : 2 / 5


<br><br>

#### **Geometric Average Precision Scores**
그 다음 단계로 아래와 같은 식으로 이 Precision Score를 합쳐준다. 일반적으로 *`N = 4`* , *`w_n = N / 4`*

<br>

![https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b](https://github.com/matamatamong/img/blob/main/Visualizations/Geometric%20Average%20Precision%20Scores.png?raw=true)

<br><br>

#### **Brevity Penalty**
세번째 단계에서는 `Brevity Penalty` 를 계산한다. 위의 단계로 계산을 한다면 단어가 적을 수록 정확도가 높아지기 때문에 모델은 정확도를 높이기 위해 최대한 적은 단어를 예측하게 되어버릴 수 있다. 이것을 막기 위해서 이름 그대로 짧은 문장에 패널티를 부여한다.

<br>

![https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b](https://github.com/matamatamong/img/blob/main/Visualizations/Brevity%20Penalty.png?raw=true)

- c 는 예상 길이 = 예상 문장의 단어 수
- r 은 target 길이 = target 문장의 단어 수

이렇게 해주면 예상 문장이 target보다 길어도 `Brevity Penalty` 가 1보다 클 수 가 없으며 예상 문장의 길이가 짧으면 값도 작아진다.

<br>

이것을 위의 예제에 적용해보면 다음과 같다.
```
c = 8
r = 8
Brevity Penalty = 1
```

<br><br>

#### **Bleu Score**
마지막으로 Bleu Score를 계산하기 위해 앞서 구해뒀던  **Geometric Average Precision Scores** 값과 **Brevity Penalty** 를 곱한다.

```
Bleu(N) = Brevity Penalty * Geometric Average Precision Scores (N)
```

<br>

보통 N값을 다르게 해서 Bleu score를 구하는데 보통 *`n = 4`* 를 사용한다. <br>
- BLEU-1은 `unigram Precision score`를 사용
- BLEU-2는 `geometric average`의 `unigram` 과 `bigram-precision` 을 사용
- BLEU-3은 `geometric average`의 `unigram` 과 `bigram-precision`, `trigram` 을 사용
- ... 등등

<br><br>

또한 아래의 공식도 달라보이지만 결국에는 우리가 했던 똑같은 식이다.

![https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b ](https://github.com/matamatamong/img/blob/main/Visualizations/log%20bleu.png?raw=true)

<br><br><br>

## **CharacterErrorRate (CER)**
- Word Error Rate (WER) 과 비슷하지만 Character 단위로 작동됨.


<br><br><br>

* * *
https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b <br>
