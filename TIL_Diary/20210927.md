# Full Stack Deep Learning Lab 3: RNNs

## **PyTorch의 [CrossEntropyLoss ](https://pytorch.org/docs/stable/nn.functional.html#cross-entropy)**

`cross-entropy` loss를 적용하려면 마지막 레이어에 `softmax`가 적용되어있어야한다. 하지만 PyTorch에서는 log_softmax와 nll_loss를 하나의 레이어에 결합한 `CrossEntropyLoss`를 제공해주기 때문에 상관없음. <br>
- multi labels도 받을 수 있음(`k-dimensinal` loss라고 한다고 함).
- 불균형 데이터셋에 좋다고 함.
(`softmax`와 `cross-entropy` 조합을 합쳐서 사람들이 softmax-loss라고도 부른다는데 이것때문에 softmax가 loss인 줄 알 수 있다고하니 조심)

```python
torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
```
<br><br><br>

## **CTC**
CTC 는 Connectionist Temporal Classification 약자로, 시퀀스간의 정렬이 필요하지만 그것이 너무 어려운 작업을 위해 설계됐다고 한다. CTC가 있으면 잘 정렬된 데이터셋이 아니더라도 작업이 쉬워지는 것.

<br>

### 어느 상황에서 필요한데?
보통 우리는 text-line 이미지의 데이터셋을 준비해놓고 이미지의 각 수평 위치에 해당하는 문자를 지정한다. 그리고 신경망을 학습하여 각각의 수평 위치에 해당하는 char의 점수를 출력한다. 그런데 여기서는 두가지의 문제점이 있다.
- char레벨에서 데이터셋을 하나하나 분석하는건 시간이 많이 걸린다.
- char의 점수만 얻기 때문에, 최종적인 text를 가지려면 추가적인 처리가 필요하다. 하나의 char가 수평위치에서 넓게 위치한다면 그 char는 중복이 될 수 있음. 이걸 지우자니 중복이 아닐 수도 있는데 지우기도 애매함.

![https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c](https://miro.medium.com/max/521/1*rfeNj9aNfUSqA-SXB_i4Bw.png)

<br>

CTC는 위의 문제점을 해결한다. <br>
- CTC에게 이미지에서 발생하는 text만 말해주면 되기때문에 text의 위치와 너비는 무시해도 됨.
- 인식 된 text에 대해서 더 이상 다른 처리를 하지않아도 된다.

<br>

### CTC가 작동하는 방식
신경망의 학습을 CTC loss함수가 가이드한다. 그냥 CTC loss 함수에 신경망의 출력 행렬과 그와 대응하는 실제 text 값만 집어넣어주면 끝인 것이다. CTC는 그걸 받아서 이미지의 실제 text값 정렬의 모든 가능성을 시도하고 그 가능성들의 점수를 더한다. 이렇게 하면 이 점수가 높을수록 실제 text값의 점수가 높아지게 된다.

<br>

### CTC가 중복문자를 해결하는 방식
인코딩할 때 임의의 `-`(pseudeo-character, blank)를 넣는데 이 때 중복 문자 사이에는 꼭 집어넣는다(이것은 나중에 디코딩할 때는 없어짐). 이렇게 하면 같은 text에서 다른 정렬을 쉽게 뽑아낼 수 있고 각 step에 있어 확률을 합산한다.

<br>

```python
torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)
```


<br><br><br>

* * *
https://pytorch.org/docs/stable/nn.functional.html#cross-entropy <br>

https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c <br>

https://machinelearning-blog.com/2018/09/05/753/ <br>