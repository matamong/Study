# 20210819
## ML
공부가 점점 복잡해져가기에 오늘 하루 공부를 쓰면서 정리할 수 있도록 TIL에 돌아왔다. 👍 <br>
프로젝트 할 때는 커밋이 남다보니 굳이 정리할 필요가 없어서 TIL에 소홀해졌었는데 다시 습관을 들여야겠다.
조금 더 파고들 공부는 따로 글을 써야하기에 여기는 딱 정리만 하는걸로.

### Coursera -  Hyperparameter
모델의 `Weight`를 건드렸던 `Regurlarizing` 다음으로 `Optimization(최적화)` 과 관련된 `Hyperparameter` 세팅을 배웠다. <br>
- `Input`을 `Normalization` 해서 데이터셋의 분포를 좋게 최적화
- 딥뉴럴네트워크에서 레이어가 깊어질 수록 기울기가 폭발적으로 증가하거나 감소되어 학습에 어려움이 있을 수 있는데,`Weight` 를 특정하게 `Initialize` 함으로써 최적화!
    - `Weight` 는 0으로 초기화되면 안되며 랜덤으로 초기화되어야 symmetry를 피할 수 있다. 0으로하면 효과없음
    - `Weight` 를 너무 크게 잡으면 최적화를 느리게 할 것이다.
-  복잡한 역전파구현의 버그를 어떻게 빨리 찾아서 최적화할수있을까?`Gradient Checking` 으로 경사검사를 해서 역전파를 맞게 구현했는지 확인할 수 있다.
    - `Gradient Checking` 은 어어어엄청 느려서 학습할 때 사용할 수 없고 디버깅에서 확인할 때만 쓴다.
    - `Gradient Checking` 은 `Dropout` 에서 동작 안한다.

<br><br>


### 경사하강법과 역전파

- 모델을 학습시키면 예측값을 뱉어낼것이다
- 그 예측값과 실제값을 이용하여 오차를 확인할 것이다. 이 때 오차를 죄다 구할 수 있는 비용함수를 사용한다.
- 비용함수에 어떠한 파라미터 w를 넣었을 때 값이 작아야 오차없는 모델일 것이다.
- 머신러닝의 모든 목적은 이 비용함수의 값을 최소한으로 만드는 w, b 파라미터를 찾는 것이다.
- 어떻게 찾느냐?  **`경사하강법`**을 통해 오차함수에서 기울기가 0인 부분을 찾아 내려가서 그것에 해당하는 파라미터를 업데이트 해주면 오차함수의 값이 줄어드는 것이다.
- 엌케이. 설레는 마음을 가지고 모델을 만지작 거려보자
- 처음엔 비용함수가 0이되는, 즉, 오차가 적은 w를 한번에 딱 찾을 수는 없을 것이다.
- 그래서 우리는 랜덤하게 초기화 해놓고 **경사하강법** 을 통해서 천천히 파라미터들을 업데이트 할 것이다.
- **경사하강법** 을 하려면 오차함수의 `기울기의 방향` 을 알아야 기울기가 0인 부분으로 점점 내려갈 수 있겠지?
- `기울기의 방향`을 알기위해서는 어떠한 w의 값을 넣었을 때의 **`오차함수의 도함수(편미분)`** 를 구해야한다.
- 근데 편미분을 한두번도아니고 w값 마다 해야하니까 너무 너무 너무 너무 많이해야한다. 수백만개를 해야 될 수 있다고!
- 근데 짜잔, **`역전파` 를 사용하면 **`오차함수의 도함수(편미분)`** 를 쉽게 계산할 수 있다.**
- 어휴 이제 오차함수의 도함수를 알았으니 기울기의 방향을 알 수 있게 되고 **경사하강법** 을 진행한다!


 * * *

 [Debugging: Gradient Checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)
 [](https://www.quora.com/What-is-the-difference-between-gradient-descent-and-back-propagation-in-deep-learning-Are-they-not-the-same-thing)