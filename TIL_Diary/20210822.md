# ML
## Coursera Optimization Model 과제
- 평범한 경사하강법과 미니배치, SGD, 모멘텀, 아담을 구현해보았다.
    - 모멘텀이 좋긴 했는데, 학습률이 낮고 데이터세트가 단순할 때에는 뭐 뛰어나진 않더라.
    - Adam이 진짜 미니배치랑 모멘텀보다 훨씬 괜찮았다. 최적화는 Adam!!
- 각 최적화방법에 Learning Decay 구현 
    - Learning Decay 를 구현할 경우에는 모멘텀이용한 SGD랑 그냥 SGD가 Adam이랑 비슷할정도로 속도와 정확도성능이 좋았다
    - 근데 Adam에 Learning Decay 적용하니까 위에 두개와 비교해서 learning curve 는 비슷했지만 더 빨랐음
    - 최적화는 Adam!!

## Kaggle
### Titanic
- 오늘은 모델을 `Ensembling` 하고 `Stacking` 하는 것을 필사해봤다.
- 하기에 앞서 `SklearnHelper` 객체를 만들어서 편리하게 Sklearn classifier 들을 사용하는 방법을 봤는데 괜찮았다.

- `Stacking` 이란?
    - 개별 모델이 예측한 데이터를 다시 `training-set`으로 사용해서 학습하는 것. (여러개의 모델을 stacking 하는 모습에서 이름을 붙였겠지만 왜 나는 게워낸걸 다시 먹는 모습이... 🤮)
    - 여러 모델을 학습시켜서 예측한 값을 다시 학습시키고 최종모델의 예측값을 쓰는 것!
    - kaggle에서 점수를 올리기 위해서 많이들 사용한다.
    - 베이스 모델의 예측값을 두번째 레벨 모델의 입력값으로  사용하기 때문에 `Overfitting`이 일어날 확률이 높다. 그래서 **`Stacking` 을 할 때, `OOf(Out Of Fold)` 예측을 사용한다.**
        -  `OOf(Out Of Fold)` : 모델들이 K-fold 방식을 이용하여 예측을 하면 그 예측값들을 동일한 `test_set` 에 대해 예측시켜서 `test_set`에 대한 예측값들을 얻는다. 그 다음 그 예측값들의 평균값을 취하여 최종 예측값을 계산하는 것(이 때, reshape 필수). 대회용으로만 쓰이는 듯.


- `np.ravel()`
    - 다차원 배열을 1차원 배열로 펴주는 함수. 

kaggle이 전세계에서 모인 커뮤니티다보니 영문이 뒤죽박죽인 문장이 꽤 있어서 해석하는게 더 오래걸리는듯...다들 번역기 돌리는구나 정겹다. 나만 그러는게 아니었구나.