# ML
# Coursera - Sequence Models - Transformer Network
## Transformer Network Intuition
비교적 복잡한 신경망구조이지만 NLP에 지대한 영향을 끼치고 있는 Transformer 신경망에 대하여 알아보자. 

<br>

### Transformer Network Motivation
- RNN은 경사소실문제를 가지고 있어 긴 문장을 소화하지 못했다.
- GRU와 LSTM(Gate)으로 이 문제를 해결했다.
- RNN에서 GRU, LSTM을 거쳐가면서 모델은 점점 복잡해져갔다. 그리고 각각의 유닛은 정보의 흐름에 있어서 병목이 되어버렸다.
- Transformer 위의 것들과는 다르게 왼쪽에서 오른쪼으로 한 단어씩 소화하지않고 한 번에 소화한다.

### Transformer Network Intuition
**Attention** 을 기반으로한 표현을 사용하는 것과 **CNN** 스타일의 과정을 합친 것이다. 
- RNN이 한 타임에 한 입력을 계산하고
    - 단어의 표현을 굉장히 잘 계산함
    - `Self-Attention`
        - 5개의 단어가 있다면 5개의 표현이 
    - `Multi-Head Attention`
        - self-attention의 loop라고 보면 됨
- CNN은 입력을 모두 픽셀에 넣어 동시에 계산함.

<br><br>

## Self-Attention
- 문맥에 따라서 이 단어가 무엇을 표현하는지를 찾음.
- 앞에서 공부했던 Attention 매커니즘과 비슷함 그러나 다른 점은 Self-Attention은 문장에 있는 단어들을 모두 동시에 계산한다는 것.

<br>

### Self-Attention Calculation
![Self-Attention Calculation]()

위의 예에서 `l'Afrique` 를 self-attention A^<3>로 계산해보자.

- 문장의 각각의 단어를 세 쌍의 행렬값으로 구분해준다.
    - `Query (Q)`
    - `Key (K)`
    - `Value (V)`
    - 이 값들은 모두 DB에서 나오는 알고있는 개념임.
        - Query는 그 단어에 대한 질문, Key는 person,action같은 카테고리
- X^<3>가 l'Afrique의 word embedding이라고 하면, q^<3>는 학습된 행렬로 계산되고 K와 V도 비슷하게 계산된다.
```
 q^<3> = W^Q * x<3>
 K^<3> = W^K * x^<3>
 V^<3> = W^V * X^<3>
```
- W^Q, W^K, W^V 는 이 알고리즘의 파라미터이다.
- q^<3>와 K^<1>을 계산하면 쿼리에 대한 단어의 답이 얼마나 좋은지를 계산함
   
- q에 대한 각 단어의 k를 계산하고 합쳐서 softmax를 해줌.
    - e.g) q^<3>가 '이 단어에서 뭐가 일어나고 있는가?'의 물음이라면 가장 어울리는 표현인 동사 'visit'이 여기서 큰 값을 가짐
- softmax해준 값들을 각 단어의 V와 곱해줌.
- 그 값들을 모두 더해줌
- 이러한 방법으로 A^<3>에 가장 어울리는 표현을 찾는다.

이렇게 해주면 `l'frique`는 정해진 word embedding이 아니라 self-attention에게 `l'frique`는 visit의 목적이구나를 깨닫게 해주는 것이 될 수 있다. 더 풍부하고 쓸만한 그 단어의 표현 계산해주는 것이다.

<br><br>

## Multi-Head Attention
Multi-Head Attention에 대해서 알아보자. 수식이 좀 어려울 수 있는데 기본적으로는 앞에서 배웠던 self attention 매커니즘을 큰 for loop에 돌려버리는 것이라고 생각하면서 보자.(실제로는 동시에 for loop이 아니라 계산함) <br>

### Multi-Head Attention

![Multi-Head Attention]()

- sequnece에 대해 self-attention을 계산하는 것을 `head`라고 부른다.
- 앞에서와 마찬가지로 q,k,v를 각 단어의 입력으로 한다.
- 새로운 파라미터로 self attention을 여러번 계산한다.

<br><br>

## Transformer Network
self-attention과 multi-head attention을 배웠으니 이것을 합쳐서 Transformer Network를 만들어보자. <br>

### Transformer

![Transformer]()

- 주의할 점은 데이터를 한 번에 넣어서 돌려버리기 때문에 순서에 대한 정보가 지급되지 않아서 positional 데이터를 인코딩을 사용한다. positional 데이터 인코딩과 단어 임베딩 합을 사인 및 코사인 방정식 값으로 추가해줌.
- Masking을 해줌으로써 입력 문장의 길이를 균등하게 해줌.

<br><br>

# Kaggle
- Confusion Metrics 다시 복습
- Under-sampling 구현
- over-sampling 구현

